{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exposure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOc+KeJBaVuCbKKc0mOKl+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/exposure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPpCkgv0271l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import datetime\n",
        "import math\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7PR_gGjwNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONSTANTS\n",
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JgvcYwsfpy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# READ JSON\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyIm6NR55FGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrfK_M25rAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD user.json FROM DRIVE\n",
        "users_dataset_id = '1JokoV68YD5Iq2l4Y_IV2RJzBpD_mSCyq'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': users_dataset_id})\n",
        "download.GetContentFile('user.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMGOW-SHRrP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD business.json FROM DRIVE\n",
        "business_dataset_id = '1Qoy132gb205xAIFjkBbZ2CyYDeaz9yqU'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': business_dataset_id})\n",
        "download.GetContentFile('business.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tkQRFfw16x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD review.json FROM DRIVE\n",
        "review_dataset_id = '1mW1WbpMFjN0qQpLnM-R_TzNpAkFsBLHQ'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': review_dataset_id})\n",
        "download.GetContentFile('review.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcwnWi7G4w5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "users = read_json('user.json')\n",
        "users = pd.DataFrame(users)\n",
        "business = read_json('business.json')\n",
        "business = pd.DataFrame(business)\n",
        "reviews = read_json('review.json')\n",
        "reviews = pd.DataFrame(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40JdFbw0Qqmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FILTER RESTAURANTS\n",
        "is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "restaurants = business[is_restaurant]\n",
        "\n",
        "# Filter closed restaurants\n",
        "restaurants = restaurants[restaurants['is_open'] == 1]\n",
        "\n",
        "# Order restaurants by review_count\n",
        "restaurants = restaurants.sort_values('review_count')\n",
        "\n",
        "restaurants.to_csv('restaurants_input.csv')\n",
        "# NOW CHOOSE THE RESTAURANTS AND SAVE THEIR IDS INTO TXT FILE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYSaLtPHqsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXEC IN LOCAL THE DOWNLOAD OF REVIEW RANKINGS OF EACH RESTAURANT\n",
        "# UPLOAD CSV IN COLAB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbM06dpOYYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################# EXECUTE IN LOCAL (if not blocked by yelp) #############################################\n",
        "def get_ranking_from_call(url_business, lang, sort, query):\n",
        "    headers = [{\"name\": \"Accept\", \"value\": \"*/*\"}, {\"name\": \"Accept-Encoding\", \"value\": \"gzip, deflate, br\"},\n",
        "               {\"name\": \"Accept-Language\", \"value\": \"it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3\"},\n",
        "               {\"name\": \"Connection\", \"value\": \"keep-alive\"},\n",
        "               {\"name\": \"Content-Type\", \"value\": \"application/x-www-form-urlencoded; charset=utf-8\"}, {\"name\": \"Cookie\",\n",
        "                                                                                                       \"value\": \"qntcst=D; hl=en_US; wdi=1|3C26116D69138F61|0x1.78d019f71a444p+30|a7756ff94751d3a9; _ga=GA1.2.3C26116D69138F61; location=%7B%22city%22%3A+%22New+York%22%2C+%22state%22%3A+%22NY%22%2C+%22country%22%3A+%22US%22%2C+%22latitude%22%3A+40.713%2C+%22longitude%22%3A+-74.0072%2C+%22max_latitude%22%3A+40.8523%2C+%22min_latitude%22%3A+40.5597%2C+%22max_longitude%22%3A+-73.7938%2C+%22min_longitude%22%3A+-74.1948%2C+%22zip%22%3A+%22%22%2C+%22address1%22%3A+%22%22%2C+%22address2%22%3A+%22%22%2C+%22address3%22%3A+null%2C+%22neighborhood%22%3A+null%2C+%22borough%22%3A+null%2C+%22provenance%22%3A+%22YELP_GEOCODING_ENGINE%22%2C+%22display%22%3A+%22New+York%2C+NY%22%2C+%22unformatted%22%3A+%22New+York%2C+NY%2C+US%22%2C+%22accuracy%22%3A+4.0%2C+%22language%22%3A+null%7D; xcj=1|Ptt9P03gfc75x_PBT9zmqCkUuSuyB7PR-wWUBvABNi4; __qca=P0-60561249-1581956668708; G_ENABLED_IDPS=google; __cfduid=db8764ff59d8028a6c2e1b214867927d81583160194; _gid=GA1.2.2014867238.1583835527; bse=05dcd9d5de304ef0b1d9a76fa768b10f; sc=8a1ca0dbc2; pid=505721aa4569e7bb\"},\n",
        "               {\"name\": \"Host\", \"value\": \"www.yelp.com\"},\n",
        "               {\"name\": \"Referer\", \"value\": \"https://www.yelp.com/biz/noche-de-margaritas-new-york\"},\n",
        "               {\"name\": \"TE\", \"value\": \"Trailers\"}, {\"name\": \"User-Agent\",\n",
        "                                                     \"value\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0\"},\n",
        "               {\"name\": \"X-Requested-By-React\", \"value\": \"true\"},\n",
        "               {\"name\": \"X-Requested-With\", \"value\": \"XMLHttpRequest\"}]\n",
        "    headers_ok = {}\n",
        "    for header in headers:\n",
        "        temp = {\n",
        "            header['name']: header['value']\n",
        "        }\n",
        "        headers_ok.update(temp)\n",
        "\n",
        "    x = 0\n",
        "    reviews_list = []\n",
        "    position = 1\n",
        "    url = url_business + \"/review_feed?rl=\" + lang + \"&sort_by=\" + sort + \"&q=\" + query\n",
        "\n",
        "    while 1:\n",
        "        if x == 0:\n",
        "            page_load = requests.get(url + '&start=', headers=headers_ok)\n",
        "        else:\n",
        "            page_load = requests.get(url + '&start=' + str(x), headers=headers_ok)\n",
        "        print(page_load)\n",
        "        x = x + 20\n",
        "        reviews = page_load.json()['reviews']\n",
        "        # print(json.dumps(reviews, indent=4, sort_keys=True))\n",
        "        if not reviews:\n",
        "            break\n",
        "        for review in reviews:\n",
        "            reviews_list.append((position, review['userId'], review['user'],#['reviewCount'],\n",
        "                                 datetime.datetime.strptime(review['localizedDate'], '%m/%d/%Y')))\n",
        "            position = position + 1\n",
        "    df_reviews = pd.DataFrame(reviews_list, columns=[\"position\", \"user_id\", \"user\", \"date\"])\n",
        "    return df_reviews\n",
        "\n",
        "\n",
        "def retrieve_rankings(business_id):\n",
        "    df_rel_ranking = get_ranking_from_call(\"https://www.yelp.com/biz/\" + business_id, \"en\", \"relevance_desc\", \"\")\n",
        "    df_date_ranking = df_rel_ranking.sort_values(by=['date']).reset_index(drop=True)\n",
        "    df_date_ranking['position'] = df_date_ranking.index + 1\n",
        "    df_rand_ranking = df_rel_ranking.sample(frac=1).reset_index(drop=True)\n",
        "    df_rand_ranking['position'] = df_rand_ranking.index + 1\n",
        "\n",
        "    df_rel_ranking.to_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "    df_date_ranking.to_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "    df_rand_ranking.to_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "\n",
        "\n",
        "rest_ids = []\n",
        "\n",
        "with open('rest_ids.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    rest_ids.append(line[:-1])\n",
        "\n",
        "print(rest_ids)\n",
        "for id in rest_ids:\n",
        "    retrieve_rankings(id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070u7qOzHsiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# JOIN USER FROM YELP WITH USER FROM DATASET AND PRINT LOST USERS\n",
        "def filter_user_from_dataset(df, users):\n",
        "  df = df.rename(columns={'Position':'position','User_Id': 'user_id',\n",
        "                          'Date':'date'})\n",
        "  df = df[['position','user_id','date']]\n",
        "  df_merged = df.merge(users, on='user_id')\n",
        "  total_review = df['position'].max()\n",
        "  print('Utenti persi: totali ' + str(total_review) + ' - utenti nel dataset ' +\n",
        "        str(len(df_merged.index)) + ' = ' + str(total_review - len(df_merged.index)))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "\n",
        "  cols = df_merged.columns.tolist()\n",
        "\n",
        "  df_merged = df_merged[['position', 'user_id', 'date', 'fans', 'average_stars', 'review_count']]\n",
        "\n",
        "  return df_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RAaBHLaVMDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADD REVIEW INFO IN RANKING DATAFRAME\n",
        "def integrate_review_info(df, reviews, business_id):\n",
        "  business_reviews = reviews[reviews['business_id'] == business_id]\n",
        "  df_merged = business_reviews.merge(df, on='user_id')\n",
        "  del df_merged['date_y']  # it's date from yelp website (not updated in dataset)\n",
        "  df_merged = df_merged.rename(columns={'date_x':'date'})\n",
        "  # drop duplicates reviews from same user\n",
        "  df_merged = df_merged.sort_values('date').drop_duplicates('user_id',keep='last')  \n",
        "  df_merged = df_merged.sort_values(by=['position']).reset_index(drop=True)\n",
        "  print('Review perse: ', len(df.index) - len(df_merged.index))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "\n",
        "  df_merged = df_merged[['position', 'user_id', 'review_id', 'date', 'text', 'fans', 'average_stars', 'review_count', 'business_id']]\n",
        "  \n",
        "  return df_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezxCXIZBkcAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADD USER DATA TO GROUP USERS\n",
        "def integrate_user_data(df_users, reviews, business, id):\n",
        "  vectors = df_users[['user_id', 'review_count', 'fans' , 'average_stars']]\n",
        "  vectors[\"top_location\"] = np.NaN\n",
        "  vectors[\"useful\"] = np.NaN\n",
        "  vectors[\"funny\"] = np.NaN\n",
        "  vectors[\"cool\"] = np.NaN\n",
        "  for index, user in vectors.iterrows():\n",
        "    print(index)\n",
        "    # TOP THREE LOCATION\n",
        "    location_list = get_top_three_loc(user, reviews, business)\n",
        "    top_three_location = str(location_list)\n",
        "    vectors.loc[index, 'top_location'] = top_three_location\n",
        "\n",
        "    # USEFUL FUNNY COOL\n",
        "    useful, funny, cool = get_details_user(user, reviews)\n",
        "    vectors.loc[index, 'useful'] = useful\n",
        "    vectors.loc[index, 'funny'] = funny\n",
        "    vectors.loc[index, 'cool'] = cool\n",
        "\n",
        "    # DEMOGRAPHICS\n",
        "    # TODO\n",
        "    \n",
        "  vectors.to_csv('user_vectors_' + id + '.csv')\n",
        "  return vectors\n",
        "\n",
        "\n",
        "def get_details_user(user, reviews):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  return user_reviews['useful'].sum(), user_reviews['funny'].sum(), user_reviews['cool'].sum()\n",
        "\n",
        "\n",
        "def get_top_three_loc(user, reviews, business):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  result = user_reviews.merge(business, on='business_id')\n",
        "  return result['state'].value_counts().index.tolist()[:3]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW2AKZ8lHgoB",
        "colab_type": "code",
        "outputId": "274e170e-0075-44d6-e065-04b2e1cf2048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PIPELINE\n",
        "attribute = 'review_count'\n",
        "# id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg']\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "for id in id_list:\n",
        "\n",
        "  df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1(id, users, reviews)\n",
        "  \n",
        "  group_list, method = pipeline2(df_rel_ranking, reviews, business, id)\n",
        "  \n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews)\n",
        "  "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Utenti persi: totali 2091 - utenti nel dataset 1714 = 377\n",
            "Utenti persi: totali 2091 - utenti nel dataset 1714 = 377\n",
            "Utenti persi: totali 2091 - utenti nel dataset 1714 = 377\n",
            "Review perse:  255\n",
            "Review perse:  255\n",
            "Review perse:  255\n",
            "\n",
            "++++++++++++++++ RANKING ++++++++++++++++++\n",
            "\n",
            "Ranking by Yelp filter:\n",
            "      position                 user_id               review_id  \\\n",
            "0            1  mCrhj_CG3_pOmDFcfKotRQ  xDSSvsU44pqQeEbinlX3Hw   \n",
            "1            2  VlcasgkqiTuPi-nVT7rEtw  pWuMStfD6x0H1z0ag8UFLA   \n",
            "2            3  xDl9ZF3SckkZde_48W6WeA  dqE-KMgIsYywo0CEGTzOPg   \n",
            "3            4  TKwzreBMGFbu9zN6taeVLA  qb17dS1Taj_O6_3S2JIqqw   \n",
            "4            5  GWOf1oB2mTalRY_A-EjSiQ  4hs0ARCPqpBZWNQQGJZDIg   \n",
            "...        ...                     ...                     ...   \n",
            "1454      1455  03_OcS8SbBcO4RTx_jYAUw  ITdCpwUQBirWyFYP-rbepA   \n",
            "1455      1456  vpCdKYNXUjs5o-tVSmwGAg  Uky0g-WO7e1dmkYgt0V_gA   \n",
            "1456      1457  vVBNWuQO9m5Utm69PGoRCw  YsL15iO3svxNiI3y9XJ_Rw   \n",
            "1457      1458  h7zHYM8LbPWmXx0ZEqOOAw  w3CG6iiaHWCQ8bscVjcdng   \n",
            "1458      1459  4wbMeUS9tp2QGqyNhj3YTg  z9l1bGaq_vZICHe1_qFTXA   \n",
            "\n",
            "                     date                                               text  \\\n",
            "0     2018-09-30 04:34:20  We decided to celebrate our 20+ wedding annive...   \n",
            "1     2018-08-12 16:31:00  I wasn't really impressed. I heard so many gre...   \n",
            "2     2018-02-04 18:45:36  Review Update: I really hate to leave this, bu...   \n",
            "3     2018-10-13 23:22:29  I had lunch at Tupelo Honey on a Saturday with...   \n",
            "4     2018-10-11 03:09:12  (3.5 stars) Pretty solid food with an extensiv...   \n",
            "...                   ...                                                ...   \n",
            "1454  2016-03-26 23:01:15  This was a disappointing first visit to Tupelo...   \n",
            "1455  2014-01-06 17:36:13  My husband and I took my siblings on the first...   \n",
            "1456  2013-12-27 20:55:11  We were thrilled to hear THC was openin in Cha...   \n",
            "1457  2015-11-23 22:42:39  So disappointed in our last visit here. My BF ...   \n",
            "1458  2014-09-01 15:12:46  My wife has been to the TH's in Asheville seve...   \n",
            "\n",
            "      fans  average_stars  review_count             business_id  \n",
            "0        2           3.73            82  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1       13           4.41           279  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "2       95           3.65           950  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "3       12           3.75           338  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "4       48           3.69           745  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "...    ...            ...           ...                     ...  \n",
            "1454     0           3.50             4  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1455     0           2.33             3  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1456     0           2.57             7  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1457     0           3.39            17  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1458     2           2.57            41  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "\n",
            "[1459 rows x 9 columns]\n",
            "\n",
            "\n",
            "Ranking by Date:\n",
            "      position                 user_id               review_id  \\\n",
            "0            1  PVyZXgOkVtnU6966FDFhuw  pyjzGmpzYMF_35rFasROmA   \n",
            "1            2  aK99XES2p7yNyWKpJjpxkQ  th1ZJDdp58YN_iGuQOK39Q   \n",
            "2            3  f6k8dOPuuFHJaj1icCV1ZA  L-Peuz58xOpUymG_kUFuMQ   \n",
            "3            4  hVXj8lnaTIMLTkD_yjSj6A  8TIrtWHkpttg_b7Mcz1vCQ   \n",
            "4            5  wCDC5NVQLKdTa-OwzLG9vg  g6s-w7urYtEBRgoeIZcFkg   \n",
            "...        ...                     ...                     ...   \n",
            "1454      1455  VlcasgkqiTuPi-nVT7rEtw  pWuMStfD6x0H1z0ag8UFLA   \n",
            "1455      1456  XGHvw9aF-8a3rn-2Lji8nw  O2nGTPz1kQhrr70i6hGnxQ   \n",
            "1456      1457  vH0RHYdDnzIXSUWmZbBo7A  s9eWSfzZsnhuK-5AWeZgRQ   \n",
            "1457      1458  BkIOqr6F6tNzZjcbx0HAUw  I7itarovek8dj4Cj1Q6hQg   \n",
            "1458      1459  mCrhj_CG3_pOmDFcfKotRQ  xDSSvsU44pqQeEbinlX3Hw   \n",
            "\n",
            "                     date                                               text  \\\n",
            "0     2013-12-09 03:17:14  Welcome to Charlotte, Tupelo Honey Cafe! Yelpe...   \n",
            "1     2013-12-09 15:29:25  The wings were amazing. The mac and cheese was...   \n",
            "2     2013-12-10 22:16:37  As expected, Tupelo Honey Charlotte did not di...   \n",
            "3     2013-12-13 03:53:55  Was pretty darn good.  Service was great.   Ja...   \n",
            "4     2013-12-14 16:01:55  We visited Tupelo Honey after only a week of b...   \n",
            "...                   ...                                                ...   \n",
            "1454  2018-08-12 16:31:00  I wasn't really impressed. I heard so many gre...   \n",
            "1455  2014-01-06 15:53:06  I could NOT wait to try out the new Tupelo Hon...   \n",
            "1456  2018-10-05 13:21:00  Really excited about this place based on revie...   \n",
            "1457  2016-01-15 20:21:07  Came here for my birthday today. My server, Al...   \n",
            "1458  2018-09-30 04:34:20  We decided to celebrate our 20+ wedding annive...   \n",
            "\n",
            "      fans  average_stars  review_count             business_id  \n",
            "0       61           3.82           572  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1        0           3.31            13  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "2        0           3.96            25  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "3        3           3.76           103  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "4        1           3.33            12  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "...    ...            ...           ...                     ...  \n",
            "1454    13           4.41           279  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1455     7           4.05           157  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1456     2           4.03           101  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1457     1           4.06            32  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1458     2           3.73            82  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "\n",
            "[1459 rows x 9 columns]\n",
            "\n",
            "\n",
            "Ranking Random:\n",
            "      position                 user_id               review_id  \\\n",
            "0            1  MjkbqbLYdLHAimX1PQ9UGg  qvWOvN8nrtIA--M29SXEHA   \n",
            "1            2  t8v-SKJTkQVw7QYQMOJKfw  makRSbxdg51n9iOI5YzKVw   \n",
            "2            3  U8AqHy8hJfD-9X6H68KZsg  Bna98PYzvBRHRMY6cW8Vpw   \n",
            "3            4  Hm6FZKfquhi6crLAdwY71Q  da4X8VoiYimXOpbnianOPw   \n",
            "4            5  HDpg3oIdH3AWDmh9VAKBTA  M0TaPTA98Cr7vCstpUx8IA   \n",
            "...        ...                     ...                     ...   \n",
            "1454      1455  JeF43VrWyHcqQIyLo0XqOw  KbbPFQyat7ouBywGahG4bw   \n",
            "1455      1456  oftCA3e_n92XBWf453f56g  IexKQIuEjDQwZQC_Q0VlUA   \n",
            "1456      1457  VlcasgkqiTuPi-nVT7rEtw  pWuMStfD6x0H1z0ag8UFLA   \n",
            "1457      1458  MWjOBJEQop__owYvB3quLg  YkdrsGjuhepwQROjRzkBJQ   \n",
            "1458      1459  yeI9EOQzS4r6F5GrrUsWxg  JjZmzvSIpaC_yAY-WlpjeA   \n",
            "\n",
            "                     date                                               text  \\\n",
            "0     2016-07-07 22:09:15  This place was amazing! My fiancé and I stoppe...   \n",
            "1     2017-08-10 16:14:20  Two words: AMAZING restaurant. My girlfriend a...   \n",
            "2     2018-07-29 00:52:53  I must say the staff here were amazing and the...   \n",
            "3     2017-04-07 18:33:52  This place is amazing. Outstanding food and dr...   \n",
            "4     2016-02-08 15:15:06  The first time I went to THC - Charlotte I was...   \n",
            "...                   ...                                                ...   \n",
            "1454  2014-05-01 00:25:52  Was really excited to eat here after reading t...   \n",
            "1455  2015-07-14 23:30:38  Adorable decor. LOVE the ceiling fans and old ...   \n",
            "1456  2018-08-12 16:31:00  I wasn't really impressed. I heard so many gre...   \n",
            "1457  2018-05-16 23:52:53  The food choices here rotate about every month...   \n",
            "1458  2016-04-20 21:07:11  ​\\nLooking at these reviews, I have to ask mys...   \n",
            "\n",
            "      fans  average_stars  review_count             business_id  \n",
            "0        0           3.00             5  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1        0           3.00             2  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "2        0           5.00             1  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "3        0           4.29            17  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "4        2           4.08            13  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "...    ...            ...           ...                     ...  \n",
            "1454     0           3.76            17  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1455     0           3.08            26  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1456    13           4.41           279  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1457     3           4.35            86  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1458     1           3.00            29  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "\n",
            "[1459 rows x 9 columns]\n",
            "\n",
            "\n",
            "\n",
            "++++++++++++++++ GROUPS CREATION ++++++++++++++++++\n",
            "\n",
            "      position                 user_id               review_id  \\\n",
            "0            1  mCrhj_CG3_pOmDFcfKotRQ  xDSSvsU44pqQeEbinlX3Hw   \n",
            "1            2  VlcasgkqiTuPi-nVT7rEtw  pWuMStfD6x0H1z0ag8UFLA   \n",
            "2            3  xDl9ZF3SckkZde_48W6WeA  dqE-KMgIsYywo0CEGTzOPg   \n",
            "3            4  TKwzreBMGFbu9zN6taeVLA  qb17dS1Taj_O6_3S2JIqqw   \n",
            "4            5  GWOf1oB2mTalRY_A-EjSiQ  4hs0ARCPqpBZWNQQGJZDIg   \n",
            "...        ...                     ...                     ...   \n",
            "1454      1455  03_OcS8SbBcO4RTx_jYAUw  ITdCpwUQBirWyFYP-rbepA   \n",
            "1455      1456  vpCdKYNXUjs5o-tVSmwGAg  Uky0g-WO7e1dmkYgt0V_gA   \n",
            "1456      1457  vVBNWuQO9m5Utm69PGoRCw  YsL15iO3svxNiI3y9XJ_Rw   \n",
            "1457      1458  h7zHYM8LbPWmXx0ZEqOOAw  w3CG6iiaHWCQ8bscVjcdng   \n",
            "1458      1459  4wbMeUS9tp2QGqyNhj3YTg  z9l1bGaq_vZICHe1_qFTXA   \n",
            "\n",
            "                     date                                               text  \\\n",
            "0     2018-09-30 04:34:20  We decided to celebrate our 20+ wedding annive...   \n",
            "1     2018-08-12 16:31:00  I wasn't really impressed. I heard so many gre...   \n",
            "2     2018-02-04 18:45:36  Review Update: I really hate to leave this, bu...   \n",
            "3     2018-10-13 23:22:29  I had lunch at Tupelo Honey on a Saturday with...   \n",
            "4     2018-10-11 03:09:12  (3.5 stars) Pretty solid food with an extensiv...   \n",
            "...                   ...                                                ...   \n",
            "1454  2016-03-26 23:01:15  This was a disappointing first visit to Tupelo...   \n",
            "1455  2014-01-06 17:36:13  My husband and I took my siblings on the first...   \n",
            "1456  2013-12-27 20:55:11  We were thrilled to hear THC was openin in Cha...   \n",
            "1457  2015-11-23 22:42:39  So disappointed in our last visit here. My BF ...   \n",
            "1458  2014-09-01 15:12:46  My wife has been to the TH's in Asheville seve...   \n",
            "\n",
            "      fans  average_stars  review_count             business_id  \n",
            "0        2           3.73            82  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1       13           4.41           279  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "2       95           3.65           950  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "3       12           3.75           338  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "4       48           3.69           745  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "...    ...            ...           ...                     ...  \n",
            "1454     0           3.50             4  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1455     0           2.33             3  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1456     0           2.57             7  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1457     0           3.39            17  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1458     2           2.57            41  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "\n",
            "[1459 rows x 9 columns]\n",
            "[20.0, 40.0, 60.0, 80.0, 100.0]\n",
            "[5.0, 13.0, 27.0, 73.0, 10022.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[item] = s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                     user_id  group_id\n",
            "0     mCrhj_CG3_pOmDFcfKotRQ       4.0\n",
            "1     VlcasgkqiTuPi-nVT7rEtw       4.0\n",
            "2     xDl9ZF3SckkZde_48W6WeA       4.0\n",
            "3     TKwzreBMGFbu9zN6taeVLA       4.0\n",
            "4     GWOf1oB2mTalRY_A-EjSiQ       4.0\n",
            "...                      ...       ...\n",
            "1454  03_OcS8SbBcO4RTx_jYAUw       0.0\n",
            "1455  vpCdKYNXUjs5o-tVSmwGAg       0.0\n",
            "1456  vVBNWuQO9m5Utm69PGoRCw       1.0\n",
            "1457  h7zHYM8LbPWmXx0ZEqOOAw       2.0\n",
            "1458  4wbMeUS9tp2QGqyNhj3YTg       3.0\n",
            "\n",
            "[1459 rows x 2 columns]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.152471\n",
            "1        1  0.154758\n",
            "2        2  0.156611\n",
            "3        3  0.165196\n",
            "4        4  0.201368\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.157829\n",
            "1        1  0.163814\n",
            "2        2  0.168505\n",
            "3        3  0.163653\n",
            "4        4  0.176794\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0    0.1729\n",
            "1        1  0.161019\n",
            "2        2  0.166672\n",
            "3        3   0.16645\n",
            "4        4  0.162414\n",
            "\n",
            "\n",
            "[-0.2, 0.8, 1.8, 2.8, 3.8]\n",
            "[0.1524710892918509, 0.15475843181272583, 0.15661126750536433, 0.1651962903842412, 0.2013684739032754]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.188271\n",
            "1        1  0.213297\n",
            "2        2  0.237668\n",
            "3        3  0.303742\n",
            "4        4  0.730022\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0    0.2072\n",
            "1        1  0.236902\n",
            "2        2  0.271403\n",
            "3        3  0.308274\n",
            "4        4  0.737969\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.218987\n",
            "1        1  0.220126\n",
            "2        2  0.251935\n",
            "3        3  0.306586\n",
            "4        4   0.57224\n",
            "\n",
            "\n",
            "[-0.2, 0.8, 1.8, 2.8, 3.8]\n",
            "[0.18827137532904517, 0.21329684131047238, 0.23766751297800362, 0.3037417695657989, 0.7300222820901642]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX8ibEsAIxCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline1(business_id, users, reviews):\n",
        "  ### READ THE RANKING FROM CSV\n",
        "  df_rel_ranking = pd.read_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "  df_date_ranking = pd.read_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "  df_rand_ranking = pd.read_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = filter_user_from_dataset(df_rel_ranking, users)\n",
        "  df_date_ranking = filter_user_from_dataset(df_date_ranking, users)\n",
        "  df_rand_ranking = filter_user_from_dataset(df_rand_ranking, users)\n",
        "\n",
        "  df_rel_ranking = integrate_review_info(df_rel_ranking, reviews, business_id)\n",
        "  df_date_ranking = integrate_review_info(df_date_ranking, reviews, business_id)\n",
        "  df_rand_ranking = integrate_review_info(df_rand_ranking, reviews, business_id)  \n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline2(df_ranking_by_relevance, reviews, business, id):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  # ------ SINGLE ATTRIBUTE --------- \n",
        "  attribute = 'review_count'\n",
        "  df_groups = create_groups_by_percentile(df_ranking_by_relevance, attribute, 5, id)\n",
        "  method = attribute\n",
        "  \n",
        "  # ------ MULTIPLE ATTRIBUTES ---------\n",
        "  # TO BUILD VECTORS, it saves a csv file\n",
        "  # df_vectors = integrate_user_data(df_ranking_by_relevance, reviews, business, id)\n",
        "  # df_groups = create_groups_by_spectral_clustering(df_ranking_by_relevance, 5, id)\n",
        "  # method = 'spectral'\n",
        "  # df_groups = create_groups_by_kmeans_clustering(df_ranking_by_relevance, 5, id)\n",
        "  # method = 'kmeans'\n",
        "  print(df_groups)\n",
        "  return df_groups, method\n",
        "\n",
        "\n",
        "def pipeline3(business_id, method, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random, reviews):\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  print(\"----------------DEMOGRAPHIC PARITY EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_relevance, \"yelp_\")\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_date, \"date_\")\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_random, \"random_\")\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, 'plot_demgr_' + method + '_' + business_id)\n",
        "\n",
        "  print(\"----------------DISPARATE IMPACT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_relevance, reviews, \"yelp_\")\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_date, reviews, \"date_\")\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_random, reviews, \"random_\")\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, 'plot_dispimp_' + method + '_' + business_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRs30-Z9heM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_kmeans_clustering(df_users, N_of_groups, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  \n",
        "  # FOR NOW WE EXCLUDE TEXTUAL ATTRIBUTES\n",
        "  # text_attribute_list = ['top_location']\n",
        "  # df_vectors = one_hot_encoding(df_vectors, text_attribute_list)\n",
        "  \n",
        "  list_vectors = df_vectors[['review_count', 'fans', 'average_stars',\n",
        "                             'useful', 'funny', 'cool']].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  kmeans = KMeans(n_clusters=N_of_groups, random_state=0).fit(X)\n",
        "  labels = kmeans.labels_\n",
        "  # kmeans.predict([[0, 0, 20], [12, 3, 5]]))\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  np.savetxt(\"centroids.csv\", centroids, delimiter=\",\")\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = labels[i]\n",
        "  new_df_users.to_csv('groups_kmeans_' + id + '.csv')\n",
        "  return new_df_users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU_zGBsWzAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_spectral_clustering(df_users, N_of_groups, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['top_location']\n",
        "  df_vectors = one_hot_encoding(df_vectors, text_attribute_list)\n",
        "  \n",
        "  list_vectors = df_vectors[['review_count', 'fans', 'average_stars', 'top_location',\n",
        "                             'useful', 'funny', 'cool']].values.tolist()\n",
        "  # Convert list of list in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  mat = cosine_similarity(list_of_arrays)\n",
        "  group_array = SpectralClustering(n_clusters=N_of_groups, affinity='precomputed').fit_predict(mat)\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = group_array[i]\n",
        "  new_df_users.to_csv('groups_spectral_' + id + '.csv')\n",
        "  return new_df_users\n",
        "\n",
        "def one_hot_encoding(df, text_attribute_list):\n",
        "  for attr in text_attribute_list:\n",
        "    text = list(df[attr])\n",
        "    vectorizer = CountVectorizer().fit_transform(text)\n",
        "    embeddings = vectorizer.toarray()\n",
        "    for i, user in df.iterrows():\n",
        "      df.at[i, attr] = embeddings[i]\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awAE4AWWxEFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_percentile(df_users, attribute, N_of_groups, business_id):\n",
        "  values = df_users[attribute].tolist()\n",
        "\n",
        "  print(df_users)\n",
        "\n",
        "  # Create array of percents to define limit of group \n",
        "  scale = 100/N_of_groups\n",
        "  percents = []\n",
        "  current_percent = 0\n",
        "  while current_percent < 100:\n",
        "    current_percent = current_percent + scale\n",
        "    '''if current_percent > 100:\n",
        "      current_percent = 100\n",
        "      percents.append(current_percent)\n",
        "      break'''\n",
        "    percents.append(current_percent)\n",
        "  # ex. percents = [20, 40, 60, 80, 100]\n",
        "  print(percents)\n",
        "\n",
        "  # Create array with limit values (max) of each group\n",
        "  i = 0\n",
        "  limit_values = []\n",
        "  while i < len(percents):\n",
        "    limit_values.append(np.percentile(values, percents[i]))\n",
        "    i = i + 1\n",
        "  # ex. limit_values = [5.0, 13.0, 27.0, 73.0, 10022.0]\n",
        "  print(limit_values)\n",
        "  with open('descr_groups_' + attribute + '_' + business_id + '.txt', 'w') as output:\n",
        "    output.write(str(percents) + '\\n' + str(limit_values))\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  \n",
        "  for i, user in df_users.iterrows():\n",
        "    id_group = 0\n",
        "\n",
        "    if (user[attribute] >= 0) and (user[attribute] <= limit_values[id_group]):\n",
        "      new_df_users.loc[i, 'group_id'] = id_group\n",
        "    id_group = id_group + 1\n",
        "    while id_group < len(percents):\n",
        "      if (user[attribute] > limit_values[id_group - 1]) and (user[attribute] <= limit_values[id_group]):\n",
        "        new_df_users.loc[i, 'group_id'] = id_group\n",
        "      id_group = id_group + 1\n",
        "  \n",
        "  new_df_users.to_csv('groups_' + attribute + '_' + business_id + '.csv')\n",
        "  return new_df_users\n",
        "\n",
        "  '''\n",
        "  group_list = []\n",
        "  percentile_up_value = math.ceil(100/N_of_groups)\n",
        "  percentile_down_value = int(100/N_of_groups)\n",
        "  initial_percentile_value = percentile_down_value\n",
        "  index = 0\n",
        "  prec_max_value = -1\n",
        "\n",
        "  while percentile_up_value <= 100 or percentile_down_value <= 100:\n",
        "    if percentile_up_value > 100:\n",
        "      percentile_up_value = 100\n",
        "\n",
        "  while percentile_up_value <= 100 or percentile_down_value <= 100:\n",
        "    group_list.append([])\n",
        "    if percentile_up_value > 100:\n",
        "      percentile_up_value = 100\n",
        "    max_value_in_group = np.percentile(array_of_values, percentile_up_value)\n",
        "    for i, user in df_users.iterrows():\n",
        "      if user[attribute] <= max_value_in_group and user[attribute] > prec_max_value:\n",
        "        group_list[index].append(user)\n",
        "    index = index + 1\n",
        "    percentile_up_value = percentile_up_value + initial_percentile_value\n",
        "    percentile_down_value = percentile_down_value + initial_percentile_value\n",
        "    prec_max_value = max_value_in_group\n",
        "  return group_list\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbpLTiPSWuL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_disparate_impact_exposure(business_id, method,  df_ranking, reviews, filename):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp = disparate_impact_exposure(df_groups[df_groups['group_id'] == i], df_ranking, reviews, business_id)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  exposures.to_csv('exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures\n",
        "\n",
        "\n",
        "def print_demographic_parity_exposure(business_id, method, df_ranking, filename):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp = demographic_parity_exposure(df_groups[df_groups['group_id'] == i], df_ranking)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  exposures.to_csv('exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures\n",
        "\n",
        "\n",
        "def disparate_impact_exposure(df_group, ranking, reviews, business_id):\n",
        "    return dmp_sommatory(df_group, ranking, business_id, reviews)/(len(df_group.index))\n",
        "\n",
        "\n",
        "def demographic_parity_exposure(df_group, ranking):\n",
        "    return demgr_sommatory(df_group, ranking)/(len(df_group.index))\n",
        "\n",
        "\n",
        "def dmp_sommatory(df_group, ranking, business_id, reviews):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "\n",
        "        #Integrate reviews info for clicks counting\n",
        "        df_temp = reviews[(reviews['business_id'] == business_id) & (reviews['user_id'] == user_id)]\n",
        "        useful = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['useful']\n",
        "        funny = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['funny']\n",
        "        cool = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['cool']\n",
        "        counts = useful + funny + cool + 1\n",
        "\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        addend = exp(position) * counts\n",
        "        sum = sum + addend\n",
        "    return sum\n",
        "\n",
        "\n",
        "def demgr_sommatory(df_group, ranking):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        sum = sum + exp(position)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/np.log(1 + position)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPGOC05EljLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_plots(yelp_exposures, date_exposures, random_exposures, title):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  print(x1)\n",
        "  print(y1)\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='yelp')\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date')\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0))\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUr0D9xWp5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_size(df_users, attribute, range):\n",
        "    # order df_users by review_count\n",
        "    df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "    group_list = [[]]\n",
        "    i = 0\n",
        "    group_index = 0\n",
        "    prec = -1\n",
        "    for index, user in df_users.iterrows():\n",
        "        if i == range:\n",
        "            i = 0\n",
        "            group_list.append([])\n",
        "            group_index = group_index + 1\n",
        "        temp = user[attribute]\n",
        "        if temp != prec:\n",
        "            prec = temp\n",
        "            i = i + 1\n",
        "        group_list[group_index].append(user)\n",
        "    print(\"Number of groups created: \" + str(group_index+1))\n",
        "    return group_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYumnpYHecF",
        "colab_type": "code",
        "outputId": "e54f518e-730b-47d2-c8dd-a165910cde70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "### UNIQUE VALUE OF state, city AND categories OF BUSINESS\n",
        "# To filter only RESTAURANTS\n",
        "# is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "# restaurants = business[is_restaurant]\n",
        "\n",
        "print('UNIQUE VALUE OF \\'state\\' ATTRIBUTE IN RESTAURANTS = ', business['state'].nunique())\n",
        "print('UNIQUE VALUE OF \\'city\\' ATTRIBUTE IN RESTAURANTS = ', business['city'].nunique())\n",
        "df_cat = business[['categories']]\n",
        "distinct_categories_list = []\n",
        "for index, row in df_cat.iterrows():\n",
        "  if row['categories'] != None:\n",
        "    lst = [item.strip() for item in row['categories'].split(',')]\n",
        "    distinct_categories_list = list(set(distinct_categories_list + lst))\n",
        "print('UNIQUE VALUE OF \\'categories\\' ATTRIBUTE distinct values ', len(distinct_categories_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIQUE VALUE OF 'state' ATTRIBUTE IN RESTAURANTS =  36\n",
            "UNIQUE VALUE OF 'city' ATTRIBUTE IN RESTAURANTS =  1204\n",
            "UNIQUE VALUE OF 'categories' ATTRIBUTE distinct values  1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05AeyBn2gGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRINT THE state OF EACH user FOR HIS RESTAURANT reviews\n",
        "def statistics(users, reviews, business):\n",
        "  res = pd.DataFrame()\n",
        "  cont = 0\n",
        "  prec_user_id = '0'\n",
        "\n",
        "  for i, user in users.iterrows():\n",
        "    if cont == 1000:\n",
        "      break\n",
        "    user_id = user['user_id']  # 4, 9, 16, 21!, 22!, 23, 24\n",
        "    # Get all reviews of one user\n",
        "    # 'https://www.yelp.com/user_details_reviews_self?userid=NQffx45eJaeqhFcMadKUQA&rec_pagestart=90'??? no\n",
        "    user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "    # Get all restaurants\n",
        "    business_ids = user_reviews[['business_id']]\n",
        "    result = business_ids.merge(business)\n",
        "    is_restaurant = result['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "    user_restaurant_reviews = result[is_restaurant]\n",
        "    user_restaurant_reviews['user_id'] = user_id\n",
        "    #user_restaurant_reviews.to_csv('user_restaurant_reviews' + str(cont) + '.csv')\n",
        "    res = res.append(user_restaurant_reviews.groupby(['user_id', 'state'])['user_id'].count().reset_index(name=\"review_count\"))\n",
        "    if (user_id != prec_user_id) and not user_restaurant_reviews.empty:\n",
        "      cont = cont + 1\n",
        "    # print(res)\n",
        "    prec_user_id = user_id\n",
        "\n",
        "  res.to_csv('result.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0M0ETqjMrqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_df_users(ranking):\n",
        "  df_ranking = pd.DataFrame(ranking, columns=['position', 'user_id', 'review_count', 'date']).sort_values('position')\n",
        "  return df_ranking"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6gUlnqBZ9eoa",
        "colab": {}
      },
      "source": [
        "# TODO: get review_count from web page\n",
        "def get_ranking(url):\n",
        "\n",
        "  i=1\n",
        "  reviews_text = []\n",
        "  reviews_date = []\n",
        "  reviews_userid = []\n",
        "  x=0\n",
        "\n",
        "  while 1:\n",
        "    if x == 0:\n",
        "      page_content = requests.get(url)\n",
        "    else:\n",
        "        page_content = requests.get(url + '?start=' + str(x))\n",
        "    x = x + 20\n",
        "    \n",
        "    tree = html.fromstring(page_content.content)\n",
        "    recommended_reviews_text = \"Recommended Reviews\"\n",
        "    reviews_list = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/*' % recommended_reviews_text)\n",
        "    if not reviews_list:\n",
        "      break\n",
        "    else:\n",
        "      # Index for ranking\n",
        "      j=1\n",
        "      for review in reviews_list:\n",
        "          reviews_text.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[p]/p/span' % (recommended_reviews_text, j))[0].text))\n",
        "          reviews_date.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[1]/div/div[2]/span' % (recommended_reviews_text, j))[0].text))\n",
        "          user_link = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div/div/div/div/div/div/a/@href' % (recommended_reviews_text, j))[0]\n",
        "          reviews_userid.append((i,user_link[user_link.find('=')+1:]))\n",
        "          i = i + 1\n",
        "          j = j + 1\n",
        "\n",
        "\n",
        "  print(reviews_text)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_text)))\n",
        "  print(reviews_date)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_date)))\n",
        "  print(reviews_userid)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_userid)))\n",
        "  return reviews_userid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHl6bORqNzeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_users_in_dataset(users):\n",
        "    data = []\n",
        "    with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            data.append(json_data)\n",
        "    df = pd.DataFrame(data)\n",
        "    df_ranking = pd.DataFrame(users, columns=['Position', 'user_id'])\n",
        "    df_merged = df_ranking.merge(df, on='user_id')\n",
        "    # temp = df[df['user_id'].isin(users)]\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def exists_in_dataset(userid):\n",
        "  found = False\n",
        "  with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            if json_data['user_id'] == userid:\n",
        "              found = True\n",
        "  if found:\n",
        "    print(\"Trovato\")\n",
        "  else:\n",
        "    print(\"Non trovato\")\n",
        "  return found\n",
        "\n",
        "\n",
        "def find_users_in_chopped_dataset(users):\n",
        "    found = False\n",
        "    data = []\n",
        "    i = 1\n",
        "    j = 100000\n",
        "    df_users = pd.DataFrame()\n",
        "    while not found:\n",
        "        path = \"user\" + str(i) + \"-\" + str(j) + \".json\"\n",
        "        try:\n",
        "            with open(path, 'r') as file:\n",
        "                for line in file:\n",
        "                    json_data = json.loads(line)\n",
        "                    data.append(json_data)\n",
        "            df = pd.DataFrame(data)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Cannot find all users into the dataset\")\n",
        "            return 0\n",
        "        else:\n",
        "            temp = df[df['user_id'].isin(users)]  # df of users of the ranking\n",
        "            df_users = df_users.append(temp)\n",
        "            if df_users.shape[0] == len(users):\n",
        "                found = True\n",
        "            else:\n",
        "                i = i + 100000\n",
        "                j = j + 100000\n",
        "    return df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}