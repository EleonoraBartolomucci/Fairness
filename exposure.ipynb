{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exposure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/exposure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPpCkgv0271l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import datetime\n",
        "import math\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#!pip install balanced_kmeans\n",
        "#from balanced_kmeans import kmeans\n",
        "#from balanced_kmeans import kmeans_equal\n",
        "\n",
        "import networkx as nx\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkoXGrGIEQB3",
        "colab_type": "code",
        "outputId": "52c07dbf-9541-45a7-9f3d-2deb9748e833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COSfvffRFRRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "root_path = 'gdrive/My Drive/Tesi/Fairness/data/fairness_data/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7PR_gGjwNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONSTANTS\n",
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n",
        "FOLDER = {'WbJ1LRQdOuYYlRLyTkuuxw':\n",
        "            {'FID_RANKING': '1U0Ml5puObfOk3qHCKhO9bQ06WyhJoffW',\n",
        "             'FID_G1': '1cauvqKXzF5VhiveP84aL3U0oDrXP32XV',\n",
        "             'FID_G2': '1vC1fc2owpCis_PTl1ABKT6BfG6-2yoiJ',\n",
        "             'FID_G3': '1_iKbmJkBwWEEIIh73UvR431Ss6xnGb2f',\n",
        "             'FID_G4': '1m_feamMvpx5B5mTe_lH6qlhoZFEpwlSs',\n",
        "             'FID_G5': '1qJgFbR4FUX5t7O9h5cHgp6aE6lpfbdOs',\n",
        "             'FID_G6': '1BSQrghVusO_eNbe10fFE4-bVfRK8ly0y',\n",
        "             'FID_G7': '1fJD4WIErH-u5plesyTePCAQ3IyfjoOKf',\n",
        "             'FID_G8': '1fD_fdl-1KAzXCjPOsEvFOZS3u354XhLL',\n",
        "             'FID_G9': '1ZdiaS2cZP9Ks3rheSSVaoLpw-qMYF9EA'\n",
        "             },\n",
        "          'T2tEMLpTeSMxLKpxwFdS3g':\n",
        "            {'FID_RANKING': '136FZ0Y90Zx-4UYDyW50cX8zUa_DhN3XS',\n",
        "             'FID_G1': '1eiju1aTzsRi4QQZ8XrSouqcLtFSN-tbj',\n",
        "             'FID_G2': '1VuU8MAcXgN_E85q9SH_P1HsHVkeJakwH',\n",
        "             'FID_G3': '1L240ycUElvmF0R4l8PbnuoBbqQY6q1og',\n",
        "             'FID_G4': '1S4oTf-ZSD8q9aTOjrDEA2KeNJ99uh-2n',\n",
        "             'FID_G5': '1u9ZFeCwmol1uXRH-l8djJ2hst0B01VI3',\n",
        "             'FID_G6': '19TgwV3uHOtDDJ0a8krpvLUeooe9rhQFJ',\n",
        "             'FID_G7': '11YpFHw34cDglPD87W8yWvRM1VAOWXPtF',\n",
        "             },\n",
        "          'ALwAlxItASeEs2vYAeLXHA':\n",
        "            {'FID_RANKING': '12zXi3XyQaNgukGHW_805cyrkHhSdF7Df',\n",
        "             'FID_G1': '1NB-isOm1cDArAlwIMDMou1Q_XAap09KA',\n",
        "             'FID_G2': '1ELMUBbKGryblHDnbh7ghiIgMdZ48pBG1',\n",
        "             'FID_G3': '1QZ90pwDjBoaU41P8wP8v_I9tsuhObEUP',\n",
        "             'FID_G4': '1MXivbV9VGB0vBGGuLhYu0zJlpv6O-2Xb',\n",
        "             'FID_G5': '1bTetFx3Jhy5QbRKYGV09muMy7D1NKmJT',\n",
        "             'FID_G6': '1F_7bdMS5MW4sIpyUbDz62ZTYJtHoouGG',\n",
        "             'FID_G7': '1uRFceNIlYk5vQXEGd2H6yanrlwrSIM9V',\n",
        "             },\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw':\n",
        "            {'FID_RANKING': '1ZyNQavpG0akr3ca3PJjjl_D89IA5wlcc',\n",
        "             'FID_G1': '1kQkAi_9V3mSld1LlP50j6uw0bwqnkLrR',\n",
        "             'FID_G2': '1hGR4eZiP3Q9Etn7RdlY58IsNsTMMTydz',\n",
        "             'FID_G3': '1YcAPHxioFpiVXWndO5tK17US1h36cWgK',\n",
        "             'FID_G4': '1NxzzH0l18fGaO4bZ35D9NMq2CeW5BRVP',\n",
        "             'FID_G5': '1HC1wZGPtbb8UvRlfbGoHl_ijl3G67Ei4',\n",
        "             'FID_G6': '1J_15pO-CFbHR_0YsHK5kDnKp49kDL0kZ',\n",
        "             'FID_G7': '1RTwCqhYzB3djzpe3-uwbQ3Ur5P8YoECa',\n",
        "             },\n",
        "          'Sovgwq-E-n6wLqNh3X_rXg':\n",
        "            {'FID_RANKING': '1EZqvt9x5PN07BgUad1RtNIUXTVqujA0g',\n",
        "             'FID_G1': '1pkUEBYeZ66GfIvXoTkzZ28gGFlZNWmEq',\n",
        "             'FID_G2': '1lJzHAfBlvL9_EgNizH4vnyPTn-j-Vbrx',\n",
        "             'FID_G3': '1NRYqjUsQJTwgB0JXtcCx7lQErirMH1TK',\n",
        "             'FID_G4': '1nLtZbS0NGXKAs6aETNs7EWasNs8omaw6',\n",
        "             'FID_G5': '14w-jVhOuod3s_EYSipvS92SPx_3zT_FD',\n",
        "             'FID_G6': '1tbUyiamFmyAoeA3W6j51x97h3MjUSVsd',\n",
        "             'FID_G7': '1TNAmibHwJHkvNajHhRffN1gEY0-QnBZH',\n",
        "             },\n",
        "          'j5nPiTwWEFr-VsePew7Sjg':\n",
        "            {'FID_RANKING': '18bQVXYZ03vIpfEFLPh8I2i7cPTlokGxv',\n",
        "             'FID_G1': '1U8STZ7irZLPcUpP1ALR6QYIFnoXB2tbE',\n",
        "             'FID_G2': '1xScKc0_DlnQZucHLeb26hLqpgjqMA3RH',\n",
        "             'FID_G3': '1JFYl2eKtDyWbsyVdBpTXsvPn2h6MO3Cr',\n",
        "             'FID_G4': '1aFYpJmymdnHxquuqbbMwXzN5wG-3Zf2O',\n",
        "             'FID_G5': '1NfHlsBTk87jscmZfagtnd--kcdpy8t_8',\n",
        "             'FID_G6': '1b88afNqrZSRptYA0pL3lCEV49mFaEP6Q',\n",
        "             'FID_G7': '15rDbY-MTUBVjHxOedbenZIjTxQ_wEAZW',\n",
        "             },\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg':\n",
        "            {'FID_RANKING': '1nl6A997UnuR5ceYRZS1242JJCvK',\n",
        "             'FID_G1': '1hrssLlhPmnS48yVxXpwBBIuhL1YdDNhj',\n",
        "             'FID_G2': '1shLPc3aUsVaxeZhZ_SvDS3agPFlMw1dn',\n",
        "             'FID_G3': '13eUtYFVp-KKjxV6-mhCsxQGHkKFOgB82',\n",
        "             'FID_G4': '1nQVPHZT4sHpBXD32qX_x41VywkmyppFj',\n",
        "             'FID_G5': '1gJpfGhb721Q64gynq4jSGMis5pDA7qAx',\n",
        "             'FID_G6': '1WbZvolaSwambWOKHx4v4eFIV-KUzygGF',\n",
        "             'FID_G7': '1IWyQpgMjLny2JTbw0nUuHkNRPV5CCA_A',\n",
        "             },\n",
        "          'e4NQLZynhSmvwl38hC4m-A':\n",
        "            {'FID_RANKING': '1K566Y5Q2N6Lw7S6yDicKC_R-zFVRqnFr',\n",
        "             'FID_G1': '1RNxub2faGfc4NAFvau5SsMADAzBlQd2j',\n",
        "             'FID_G2': '16tjv4k5CJSOxwgvC0wDJU67MEjRaw_0o',\n",
        "             'FID_G3': '1p-9HrVncD_FZOxO0YzHiY7KTe47ugSTQ',\n",
        "             'FID_G4': '1qGpECepAyJjqyfPx8Q94odPOCHbOixtZ',\n",
        "             'FID_G5': '15AGLE1YT0v8tQ3r6i6k7j5XaNORToUct',\n",
        "             'FID_G6': '1nNbAPulqJCzO6urHNsqc4qzSM7Yih8uk',\n",
        "             'FID_G7': '1o9Ew39-Q9VO3wjEyv4qGJ9YlfRmkzTed',\n",
        "             },\n",
        "          'S-oLPRdhlyL5HAknBKTUcQ':\n",
        "            {'FID_RANKING': '1FJqfJgaJinXS3oIvdA53FMuO9O7bCDVn',\n",
        "             'FID_G1': '1CcLHVEA41AgSzJKl37YlcJKf-o5NaCUo',\n",
        "             'FID_G2': '1j-lp1gcu6YGTwcOV4aYiY7dh2G9_TL6J',\n",
        "             'FID_G3': '1I-Qn9oz4VopXbpj5i8_iqLvcSp8jpb2_',\n",
        "             'FID_G4': '1gsS9uFbRyI18ifF5yLp20WHhP66gdr57',\n",
        "             'FID_G5': '1LJIVm9tCZMcgzBkGilYem0nqKoUHpdpX',\n",
        "             'FID_G6': '1syB0wF48fiSo0_uW9dedS-ZlFfXUv1B8',\n",
        "             'FID_G7': '1cBPjBprJ19RxI6-Bcpb-Fifv0s1bQSV2',\n",
        "             }\n",
        "        }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bw0Twhc3qgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upload_file(filename, folder_id):\n",
        "  drive = authenticate()\n",
        "  fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "  drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Check if file already exists in Google Drive (prevents duplicates)\n",
        "  for file in fileList:\n",
        "      if file['title'] == filename:  # The file already exists, then overwrite it\n",
        "          fileID = file['id']\n",
        "          drive_file = drive.CreateFile({'id': fileID, 'title': filename, 'parents': [{'id': folder_id}]})\n",
        "\n",
        "  # Create a local copy of user picture\n",
        "  # Already created!\n",
        "\n",
        "  # Upload user picture on Google Drive\n",
        "  drive_file.SetContentFile(filename)  # path of local file content\n",
        "  drive_file.Upload()  # Upload the file.\n",
        "  \n",
        "  # Delete local user pictures\n",
        "  #if os.path.exists(filename):\n",
        "  #    os.remove(filename)\n",
        "  #else:\n",
        "  #    print(\"The file does not exist\")\n",
        "\n",
        "def set_file_destination(lst, method, id):\n",
        "  if lst == ['review_count']:\n",
        "    return FOLDER[id]['FID_G1']\n",
        "  if method == 'kmeans':\n",
        "    if lst == ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']:\n",
        "      return FOLDER[id]['FID_G2']\n",
        "    if lst == ['loc1', 'loc2', 'loc3']:\n",
        "      return FOLDER[id]['FID_G3']\n",
        "    if lst == ['age', 'gender', 'ethnicity']:\n",
        "      return FOLDER[id]['FID_G4']\n",
        "    if lst == ['fans']:\n",
        "      return FOLDER[id]['FID_G6']\n",
        "    if lst == ['useful','funny', 'cool']:\n",
        "      return FOLDER[id]['FID_G7']\n",
        "  if method == 'balanced_kmeans':\n",
        "    if lst == ['age', 'gender', 'ethnicity']:\n",
        "      return FOLDER[id]['FID_G8']\n",
        "    if lst == ['loc1', 'loc2', 'loc3']:\n",
        "      return FOLDER[id]['FID_G9']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JgvcYwsfpy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# READ JSON\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyIm6NR55FGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "drive = authenticate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrfK_M25rAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD user.json FROM DRIVE\n",
        "users_dataset_id = '1JokoV68YD5Iq2l4Y_IV2RJzBpD_mSCyq'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': users_dataset_id})\n",
        "download.GetContentFile('user.json')\n",
        "users = read_json('user.json')\n",
        "users = pd.DataFrame(users)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMGOW-SHRrP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD business.json FROM DRIVE\n",
        "business_dataset_id = '1Qoy132gb205xAIFjkBbZ2CyYDeaz9yqU'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': business_dataset_id})\n",
        "download.GetContentFile('business.json')\n",
        "business = read_json('business.json')\n",
        "business = pd.DataFrame(business)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tkQRFfw16x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD review.json FROM DRIVE\n",
        "review_dataset_id = '1mW1WbpMFjN0qQpLnM-R_TzNpAkFsBLHQ'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': review_dataset_id})\n",
        "download.GetContentFile('review.json')\n",
        "reviews = read_json('review.json')\n",
        "reviews = pd.DataFrame(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYF2ESayjKVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD DEMOGRAPHICS\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A']\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "file_id_list = {'WbJ1LRQdOuYYlRLyTkuuxw':['13amOFvuku27snF8mea8Yp7rH36Mve42N',\n",
        "                                          '1VskD_0Ijwe3_fzVbgYqK9Bk-YOCK046t',\n",
        "                                          '1uuA0QH-DJklvtiekxS-7yaXfKRi9ux06'],\n",
        "                'T2tEMLpTeSMxLKpxwFdS3g':['11fM_aHYkCLQ2z3g7Z76aZZ3cAsIrpezN',\n",
        "                                          '1TPexYC2YHq_8ywZSuRRIJkrUCstwoxSp',\n",
        "                                          '1ObvjBbxRNIp1vOQipUGZxZEi_eQH7kJQ'],\n",
        "                'ALwAlxItASeEs2vYAeLXHA':['1crTNpDR2VjBdGSgavIvN4QNhwTUR3-Bk',\n",
        "                                          '1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk',\n",
        "                                          '1tgJnMW0ZImx1vbDzWpvFBRX0FJu1Kdbt'],\n",
        "                'OVTZNSkSfbl3gVB9XQIJfw':['1Vmkzpdi_0m9CGcyp5ggeNsUsueBzqufH',\n",
        "                                          '12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF',\n",
        "                                          '1qecYhCBytq1Z6lm_ueCWIuithvRG0wIq'],\n",
        "                'Sovgwq-E-n6wLqNh3X_rXg':['1VVePrvWR7e5XkRmhp5Q5qOPXPskVmnnt',\n",
        "                                          '1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT',\n",
        "                                          '1R5GFYo2c2YC_AOB9G0pdOfROTyglePDI'],\n",
        "                'j5nPiTwWEFr-VsePew7Sjg':['1txkAzGKMRjXku18rv9222q-FqVo42A9o',\n",
        "                                          '1e8iSM3SpyB2Y_h6rzRSVVgphjHEIJnZU',\n",
        "                                          '16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                'aiX_WP7NKPTdF9CfI-M-wg':['1VrVgBoJp5cRVMH4I-5uKvdF3b-PSfbmL',\n",
        "                                          '1f7T7ksdCCGxhVF_RJU9zu38-r8aTTJcD',\n",
        "                                          '1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                'e4NQLZynhSmvwl38hC4m-A':['1yYG8ftjE1i9prP61ej5EdGfJDlweiK3C',\n",
        "                                          '1KSdN24eXC-Bnk2_mTQR6oNaGll1KP5hO',\n",
        "                                          '1PA2eYsYj2bj4Mo7ofbnvbQgYjmd2SwRG'],\n",
        "                'S-oLPRdhlyL5HAknBKTUcQ':['1qcX9pGfmogOokbBOPnZY-NbroolffAlg',\n",
        "                                          '1YVdSND3VNit4ieORXC_mVzcv4mphGUnY',\n",
        "                                          '1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "for business_id in id_list:\n",
        "  df = pd.DataFrame()\n",
        "  for file_id in file_id_list[business_id]:\n",
        "    download = drive.CreateFile({'id': file_id})\n",
        "    download.GetContentFile('demographics_' + business_id + '.csv')\n",
        "    df_temp = pd.read_csv('demographics_' + business_id + '.csv')\n",
        "    df = df.append(df_temp)\n",
        "  df = df.drop_duplicates('id',keep='first').reset_index(drop=True)\n",
        "  df = df.rename(columns={'id':'user_id'})\n",
        "  df.to_csv('demographics_' + business_id + '.csv')\n",
        "  upload_file('demographics_' + business_id + '.csv',FOLDER[business_id]['FID_G4'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40JdFbw0Qqmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FILTER RESTAURANTS\n",
        "is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "restaurants = business[is_restaurant]\n",
        "\n",
        "# Filter closed restaurants\n",
        "restaurants = restaurants[restaurants['is_open'] == 1]\n",
        "\n",
        "# Order restaurants by review_count\n",
        "restaurants = restaurants.sort_values('review_count')\n",
        "\n",
        "restaurants.to_csv('restaurants_input.csv')\n",
        "# NOW CHOOSE THE RESTAURANTS AND SAVE THEIR IDS INTO TXT FILE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYSaLtPHqsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXEC IN LOCAL THE DOWNLOAD OF REVIEW RANKINGS OF EACH RESTAURANT\n",
        "# UPLOAD CSV IN COLAB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbM06dpOYYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################# EXECUTE IN LOCAL (if not blocked by yelp) #############################################\n",
        "def get_ranking_from_call(url_business, lang, sort, query):\n",
        "    headers = [{\"name\": \"Accept\", \"value\": \"*/*\"}, {\"name\": \"Accept-Encoding\", \"value\": \"gzip, deflate, br\"},\n",
        "               {\"name\": \"Accept-Language\", \"value\": \"it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3\"},\n",
        "               {\"name\": \"Connection\", \"value\": \"keep-alive\"},\n",
        "               {\"name\": \"Content-Type\", \"value\": \"application/x-www-form-urlencoded; charset=utf-8\"}, {\"name\": \"Cookie\",\n",
        "                                                                                                       \"value\": \"qntcst=D; hl=en_US; wdi=1|3C26116D69138F61|0x1.78d019f71a444p+30|a7756ff94751d3a9; _ga=GA1.2.3C26116D69138F61; location=%7B%22city%22%3A+%22New+York%22%2C+%22state%22%3A+%22NY%22%2C+%22country%22%3A+%22US%22%2C+%22latitude%22%3A+40.713%2C+%22longitude%22%3A+-74.0072%2C+%22max_latitude%22%3A+40.8523%2C+%22min_latitude%22%3A+40.5597%2C+%22max_longitude%22%3A+-73.7938%2C+%22min_longitude%22%3A+-74.1948%2C+%22zip%22%3A+%22%22%2C+%22address1%22%3A+%22%22%2C+%22address2%22%3A+%22%22%2C+%22address3%22%3A+null%2C+%22neighborhood%22%3A+null%2C+%22borough%22%3A+null%2C+%22provenance%22%3A+%22YELP_GEOCODING_ENGINE%22%2C+%22display%22%3A+%22New+York%2C+NY%22%2C+%22unformatted%22%3A+%22New+York%2C+NY%2C+US%22%2C+%22accuracy%22%3A+4.0%2C+%22language%22%3A+null%7D; xcj=1|Ptt9P03gfc75x_PBT9zmqCkUuSuyB7PR-wWUBvABNi4; __qca=P0-60561249-1581956668708; G_ENABLED_IDPS=google; __cfduid=db8764ff59d8028a6c2e1b214867927d81583160194; _gid=GA1.2.2014867238.1583835527; bse=05dcd9d5de304ef0b1d9a76fa768b10f; sc=8a1ca0dbc2; pid=505721aa4569e7bb\"},\n",
        "               {\"name\": \"Host\", \"value\": \"www.yelp.com\"},\n",
        "               {\"name\": \"Referer\", \"value\": \"https://www.yelp.com/biz/noche-de-margaritas-new-york\"},\n",
        "               {\"name\": \"TE\", \"value\": \"Trailers\"}, {\"name\": \"User-Agent\",\n",
        "                                                     \"value\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0\"},\n",
        "               {\"name\": \"X-Requested-By-React\", \"value\": \"true\"},\n",
        "               {\"name\": \"X-Requested-With\", \"value\": \"XMLHttpRequest\"}]\n",
        "    headers_ok = {}\n",
        "    for header in headers:\n",
        "        temp = {\n",
        "            header['name']: header['value']\n",
        "        }\n",
        "        headers_ok.update(temp)\n",
        "\n",
        "    x = 0\n",
        "    reviews_list = []\n",
        "    position = 1\n",
        "    url = url_business + \"/review_feed?rl=\" + lang + \"&sort_by=\" + sort + \"&q=\" + query\n",
        "\n",
        "    while 1:\n",
        "        if x == 0:\n",
        "            page_load = requests.get(url + '&start=', headers=headers_ok)\n",
        "        else:\n",
        "            page_load = requests.get(url + '&start=' + str(x), headers=headers_ok)\n",
        "        print(page_load)\n",
        "        x = x + 20\n",
        "        reviews = page_load.json()['reviews']\n",
        "        # print(json.dumps(reviews, indent=4, sort_keys=True))\n",
        "        if not reviews:\n",
        "            break\n",
        "        for review in reviews:\n",
        "            reviews_list.append((position, review['userId'], review['user'],#['reviewCount'],\n",
        "                                 datetime.datetime.strptime(review['localizedDate'], '%m/%d/%Y')))\n",
        "            position = position + 1\n",
        "    df_reviews = pd.DataFrame(reviews_list, columns=[\"position\", \"user_id\", \"user\", \"date\"])\n",
        "    return df_reviews\n",
        "\n",
        "\n",
        "def retrieve_rankings(business_id):\n",
        "    df_rel_ranking = get_ranking_from_call(\"https://www.yelp.com/biz/\" + business_id, \"en\", \"relevance_desc\", \"\")\n",
        "    df_date_ranking = df_rel_ranking.sort_values(by=['date']).reset_index(drop=True)\n",
        "    df_date_ranking['position'] = df_date_ranking.index + 1\n",
        "    df_rand_ranking = df_rel_ranking.sample(frac=1).reset_index(drop=True)\n",
        "    df_rand_ranking['position'] = df_rand_ranking.index + 1\n",
        "\n",
        "    df_rel_ranking.to_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "    df_date_ranking.to_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "    df_rand_ranking.to_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "\n",
        "\n",
        "rest_ids = []\n",
        "\n",
        "with open('rest_ids.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    rest_ids.append(line[:-1])\n",
        "\n",
        "print(rest_ids)\n",
        "for id in rest_ids:\n",
        "    retrieve_rankings(id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdV4KHSkePdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ALTERNATIVE METHOD TO SHUFFLE THE RANKING FOR RANDOM\n",
        "#see https://gist.github.com/cadrev/6b91985a1660f26c2742\n",
        "business_id = 'WbJ1LRQdOuYYlRLyTkuuxw'\n",
        "df_rel_ranking = pd.read_csv('rel_ranking_' + business_id + '.csv')\n",
        "df_date_ranking = df_rel_ranking.sort_values(by=['Date']).reset_index(drop=True)\n",
        "df_date_ranking['Position'] = df_date_ranking.index + 1\n",
        "df_rand_ranking = df_rel_ranking\n",
        "\n",
        "#random.shuffle(df_rand_ranking) DOESN'T WORK KEY ERROR\n",
        "df_rand_ranking = df_rand_ranking.reindex(np.random.permutation(df_rand_ranking.index))\n",
        "\n",
        "df_rand_ranking = df_rand_ranking.reset_index(drop=True)\n",
        "\n",
        "# drop all the unnamed columns\n",
        "cols = [c for c in df_rand_ranking.columns if c.lower()[:7] != 'unnamed']\n",
        "df_rel_ranking=df_rel_ranking[cols]\n",
        "df_rand_ranking=df_rand_ranking[cols]\n",
        "df_date_ranking=df_date_ranking[cols]\n",
        "\n",
        "df_rand_ranking['Position'] = df_rand_ranking.index + 1\n",
        "\n",
        "print(df_rand_ranking)\n",
        "\n",
        "df_rel_ranking.to_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "df_date_ranking.to_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "df_rand_ranking.to_csv(\"rand_ranking_\" + business_id + \".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070u7qOzHsiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# JOIN USER FROM YELP WITH USER FROM DATASET AND PRINT LOST USERS\n",
        "def filter_user_from_dataset(df, users):\n",
        "  df = df.rename(columns={'Position':'position','User_Id': 'user_id',\n",
        "                          'Date':'date'})\n",
        "  df = df[['position','user_id','date']]\n",
        "  df_merged = df.merge(users, on='user_id')\n",
        "  total_review = df['position'].max()\n",
        "  print('Utenti persi: totali ' + str(total_review) + ' - utenti nel dataset ' +\n",
        "        str(len(df_merged.index)) + ' = ' + str(total_review - len(df_merged.index)))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "\n",
        "  cols = df_merged.columns.tolist()\n",
        "\n",
        "  df_merged = df_merged[['position', 'user_id', 'date', 'fans', 'average_stars', 'review_count']]\n",
        "\n",
        "  return df_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RAaBHLaVMDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADD REVIEW INFO IN RANKING DATAFRAME\n",
        "def integrate_review_info(df, reviews, business_id):\n",
        "  business_reviews = reviews[reviews['business_id'] == business_id]\n",
        "  df_merged = business_reviews.merge(df, on='user_id')\n",
        "  del df_merged['date_y']  # it's date from yelp website (not updated in dataset)\n",
        "  df_merged = df_merged.rename(columns={'date_x':'date'})\n",
        "  # drop duplicates reviews from same user\n",
        "  df_merged = df_merged.sort_values('date').drop_duplicates('user_id',keep='last')\n",
        "  df_merged = df_merged.sort_values(by=['position']).reset_index(drop=True)\n",
        "  print('Review perse: ', len(df.index) - len(df_merged.index))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "\n",
        "  df_merged = df_merged[['position', 'user_id', 'review_id', 'date', 'text', 'fans', 'average_stars', 'review_count', 'business_id']]\n",
        "  \n",
        "  return df_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezxCXIZBkcAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADD USER DATA TO GROUP USERS\n",
        "def create_vectors(df_users, reviews, business, business_id, list_of_attributes, method):\n",
        "  vectors = df_users[['user_id', 'review_count', 'fans' , 'average_stars']]\n",
        "  vectors[\"useful\"] = np.NaN\n",
        "  vectors[\"funny\"] = np.NaN\n",
        "  vectors[\"cool\"] = np.NaN\n",
        "  vectors[\"loc1\"] = np.NaN\n",
        "  vectors[\"loc2\"] = np.NaN\n",
        "  vectors[\"loc3\"] = np.NaN\n",
        "  for index, user in vectors.iterrows():\n",
        "    print(index)\n",
        "    # TOP THREE LOCATION\n",
        "    location_list = get_top_three_loc(user, reviews, business)\n",
        "    i = 0\n",
        "    while i < len(location_list):\n",
        "      vectors.loc[index, 'loc'+str(i+1)] = location_list[i]\n",
        "      i = i + 1\n",
        "\n",
        "    # USEFUL FUNNY COOL\n",
        "    useful, funny, cool = get_details_user(user, reviews)\n",
        "    vectors.loc[index, 'useful'] = useful\n",
        "    vectors.loc[index, 'funny'] = funny\n",
        "    vectors.loc[index, 'cool'] = cool\n",
        "\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "    \n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "\n",
        "def get_demographics(vectors, id):\n",
        "  df_demographics = pd.read_csv('demographics_' + id + '.csv')\n",
        "  new_vectors = pd.merge(vectors, df_demographics, on='user_id', how='left')\n",
        "  return new_vectors\n",
        "\n",
        "\n",
        "def get_details_user(user, reviews):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  return user_reviews['useful'].sum(), user_reviews['funny'].sum(), user_reviews['cool'].sum()\n",
        "\n",
        "\n",
        "def get_top_three_loc(user, reviews, business):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  result = user_reviews.merge(business, on='business_id')\n",
        "  top_loc = result['state'].value_counts().index.tolist()[:3]\n",
        "  top_zip = []\n",
        "  for location in top_loc:\n",
        "    # exclude postal_code of Canada, that is strings\n",
        "    temp_list = result[result['state']==location]['postal_code'].value_counts().index.tolist()\n",
        "    if temp_list != []:\n",
        "      temp_list = [elem for elem in temp_list if str(elem).isdigit()]\n",
        "      if temp_list != []:\n",
        "        top_zip.append(int(str(temp_list[0])[:3]))\n",
        "  return top_zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY33HeAn7Xq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_groups_percents(df_groups, N_of_groups):\n",
        "  total = len(df_groups.index)\n",
        "  percents = []\n",
        "  i = 0\n",
        "  while i < N_of_groups:\n",
        "    current_length = len(df_groups[df_groups['group_id'] == i].index)\n",
        "    percents.append((current_length/total)*100)\n",
        "    i = i + 1\n",
        "  print(percents)\n",
        "  return percents\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW2AKZ8lHgoB",
        "colab_type": "code",
        "outputId": "07a43359-668d-410b-a278-9f08f5bc1b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PIPELINE\n",
        "N_of_groups = 5\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "#          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A']\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "\n",
        "#list_of_attributes = ['review_count']\n",
        "#list_of_attributes = ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']\n",
        "#list_of_attributes = ['loc1', 'loc2', 'loc3']\n",
        "list_of_attributes = ['age', 'gender', 'ethnicity']\n",
        "#list_of_attributes = ['fans']\n",
        "#list_of_attributes = ['useful', 'funny', 'cool']\n",
        "#list_of_attributes = ['age', 'ethnicity']\n",
        "method = 'balanced_kmeans'\n",
        "\n",
        "for id in id_list:\n",
        "  authenticate()\n",
        "  #df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1(id, users, reviews)\n",
        "  df_rel_ranking = pd.read_csv(\"dataset_rel_ranking_\" + id + \".csv\")\n",
        "  df_date_ranking = pd.read_csv(\"dataset_date_ranking_\" + id + \".csv\")\n",
        "  df_rand_ranking = pd.read_csv(\"dataset_rand_ranking_\" + id + \".csv\")\n",
        "  group_list, percents = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                           N_of_groups, list_of_attributes, method)\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes)\n",
        "  "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "++++++++++++++++ GROUPS CREATION ++++++++++++++++++\n",
            "\n",
            "Centroids:  [[3.90000000e+01 5.73033708e-01 4.26966292e-01 0.00000000e+00\n",
            "  2.64044944e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  7.35955056e-01]\n",
            " [4.90898876e+01 5.73033708e-01 4.26966292e-01 4.49438202e-02\n",
            "  1.06741573e-01 1.01123596e-01 2.24719101e-02 5.61797753e-03\n",
            "  7.19101124e-01]\n",
            " [3.58370787e+01 6.68539326e-01 3.31460674e-01 2.69662921e-01\n",
            "  1.06741573e-01 2.02247191e-01 1.12359551e-02 5.61797753e-03\n",
            "  4.04494382e-01]\n",
            " [2.23333333e+01 8.05555556e-01 1.94444444e-01 1.11111111e-01\n",
            "  2.38888889e-01 1.11111111e-01 0.00000000e+00 5.55555556e-03\n",
            "  5.33333333e-01]]\n",
            "labels:  [3 3 0 3 0 3 1 2 3 0 0 2 2 0 1 1 2 2 2 0 3 1 2 1 1 2 1 1 2 3 2 3 1 1 3 0 0\n",
            " 3 2 0 2 0 0 2 3 2 2 3 1 0 2 1 1 2 3 3 0 2 2 0 3 2 0 0 1 3 1 3 2 0 3 1 1 2\n",
            " 3 0 1 0 1 2 1 2 0 3 1 2 3 2 0 1 0 3 2 1 3 0 0 0 1 3 2 2 0 0 3 1 0 0 2 3 2\n",
            " 3 2 2 1 3 1 0 2 2 0 3 3 2 0 1 0 1 2 3 3 1 3 0 3 3 2 3 3 3 2 2 1 3 1 2 3 2\n",
            " 0 2 0 3 3 3 1 3 2 2 2 0 2 1 3 1 3 0 3 2 2 1 3 0 1 3 2 1 3 2 2 2 0 2 2 1 2\n",
            " 3 1 3 2 3 2 0 2 3 3 2 2 3 2 3 3 3 1 1 3 1 2 0 0 1 0 3 0 2 1 1 0 2 3 3 0 2\n",
            " 3 2 2 1 1 0 1 0 2 1 2 3 3 3 3 0 0 1 2 2 0 3 0 0 0 2 3 0 3 0 2 0 3 0 0 1 0\n",
            " 1 0 1 3 2 1 1 3 3 1 0 1 0 0 1 1 1 2 2 2 3 0 3 0 1 0 0 3 2 2 0 1 3 3 0 2 1\n",
            " 2 2 1 3 3 2 2 2 2 0 3 3 0 3 1 1 0 1 0 2 0 1 3 3 3 1 2 0 3 0 1 3 3 1 1 1 2\n",
            " 1 1 1 2 0 3 3 0 3 2 2 2 1 2 2 2 1 3 1 3 2 0 1 1 1 2 2 0 2 2 0 1 3 2 0 0 3\n",
            " 3 0 0 0 3 3 1 1 3 0 1 2 3 3 1 1 0 1 0 1 0 1 0 0 3 3 3 2 0 0 1 1 3 3 1 0 1\n",
            " 2 2 0 3 0 0 2 1 0 0 2 2 2 3 2 0 0 2 2 1 3 2 3 0 3 2 0 1 2 2 3 0 0 1 1 1 3\n",
            " 0 3 1 2 0 1 1 2 0 1 1 3 1 3 2 3 1 1 0 2 3 2 2 1 0 3 1 1 3 1 2 1 0 2 2 1 1\n",
            " 1 1 3 1 3 1 1 2 1 0 1 2 0 3 2 3 3 3 2 0 1 2 0 0 1 1 1 1 1 0 3 2 0 0 0 0 2\n",
            " 1 1 1 3 2 0 3 0 1 2 2 3 0 1 0 1 3 2 3 3 0 3 2 0 0 3 0 2 2 3 1 1 1 3 0 2 0\n",
            " 3 0 1 3 1 0 2 0 2 3 1 1 2 2 0 1 0 0 2 2 2 3 1 2 2 1 3 3 1 2 3 1 3 3 2 2 3\n",
            " 1 0 1 2 1 2 3 1 0 0 3 0 1 2 3 0 1 3 0 0 1 3 1 1 3 1 0 3 1 3 2 3 2 3 0 0 1\n",
            " 2 2 0 3 3 2 0 0 3 0 2 0 0 3 2 1 0 0 0 0 0 2 0 2 2 3 0 2 2 2 3 0 2 0 3 1 3\n",
            " 0 3 0 0 2 1 1 1 3 1 2 0 1 3 3 1 0 0 1 0 1 3 1 0 3 2 3 1 1 2 3 0 0 3 3 1 3\n",
            " 2 0 3 1 2 1 1 0 1 0 2]\n",
            "[12.200137080191913, 12.200137080191913, 12.200137080191913, 12.337217272104182, 51.06237148732008]\n",
            "                     user_id  group_id\n",
            "0     mCrhj_CG3_pOmDFcfKotRQ       3.0\n",
            "1     VlcasgkqiTuPi-nVT7rEtw       3.0\n",
            "2     WCZFsrIfQaN8Fg2S1ersWw       0.0\n",
            "3     mXbIphHQwWKeAM-RwWkaaQ       3.0\n",
            "4     5_vvuAY9sOVbJtBon8ce2A       0.0\n",
            "...                      ...       ...\n",
            "1454  03_OcS8SbBcO4RTx_jYAUw       4.0\n",
            "1455  vpCdKYNXUjs5o-tVSmwGAg       4.0\n",
            "1456  vVBNWuQO9m5Utm69PGoRCw       4.0\n",
            "1457  h7zHYM8LbPWmXx0ZEqOOAw       4.0\n",
            "1458  4wbMeUS9tp2QGqyNhj3YTg       4.0\n",
            "\n",
            "[1459 rows x 2 columns]\n",
            "[12.200137080191913, 12.200137080191913, 12.200137080191913, 12.337217272104182, 51.06237148732008]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.171082\n",
            "1        1  0.168481\n",
            "2        2  0.174834\n",
            "3        3  0.182596\n",
            "4        4  0.158024\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.172413\n",
            "1        1  0.171113\n",
            "2        2  0.165554\n",
            "3        3  0.163611\n",
            "4        4  0.163882\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2318d9a0f2cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n\u001b[0;32m---> 27\u001b[0;31m             percents, list_of_attributes)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7c162db9e824>\u001b[0m in \u001b[0;36mpipeline3\u001b[0;34m(business_id, method, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random, reviews, percents, list_of_attributes)\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------Ranking Random:------------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   random_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_random,\n\u001b[0;32m---> 84\u001b[0;31m                                                        \"random_\", list_of_attributes)\n\u001b[0m\u001b[1;32m     85\u001b[0m   get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n\u001b[1;32m     86\u001b[0m             'plot_demgr_' + method + '_' + business_id, list_of_attributes, method, business_id)\n",
            "\u001b[0;32m<ipython-input-20-830141ad8d45>\u001b[0m in \u001b[0;36mprint_demographic_parity_exposure\u001b[0;34m(business_id, method, df_ranking, filename, list_of_attributes)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mexposures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exp_demgr_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbusiness_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   upload_file('exp_demgr_' + method + '_' + filename + business_id + '.csv',\n\u001b[0;32m---> 37\u001b[0;31m                   destination)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexposures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-eefb94a8acbd>\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(filename, folder_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdrive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mfileList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"'\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"' in parents and trashed=false\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mdrive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parents'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfolder_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Check if file already exists in Google Drive (prevents duplicates)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/apiattr.py\u001b[0m in \u001b[0;36mGetList\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxResults'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxResults'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/apiattr.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'pageToken'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pageToken'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_GetList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pageToken'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nextPageToken'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGet_Http_Object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecoratee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_decorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36m_GetList\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \"\"\"\n\u001b[1;32m     63\u001b[0m     self.metadata = self.auth.service.files().list(**dict(self)).execute(\n\u001b[0;32m---> 64\u001b[0;31m       http=self.http)\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_metadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m         )\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Retry on SSL errors and socket timeout errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_ssl_SSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssl_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/oauth2client/transport.py\u001b[0m in \u001b[0;36mnew_request\u001b[0;34m(uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m    173\u001b[0m         resp, content = request(orig_request_method, uri, method, body,\n\u001b[1;32m    174\u001b[0m                                 \u001b[0mclean_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                 redirections, connection_type)\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# A stored token may expire between the time it is retrieved and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/oauth2client/transport.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(http, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m    280\u001b[0m     return http_callable(uri, method=method, body=body, headers=headers,\n\u001b[1;32m    281\u001b[0m                          \u001b[0mredirections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                          connection_type=connection_type)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m   1989\u001b[0m                         \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m                         \u001b[0mredirections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m                         \u001b[0mcachekey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m                     )\n\u001b[1;32m   1993\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m         (response, content) = self._conn_request(\n\u001b[0;32m-> 1651\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m         )\n\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_conn_request\u001b[0;34m(self, conn, request_uri, method, body, headers)\u001b[0m\n\u001b[1;32m   1587\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadStatusLine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponseNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m                 \u001b[0;31m# If we get a BadStatusLine on the first try then that means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX8ibEsAIxCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline1(business_id, users, reviews):\n",
        "  ### READ THE RANKING FROM CSV\n",
        "  df_rel_ranking = pd.read_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "  df_date_ranking = pd.read_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "  df_rand_ranking = pd.read_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = filter_user_from_dataset(df_rel_ranking, users)\n",
        "  df_date_ranking = filter_user_from_dataset(df_date_ranking, users)\n",
        "  df_rand_ranking = filter_user_from_dataset(df_rand_ranking, users)\n",
        "\n",
        "  df_rel_ranking = integrate_review_info(df_rel_ranking, reviews, business_id)\n",
        "  df_date_ranking = integrate_review_info(df_date_ranking, reviews, business_id)\n",
        "  df_rand_ranking = integrate_review_info(df_rand_ranking, reviews, business_id)  \n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  df_rel_ranking.to_csv('dataset_rel_ranking_' + business_id + '.csv')\n",
        "  df_date_ranking.to_csv('dataset_date_ranking_' + business_id + '.csv')\n",
        "  df_rand_ranking.to_csv('dataset_rand_ranking_' + business_id + '.csv')\n",
        "  upload_file('dataset_rel_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "  upload_file('dataset_date_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "  upload_file('dataset_rand_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline2(df_ranking_by_relevance, reviews, business, id, N_of_groups, local_list_of_attributes, method):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "\n",
        "  #df_vectors = create_vectors(df_ranking_by_relevance, reviews, business, id, local_list_of_attributes, method)\n",
        "\n",
        "  # ------ SINGLE ATTRIBUTE ---------\n",
        "  if len(local_list_of_attributes) == 1:\n",
        "    attribute = local_list_of_attributes[0]\n",
        "    df_groups = create_groups_by_percentile(df_ranking_by_relevance, attribute, N_of_groups, id)\n",
        "    method = attribute\n",
        "  \n",
        "  # ------ MULTIPLE ATTRIBUTES ---------\n",
        "  # TO BUILD VECTORS, it saves a csv file\n",
        "  elif method == 'kmeans':\n",
        "    df_groups = create_groups_by_kmeans_clustering(df_ranking_by_relevance,N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'spectral':\n",
        "    df_groups = create_groups_by_spectral_clustering(df_ranking_by_relevance, N_of_groups, id)\n",
        "  elif method == 'dbscan':\n",
        "    df_groups = create_groups_by_dbscan_clustering(df_ranking_by_relevance,N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'balanced_kmeans':\n",
        "    df_groups = create_groups_by_balanced_kmeans_clustering(N_of_groups,local_list_of_attributes,id)\n",
        "  \n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "  \n",
        "  print(df_groups)\n",
        "  print(percents)\n",
        "  return df_groups, percents\n",
        "\n",
        "\n",
        "def pipeline3(business_id, method, df_ranking_by_relevance, df_ranking_by_date, \n",
        "              df_ranking_by_random, reviews, percents, list_of_attributes):\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  print(\"----------------DEMOGRAPHIC PARITY EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                       \"random_\", list_of_attributes)\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_demgr_' + method + '_' + business_id, list_of_attributes, method, business_id)\n",
        "\n",
        "  print(\"----------------DISPARATE IMPACT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                   reviews, \"yelp_\", list_of_attributes)\n",
        "  \n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                   reviews, \"date_\", list_of_attributes)\n",
        "  \n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                     reviews, \"random_\", list_of_attributes)\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_dispimp_' + method + '_' + business_id, list_of_attributes, method, business_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_c2w7U3DNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_balanced_kmeans_clustering(N_of_groups, list_of_attributes, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "  df_no_info = df_vectors[df_vectors['age'].isnull()]['user_id']\n",
        "  df_yes_info = df_vectors[df_vectors['age'].notnull()]\n",
        "  number_of_users = len(df_yes_info.index)\n",
        "  one_cluster_size = number_of_users//(N_of_groups-1)\n",
        "  clusters_size = [one_cluster_size]*(N_of_groups-1)\n",
        "\n",
        "  list_vectors = df_yes_info[list_of_attributes].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0 \n",
        "\n",
        "  # #############################################################################\n",
        "  # COMPUTE BALANCED KMEANS WITH MIN_FLOW\n",
        "  # see https://adared.ch/constrained-k-means-implementation-in-python/\n",
        "  (centroids, labels, f) = constrained_kmeans(X, clusters_size)\n",
        "  print('Centroids: ', centroids)\n",
        "  print('labels: ', labels)\n",
        "\n",
        "  # Save centroids\n",
        "  temp_df = pd.DataFrame(data=centroids)\n",
        "  temp_df.columns = list_of_attributes\n",
        "  destination = set_file_destination(local_list_of_attributes, 'balanced_kmeans', id)\n",
        "  temp_df.to_csv(\"centroids_balanced_kmeans_\" + id + \".csv\", float_format=\"%.3f\")\n",
        "  upload_file(\"centroids_balanced_kmeans_\" + id + \".csv\",destination)\n",
        "\n",
        "  new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[j, 'group_id'] = labels[j]\n",
        "  \n",
        "  # ADD THE EXCLUDED IN LAST C\n",
        "  new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "  new_df_users = new_df_users.fillna(N_of_groups-1)\n",
        "\n",
        "  new_df_users.to_csv('groups_balanced_kmeans_' + id + '.csv')\n",
        "  upload_file('groups_balanced_kmeans_' + id + '.csv',destination)\n",
        "  return new_df_users\n",
        "  # #############################################################################\n",
        "  # BALANCED KMEANS WITH PYTORCH DOESN'T WORK\n",
        "  # see https://github.com/giannisdaras/balanced_kmeans/tree/master\n",
        "  #N = len(df_users.index)\n",
        "  #cluster_size = N // N_of_groups\n",
        "  #choices, centers = kmeans_equal(X, num_clusters=N_of_groups, cluster_size=cluster_size)\n",
        "  #print(centers)\n",
        "  #print(choices)\n",
        "\n",
        "\n",
        "def constrained_kmeans(data, demand, maxiter=None, fixedprec=1e9):\n",
        "\tdata = np.array(data)\n",
        "\t\n",
        "\tmin_ = np.min(data, axis = 0)\n",
        "\tmax_ = np.max(data, axis = 0)\n",
        "\t\n",
        "\tC = min_ + np.random.random((len(demand), data.shape[1])) * (max_ - min_)\n",
        "\tM = np.array([-1] * len(data), dtype=np.int)\n",
        "\t\n",
        "\titercnt = 0\n",
        "\twhile True:\n",
        "\t\titercnt += 1\n",
        "\t\tprint(itercnt)\n",
        "\t\t# memberships\n",
        "\t\tg = nx.DiGraph()\n",
        "\t\tg.add_nodes_from(range(0, data.shape[0]), demand=-1) # points\n",
        "\t\tfor i in range(0, len(C)):\n",
        "\t\t\tg.add_node(len(data) + i, demand=demand[i])\n",
        "\t\t\n",
        "\t\t# Calculating cost...\n",
        "\t\tcost = np.array([np.linalg.norm(np.tile(data.T,\n",
        "                                          len(C)).T - np.tile(C, len(data)).reshape(len(C) * len(data),\n",
        "                                                                                            C.shape[1]), axis=1)])\n",
        "\t\t# Preparing data_to_C_edges...\n",
        "\t\tdata_to_C_edges = np.concatenate((np.tile([range(0, data.shape[0])], len(C)).T,\n",
        "                                    np.tile(np.array([range(data.shape[0], data.shape[0] + C.shape[0])]).T,\n",
        "                                            len(data)).reshape(len(C) * len(data), 1), cost.T * fixedprec),\n",
        "                                   axis=1).astype(np.uint64)\n",
        "\t\t# Adding to graph\n",
        "\t\tg.add_weighted_edges_from(data_to_C_edges)\n",
        "\t\t\n",
        "\n",
        "\t\ta = len(data) + len(C)\n",
        "\t\tg.add_node(a, demand=len(data)-np.sum(demand))\n",
        "\t\tC_to_a_edges = np.concatenate((np.array([range(len(data), len(data) + len(C))]).T, np.tile([[a]], len(C)).T), axis=1)\n",
        "\t\tg.add_edges_from(C_to_a_edges)\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t# Calculating min cost flow...\n",
        "\t\tf = nx.min_cost_flow(g)\n",
        "\t\t\n",
        "\t\t# assign\n",
        "\t\tM_new = np.ones(len(data), dtype=np.int) * -1\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tp = sorted(f[i].items(), key=lambda x: x[1])[-1][0]\n",
        "\t\t\tM_new[i] = p - len(data)\n",
        "\t\t\t\n",
        "\t\t# stop condition\n",
        "\t\tif np.all(M_new == M):\n",
        "\t\t\t# Stop\n",
        "\t\t\treturn (C, M, f)\n",
        "\t\t\t\n",
        "\t\tM = M_new\n",
        "\t\t\t\n",
        "\t\t# compute new centers\n",
        "\t\tfor i in range(len(C)):\n",
        "\t\t\tC[i, :] = np.mean(data[M==i, :], axis=0)\n",
        "\t\t\t\n",
        "\t\tif maxiter is not None and itercnt >= maxiter:\n",
        "\t\t\t# Max iterations reached\n",
        "\t\t\treturn (C, M, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQKajNw47W7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_dbscan_clustering(df_users, N_of_groups, list_of_attributes, id):\n",
        "  # #############################################################################\n",
        "  # Generate data\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  list_vectors = df_vectors[list_of_attributes].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0\n",
        "\n",
        "  # #############################################################################\n",
        "  # Compute DBSCAN\n",
        "  i = 0\n",
        "  while i<21:\n",
        "    db = DBSCAN(eps=2, min_samples=i).fit(X)\n",
        "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "    core_samples_mask[db.core_sample_indices_] = True\n",
        "    labels = db.labels_\n",
        "\n",
        "    # Number of clusters in labels, ignoring noise if present.\n",
        "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise_ = list(labels).count(-1)\n",
        "\n",
        "    print('Estimated number of clusters: %d' % n_clusters_)\n",
        "    print('Estimated number of noise points: %d' % n_noise_)\n",
        "    i = i+1\n",
        "  #print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
        "  #print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
        "  #print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
        "  #print(\"Adjusted Rand Index: %0.3f\"\n",
        "  #      % metrics.adjusted_rand_score(labels_true, labels))\n",
        "  #print(\"Adjusted Mutual Information: %0.3f\"\n",
        "  #      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
        "  print(\"Silhouette Coefficient: %0.3f\"\n",
        "        % metrics.silhouette_score(X, labels))\n",
        "  \n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[j, 'group_id'] = labels[j]\n",
        "  new_df_users.to_csv('groups_dbscan_' + id + '.csv')\n",
        "  destination = set_file_destination(local_list_of_attributes, 'dbscan', id)\n",
        "  upload_file('groups_dbscan_' + id + '.csv',destination)\n",
        "  \n",
        "  #SEARCH FOR CLUSTER DESCRIPTION AND SAME SIZE CLUSTERING\n",
        "\n",
        "  # #############################################################################\n",
        "  # Plot result\n",
        "\n",
        "  # Black removed and is used for noise instead.\n",
        "  unique_labels = set(labels)\n",
        "  colors = [plt.cm.Spectral(each)\n",
        "            for each in np.linspace(0, 1, len(unique_labels))]\n",
        "  for k, col in zip(unique_labels, colors):\n",
        "      if k == -1:\n",
        "          # Black used for noise.\n",
        "          col = [0, 0, 0, 1]\n",
        "\n",
        "      class_member_mask = (labels == k)\n",
        "\n",
        "      xy = X[class_member_mask & core_samples_mask]\n",
        "      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "              markeredgecolor='k', markersize=14)\n",
        "\n",
        "      xy = X[class_member_mask & ~core_samples_mask]\n",
        "      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "              markeredgecolor='k', markersize=6)\n",
        "\n",
        "  plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "  plt.show()\n",
        "  ####################################################################\n",
        "  return new_df_users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRs30-Z9heM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_kmeans_clustering(df_users, N_of_groups, list_of_attributes, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  \n",
        "  # FOR NOW WE EXCLUDE TEXTUAL ATTRIBUTES\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  list_vectors = df_vectors[list_of_attributes].values.tolist()\n",
        "  # ex. ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']\n",
        "  # ex. ['loc1', 'loc2', 'loc3']\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0\n",
        "  kmeans = KMeans(n_clusters=N_of_groups, random_state=0).fit(X)\n",
        "  labels = kmeans.labels_\n",
        "  # kmeans.predict([[0, 0, 20], [12, 3, 5]]))\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  temp_df = pd.DataFrame(data=centroids)\n",
        "  temp_df.columns = list_of_attributes\n",
        "  destination = set_file_destination(local_list_of_attributes, 'kmeans', id)\n",
        "  temp_df.to_csv(\"centroids_kmeans_\" + id + \".csv\", float_format=\"%.3f\")\n",
        "  upload_file(\"centroids_kmeans_\" + id + \".csv\",destination)\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = labels[i]\n",
        "  new_df_users.to_csv('groups_kmeans_' + id + '.csv')\n",
        "  upload_file('groups_kmeans_' + id + '.csv',destination)\n",
        "  return new_df_users\n",
        "\n",
        "\n",
        "def generate_dummies(df, text_attribute_list):\n",
        "  dummy_columns = []\n",
        "  for attr in text_attribute_list:\n",
        "    gender_dummies = pd.get_dummies(df[attr])\n",
        "    dummy_columns = dummy_columns + list(gender_dummies.columns)\n",
        "    df = pd.merge(df, gender_dummies, how=\"left\",left_index=True, right_index=True)\n",
        "    \n",
        "    # drop all the unnamed columns\n",
        "    cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "    df=df[cols]\n",
        "  return df, dummy_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU_zGBsWzAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_spectral_clustering(df_users, N_of_groups, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['top_location']\n",
        "  df_vectors = one_hot_encoding(df_vectors, text_attribute_list)\n",
        "  \n",
        "  list_vectors = df_vectors[['review_count', 'fans', 'average_stars', 'top_location',\n",
        "                             'useful', 'funny', 'cool']].values.tolist()\n",
        "  # Convert list of list in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  mat = cosine_similarity(list_of_arrays)\n",
        "  group_array = SpectralClustering(n_clusters=N_of_groups, affinity='precomputed').fit_predict(mat)\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = group_array[i]\n",
        "  new_df_users.to_csv('groups_spectral_' + id + '.csv')\n",
        "  return new_df_users\n",
        "\n",
        "def one_hot_encoding(df, text_attribute_list):\n",
        "  for attr in text_attribute_list:\n",
        "    text = list(df[attr])\n",
        "    vectorizer = CountVectorizer().fit_transform(text)\n",
        "    embeddings = vectorizer.toarray()\n",
        "    for i, user in df.iterrows():\n",
        "      df.at[i, attr] = embeddings[i]\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awAE4AWWxEFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_percentile(df_users, attribute, N_of_groups, business_id):\n",
        "  values = df_users[attribute].tolist()\n",
        "\n",
        "  print(df_users)\n",
        "\n",
        "  # Create array of percents to define limit of group \n",
        "  scale = 100/N_of_groups\n",
        "  percents = []\n",
        "  current_percent = 0\n",
        "  while current_percent < 100:\n",
        "    current_percent = current_percent + scale\n",
        "    '''if current_percent > 100:\n",
        "      current_percent = 100\n",
        "      percents.append(current_percent)\n",
        "      break'''\n",
        "    percents.append(current_percent)\n",
        "  # ex. percents = [20, 40, 60, 80, 100]\n",
        "  print(percents)\n",
        "\n",
        "  # Create array with limit values (max) of each group\n",
        "  i = 0\n",
        "  limit_values = []\n",
        "  while i < len(percents):\n",
        "    limit_values.append(np.percentile(values, percents[i]))\n",
        "    i = i + 1\n",
        "  # ex. limit_values = [5.0, 13.0, 27.0, 73.0, 10022.0]\n",
        "  print(limit_values)\n",
        "  with open('descr_groups_' + attribute + '_' + business_id + '.txt', 'w') as output:\n",
        "    output.write(str(percents) + '\\n' + str(limit_values))\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  \n",
        "  for i, user in df_users.iterrows():\n",
        "    id_group = 0\n",
        "\n",
        "    if (user[attribute] >= 0) and (user[attribute] <= limit_values[id_group]):\n",
        "      new_df_users.loc[i, 'group_id'] = id_group\n",
        "    id_group = id_group + 1\n",
        "    while id_group < len(percents):\n",
        "      if (user[attribute] > limit_values[id_group - 1]) and (user[attribute] <= limit_values[id_group]):\n",
        "        new_df_users.loc[i, 'group_id'] = id_group\n",
        "      id_group = id_group + 1\n",
        "  \n",
        "  destination = set_file_destination([attribute], '', business_id)\n",
        "  new_df_users.to_csv('groups_' + attribute + '_' + business_id + '.csv')\n",
        "  upload_file('groups_' + attribute + '_' + business_id + '.csv',destination)\n",
        "  return new_df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbpLTiPSWuL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_disparate_impact_exposure(business_id, method,  df_ranking, reviews,\n",
        "                                    filename, list_of_attributes):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp = disparate_impact_exposure(df_groups[df_groups['group_id'] == i], df_ranking, reviews, business_id)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  exposures.to_csv('exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures\n",
        "\n",
        "\n",
        "def print_demographic_parity_exposure(business_id, method, df_ranking, filename, list_of_attributes):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp = demographic_parity_exposure(df_groups[df_groups['group_id'] == i], df_ranking)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  exposures.to_csv('exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures\n",
        "\n",
        "\n",
        "def disparate_impact_exposure(df_group, ranking, reviews, business_id):\n",
        "    if len(df_group.index) == 0:\n",
        "      return 0\n",
        "    return dmp_sommatory(df_group, ranking, business_id, reviews)/(len(df_group.index))\n",
        "\n",
        "\n",
        "def demographic_parity_exposure(df_group, ranking):\n",
        "    if len(df_group.index) == 0:\n",
        "      return 0\n",
        "    return demgr_sommatory(df_group, ranking)/(len(df_group.index))\n",
        "\n",
        "\n",
        "def dmp_sommatory(df_group, ranking, business_id, reviews):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "\n",
        "        #Integrate reviews info for clicks counting\n",
        "        df_temp = reviews[(reviews['business_id'] == business_id) & (reviews['user_id'] == user_id)]\n",
        "        useful = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['useful']\n",
        "        funny = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['funny']\n",
        "        cool = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['cool']\n",
        "        counts = useful + funny + cool + 1\n",
        "        base = 2  # con 10 i valori sono troppo bassi\n",
        "        counts = math.log(counts, base) \n",
        "\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        addend = exp(position) * counts\n",
        "        sum = sum + addend\n",
        "    return sum\n",
        "\n",
        "\n",
        "def demgr_sommatory(df_group, ranking):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        sum = sum + exp(position)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/np.log(1 + position)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPGOC05EljLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_plots(yelp_exposures, date_exposures, random_exposures, percents, title,\n",
        "              list_of_attributes, method, business_id):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  print(y1)\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='yelp')\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date')\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0),this_range)\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Iq3ciMMcZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO UPDATE CENTROIDS\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg'] \n",
        "method = 'kmeans'\n",
        "for id in id_list:\n",
        "  pd.options.display.float_format = '{:.3f}'.format\n",
        "  centroids = pd.read_csv('centroids_' + method + '_' + id + '.csv')\n",
        "  values = [float(x) for x in centroids.columns.values]\n",
        "  centroids.loc[-1] = values # adding a row\n",
        "  centroids.index = centroids.index + 1  # shifting index\n",
        "  centroids.sort_index(inplace=True)\n",
        "  centroids.columns = ['loc1', 'loc2', 'loc3']  \n",
        "  centroids.to_csv('centroids_' + method + '_' + id + '.csv', float_format='%.3f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22E0H3eY8Vb3",
        "colab_type": "code",
        "outputId": "5116c13a-fb95-4c8f-a24f-ef34ddfd0c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TO UPDATE EXPOSURE\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA']\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg'] #RICALCOLARE DA CAPO\n",
        "# id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "for id in id_list:\n",
        "  df_rel_ranking = pd.read_csv('dataset_rel_ranking_'+id+'.csv')\n",
        "  df_date_ranking = pd.read_csv('dataset_date_ranking_'+id+'.csv')\n",
        "  df_rand_ranking = pd.read_csv('dataset_rand_ranking_'+id+'.csv')\n",
        "\n",
        "  method = 'kmeans'\n",
        "  N_of_groups = 5\n",
        "  df_groups = pd.read_csv('groups_' + method + '_'+id+'.csv')\n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews, percents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[92.94037011651817, 0.06854009595613435, 1.0281014393420151, 5.68882796435915, 0.2741603838245374]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.162933\n",
            "1        1  0.220105\n",
            "2        2  0.240493\n",
            "3        3  0.200643\n",
            "4        4  0.185087\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.16376\n",
            "1        1  0.152814\n",
            "2        2  0.172153\n",
            "3        3  0.201048\n",
            "4        4  0.169292\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.16606\n",
            "1        1  0.156778\n",
            "2        2  0.165594\n",
            "3        3  0.164798\n",
            "4        4  0.165478\n",
            "\n",
            "\n",
            "[0.16293274380030887, 0.2201045822301589, 0.2404932137794746, 0.20064345885486787, 0.1850865033952374]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.070566\n",
            "1        1   1.11977\n",
            "2        2  0.714228\n",
            "3        3  0.329566\n",
            "4        4  0.366383\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0740012\n",
            "1        1   0.777435\n",
            "2        2   0.466203\n",
            "3        3    0.39857\n",
            "4        4   0.300483\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0673801\n",
            "1        1   0.797605\n",
            "2        2   0.473072\n",
            "3        3    0.28051\n",
            "4        4   0.303709\n",
            "\n",
            "\n",
            "[0.0705660245314008, 1.1197738832848634, 0.714228081159378, 0.3295657039859884, 0.3663834882008638]\n",
            "[93.1098696461825, 0.0931098696461825, 0.74487895716946, 0.186219739292365, 5.865921787709497]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.173895\n",
            "1        1  0.621335\n",
            "2        2   0.21503\n",
            "3        3  0.228349\n",
            "4        4  0.183252\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.173915\n",
            "1        1  0.144723\n",
            "2        2  0.182823\n",
            "3        3  0.171159\n",
            "4        4  0.196404\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.175771\n",
            "1        1  0.151612\n",
            "2        2  0.170644\n",
            "3        3  0.167176\n",
            "4        4  0.168518\n",
            "\n",
            "\n",
            "[0.17389513349465582, 0.6213349345596119, 0.2150297612553226, 0.2283487263806268, 0.18325220317761595]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0948298\n",
            "1        1    3.07822\n",
            "2        2   0.537432\n",
            "3        3   0.469229\n",
            "4        4   0.283128\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.10486\n",
            "1        1  0.716986\n",
            "2        2  0.406378\n",
            "3        3  0.326073\n",
            "4        4  0.334458\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0938564\n",
            "1        1   0.751116\n",
            "2        2   0.378691\n",
            "3        3   0.318491\n",
            "4        4   0.263216\n",
            "\n",
            "\n",
            "[0.09482980330121404, 3.0782152403097, 0.5374321645605649, 0.46922881898353463, 0.28312772973154576]\n",
            "[93.4054054054054, 0.10810810810810811, 0.9729729729729729, 5.297297297297297, 0.21621621621621623]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.176476\n",
            "1        1  0.200383\n",
            "2        2  0.208942\n",
            "3        3  0.228386\n",
            "4        4  0.460329\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.178995\n",
            "1        1  0.234594\n",
            "2        2  0.224002\n",
            "3        3   0.19136\n",
            "4        4  0.194405\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.180793\n",
            "1        1  0.166559\n",
            "2        2  0.185239\n",
            "3        3   0.16935\n",
            "4        4  0.165265\n",
            "\n",
            "\n",
            "[0.17647606219677034, 0.20038343021591398, 0.20894189258758333, 0.22838557398304843, 0.4603293537434925]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.114704\n",
            "1        1   1.01081\n",
            "2        2  0.727906\n",
            "3        3  0.548388\n",
            "4        4   2.27585\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.124164\n",
            "1        1   1.18339\n",
            "2        2  0.761529\n",
            "3        3  0.483666\n",
            "4        4  0.957387\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.111639\n",
            "1        1  0.840188\n",
            "2        2  0.633305\n",
            "3        3  0.414278\n",
            "4        4  0.814873\n",
            "\n",
            "\n",
            "[0.11470445783168728, 1.0108129969980315, 0.7279060122864212, 0.5483878282837408, 2.2758477067956013]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FcLo9mPIeHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO UPDATE PLOTS\n",
        "id = 'WbJ1LRQdOuYYlRLyTkuuxw'\n",
        "grouping = 'review_count'\n",
        "exp = 'demgr'\n",
        "y = pd.read_csv('exp_' + exp + '_' + grouping + '_yelp_' + id + '.csv')\n",
        "d = pd.read_csv('exp_' + exp + '_' + grouping + '_date_' + id + '.csv')\n",
        "r = pd.read_csv('exp_' + exp + '_' + grouping + '_random_' + id + '.csv')\n",
        "get_plots(y,d,r, [93.42, 0.17, 0.003, 5.81, 0.188], \"title\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUr0D9xWp5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_size(df_users, attribute, range):\n",
        "    # order df_users by review_count\n",
        "    df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "    group_list = [[]]\n",
        "    i = 0\n",
        "    group_index = 0\n",
        "    prec = -1\n",
        "    for index, user in df_users.iterrows():\n",
        "        if i == range:\n",
        "            i = 0\n",
        "            group_list.append([])\n",
        "            group_index = group_index + 1\n",
        "        temp = user[attribute]\n",
        "        if temp != prec:\n",
        "            prec = temp\n",
        "            i = i + 1\n",
        "        group_list[group_index].append(user)\n",
        "    print(\"Number of groups created: \" + str(group_index+1))\n",
        "    return group_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYumnpYHecF",
        "colab_type": "code",
        "outputId": "e54f518e-730b-47d2-c8dd-a165910cde70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "### UNIQUE VALUE OF state, city AND categories OF BUSINESS\n",
        "# To filter only RESTAURANTS\n",
        "# is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "# restaurants = business[is_restaurant]\n",
        "\n",
        "print('UNIQUE VALUE OF \\'state\\' ATTRIBUTE IN RESTAURANTS = ', business['state'].nunique())\n",
        "print('UNIQUE VALUE OF \\'city\\' ATTRIBUTE IN RESTAURANTS = ', business['city'].nunique())\n",
        "df_cat = business[['categories']]\n",
        "distinct_categories_list = []\n",
        "for index, row in df_cat.iterrows():\n",
        "  if row['categories'] != None:\n",
        "    lst = [item.strip() for item in row['categories'].split(',')]\n",
        "    distinct_categories_list = list(set(distinct_categories_list + lst))\n",
        "print('UNIQUE VALUE OF \\'categories\\' ATTRIBUTE distinct values ', len(distinct_categories_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIQUE VALUE OF 'state' ATTRIBUTE IN RESTAURANTS =  36\n",
            "UNIQUE VALUE OF 'city' ATTRIBUTE IN RESTAURANTS =  1204\n",
            "UNIQUE VALUE OF 'categories' ATTRIBUTE distinct values  1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05AeyBn2gGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRINT THE state OF EACH user FOR HIS RESTAURANT reviews\n",
        "def statistics(users, reviews, business):\n",
        "  res = pd.DataFrame()\n",
        "  cont = 0\n",
        "  prec_user_id = '0'\n",
        "\n",
        "  for i, user in users.iterrows():\n",
        "    if cont == 1000:\n",
        "      break\n",
        "    user_id = user['user_id']  # 4, 9, 16, 21!, 22!, 23, 24\n",
        "    # Get all reviews of one user\n",
        "    # 'https://www.yelp.com/user_details_reviews_self?userid=NQffx45eJaeqhFcMadKUQA&rec_pagestart=90'??? no\n",
        "    user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "    # Get all restaurants\n",
        "    business_ids = user_reviews[['business_id']]\n",
        "    result = business_ids.merge(business)\n",
        "    is_restaurant = result['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "    user_restaurant_reviews = result[is_restaurant]\n",
        "    user_restaurant_reviews['user_id'] = user_id\n",
        "    #user_restaurant_reviews.to_csv('user_restaurant_reviews' + str(cont) + '.csv')\n",
        "    res = res.append(user_restaurant_reviews.groupby(['user_id', 'state'])['user_id'].count().reset_index(name=\"review_count\"))\n",
        "    if (user_id != prec_user_id) and not user_restaurant_reviews.empty:\n",
        "      cont = cont + 1\n",
        "    # print(res)\n",
        "    prec_user_id = user_id\n",
        "\n",
        "  res.to_csv('result.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0M0ETqjMrqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_df_users(ranking):\n",
        "  df_ranking = pd.DataFrame(ranking, columns=['position', 'user_id', 'review_count', 'date']).sort_values('position')\n",
        "  return df_ranking"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6gUlnqBZ9eoa",
        "colab": {}
      },
      "source": [
        "# TODO: get review_count from web page\n",
        "def get_ranking(url):\n",
        "\n",
        "  i=1\n",
        "  reviews_text = []\n",
        "  reviews_date = []\n",
        "  reviews_userid = []\n",
        "  x=0\n",
        "\n",
        "  while 1:\n",
        "    if x == 0:\n",
        "      page_content = requests.get(url)\n",
        "    else:\n",
        "        page_content = requests.get(url + '?start=' + str(x))\n",
        "    x = x + 20\n",
        "    \n",
        "    tree = html.fromstring(page_content.content)\n",
        "    recommended_reviews_text = \"Recommended Reviews\"\n",
        "    reviews_list = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/*' % recommended_reviews_text)\n",
        "    if not reviews_list:\n",
        "      break\n",
        "    else:\n",
        "      # Index for ranking\n",
        "      j=1\n",
        "      for review in reviews_list:\n",
        "          reviews_text.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[p]/p/span' % (recommended_reviews_text, j))[0].text))\n",
        "          reviews_date.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[1]/div/div[2]/span' % (recommended_reviews_text, j))[0].text))\n",
        "          user_link = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div/div/div/div/div/div/a/@href' % (recommended_reviews_text, j))[0]\n",
        "          reviews_userid.append((i,user_link[user_link.find('=')+1:]))\n",
        "          i = i + 1\n",
        "          j = j + 1\n",
        "\n",
        "\n",
        "  print(reviews_text)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_text)))\n",
        "  print(reviews_date)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_date)))\n",
        "  print(reviews_userid)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_userid)))\n",
        "  return reviews_userid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHl6bORqNzeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_users_in_dataset(users):\n",
        "    data = []\n",
        "    with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            data.append(json_data)\n",
        "    df = pd.DataFrame(data)\n",
        "    df_ranking = pd.DataFrame(users, columns=['Position', 'user_id'])\n",
        "    df_merged = df_ranking.merge(df, on='user_id')\n",
        "    # temp = df[df['user_id'].isin(users)]\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def exists_in_dataset(userid):\n",
        "  found = False\n",
        "  with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            if json_data['user_id'] == userid:\n",
        "              found = True\n",
        "  if found:\n",
        "    print(\"Trovato\")\n",
        "  else:\n",
        "    print(\"Non trovato\")\n",
        "  return found\n",
        "\n",
        "\n",
        "def find_users_in_chopped_dataset(users):\n",
        "    found = False\n",
        "    data = []\n",
        "    i = 1\n",
        "    j = 100000\n",
        "    df_users = pd.DataFrame()\n",
        "    while not found:\n",
        "        path = \"user\" + str(i) + \"-\" + str(j) + \".json\"\n",
        "        try:\n",
        "            with open(path, 'r') as file:\n",
        "                for line in file:\n",
        "                    json_data = json.loads(line)\n",
        "                    data.append(json_data)\n",
        "            df = pd.DataFrame(data)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Cannot find all users into the dataset\")\n",
        "            return 0\n",
        "        else:\n",
        "            temp = df[df['user_id'].isin(users)]  # df of users of the ranking\n",
        "            df_users = df_users.append(temp)\n",
        "            if df_users.shape[0] == len(users):\n",
        "                found = True\n",
        "            else:\n",
        "                i = i + 100000\n",
        "                j = j + 100000\n",
        "    return df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}