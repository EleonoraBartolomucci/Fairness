{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exposure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOXDbbl/yKH/goYvN7ed4vo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/exposure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPpCkgv0271l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import math\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7PR_gGjwNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyIm6NR55FGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrfK_M25rAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD user.json FROM DRIVE\n",
        "users_dataset_id = '1JokoV68YD5Iq2l4Y_IV2RJzBpD_mSCyq'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': users_dataset_id})\n",
        "download.GetContentFile('user.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMGOW-SHRrP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD business.json FROM DRIVE\n",
        "business_dataset_id = '1Qoy132gb205xAIFjkBbZ2CyYDeaz9yqU'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': business_dataset_id})\n",
        "download.GetContentFile('business.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tkQRFfw16x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD review.json FROM DRIVE\n",
        "review_dataset_id = '1mW1WbpMFjN0qQpLnM-R_TzNpAkFsBLHQ'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': review_dataset_id})\n",
        "download.GetContentFile('review.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JgvcYwsfpy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# READ JSON\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcwnWi7G4w5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "ccc487d3-375f-4b00-9cab-a1820fbdec6e"
      },
      "source": [
        "users = read_json('user.json')\n",
        "users = pd.DataFrame(users)\n",
        "business = read_json('business.json')\n",
        "business = pd.DataFrame(business)\n",
        "reviews = read_json('review.json')\n",
        "reviews = pd.DataFrame(reviews)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-eba1dd53bf48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbusiness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'business.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbusiness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'review.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-8e4b8c5410f9>\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(json_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmy_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mline_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'review.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05AeyBn2gGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def statistics(users, reviews, business):\n",
        "  res = pd.DataFrame()\n",
        "  cont = 0\n",
        "  prec_user_id = '0'\n",
        "\n",
        "  for i, user in users.iterrows():\n",
        "    if cont == 1000:\n",
        "      break\n",
        "    user_id = user['user_id']  # 4, 9, 16, 21!, 22!, 23, 24\n",
        "    # Get all reviews of one user\n",
        "    # 'https://www.yelp.com/user_details_reviews_self?userid=NQffx45eJaeqhFcMadKUQA&rec_pagestart=90'??? no\n",
        "    user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "    # Get all restaurants\n",
        "    business_ids = user_reviews[['business_id']]\n",
        "    result = business_ids.merge(business)\n",
        "    is_restaurant = result['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "    user_restaurant_reviews = result[is_restaurant]\n",
        "    user_restaurant_reviews['user_id'] = user_id\n",
        "    #user_restaurant_reviews.to_csv('user_restaurant_reviews' + str(cont) + '.csv')\n",
        "    res = res.append(user_restaurant_reviews.groupby(['user_id', 'state'])['user_id'].count().reset_index(name=\"review_count\"))\n",
        "    if (user_id != prec_user_id) and not user_restaurant_reviews.empty:\n",
        "      cont = cont + 1\n",
        "    # print(res)\n",
        "    prec_user_id = user_id\n",
        "\n",
        "  res.to_csv('result.csv')  # PRINT THE state OF EACH user FOR HIS RESTAURANT reviews\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYumnpYHecF",
        "colab_type": "code",
        "outputId": "e54f518e-730b-47d2-c8dd-a165910cde70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "### UNIQUE VALUE OF state, city AND categories OF BUSINESS\n",
        "# To filter only RESTAURANTS\n",
        "# is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "# restaurants = business[is_restaurant]\n",
        "\n",
        "print('UNIQUE VALUE OF \\'state\\' ATTRIBUTE IN RESTAURANTS = ', business['state'].nunique())\n",
        "print('UNIQUE VALUE OF \\'city\\' ATTRIBUTE IN RESTAURANTS = ', business['city'].nunique())\n",
        "df_cat = business[['categories']]\n",
        "distinct_categories_list = []\n",
        "for index, row in df_cat.iterrows():\n",
        "  if row['categories'] != None:\n",
        "    lst = [item.strip() for item in row['categories'].split(',')]\n",
        "    distinct_categories_list = list(set(distinct_categories_list + lst))\n",
        "print('UNIQUE VALUE OF \\'categories\\' ATTRIBUTE distinct values ', len(distinct_categories_list))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIQUE VALUE OF 'state' ATTRIBUTE IN RESTAURANTS =  36\n",
            "UNIQUE VALUE OF 'city' ATTRIBUTE IN RESTAURANTS =  1204\n",
            "UNIQUE VALUE OF 'categories' ATTRIBUTE distinct values  1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqp91T-ezPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def divide_business_by_review_count():\n",
        "  data = read_json('business.json')\n",
        "  df = pd.DataFrame(data)\n",
        "  max = df['review_count'].max()\n",
        "  min = df['review_count'].min()\n",
        "  print('max is = ', max)\n",
        "  print('min is = ', min)\n",
        "  median = (max-min)/2  # 4172,5\n",
        "  business_with_few_review_count = df[df['review_count']<median]\n",
        "\n",
        "  # print(business_with_few_review_count)\n",
        "\n",
        "  business_with_many_review_count = df[df['review_count']>=median]\n",
        "\n",
        "  # print(business_with_many_review_count)\n",
        "  business_with_few_review_count.to_csv('few_review_business.csv')\n",
        "  business_with_many_review_count.to_csv(r'many_review_business.csv')\n",
        "  return (business_with_few_review_count, business_with_many_review_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irlisv9418xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ranking_from_call(url_business, lang, sort, query, is_random):\n",
        "  \n",
        "  headers = [{\"name\":\"Accept\",\"value\":\"*/*\"},{\"name\":\"Accept-Encoding\",\"value\":\"gzip, deflate, br\"},{\"name\":\"Accept-Language\",\"value\":\"it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3\"},{\"name\":\"Connection\",\"value\":\"keep-alive\"},{\"name\":\"Content-Type\",\"value\":\"application/x-www-form-urlencoded; charset=utf-8\"},{\"name\":\"Cookie\",\"value\":\"qntcst=D; hl=en_US; wdi=1|3C26116D69138F61|0x1.78d019f71a444p+30|a7756ff94751d3a9; _ga=GA1.2.3C26116D69138F61; location=%7B%22city%22%3A+%22New+York%22%2C+%22state%22%3A+%22NY%22%2C+%22country%22%3A+%22US%22%2C+%22latitude%22%3A+40.713%2C+%22longitude%22%3A+-74.0072%2C+%22max_latitude%22%3A+40.8523%2C+%22min_latitude%22%3A+40.5597%2C+%22max_longitude%22%3A+-73.7938%2C+%22min_longitude%22%3A+-74.1948%2C+%22zip%22%3A+%22%22%2C+%22address1%22%3A+%22%22%2C+%22address2%22%3A+%22%22%2C+%22address3%22%3A+null%2C+%22neighborhood%22%3A+null%2C+%22borough%22%3A+null%2C+%22provenance%22%3A+%22YELP_GEOCODING_ENGINE%22%2C+%22display%22%3A+%22New+York%2C+NY%22%2C+%22unformatted%22%3A+%22New+York%2C+NY%2C+US%22%2C+%22accuracy%22%3A+4.0%2C+%22language%22%3A+null%7D; xcj=1|Ptt9P03gfc75x_PBT9zmqCkUuSuyB7PR-wWUBvABNi4; __qca=P0-60561249-1581956668708; G_ENABLED_IDPS=google; __cfduid=db8764ff59d8028a6c2e1b214867927d81583160194; _gid=GA1.2.2014867238.1583835527; bse=05dcd9d5de304ef0b1d9a76fa768b10f; sc=8a1ca0dbc2; pid=505721aa4569e7bb\"},{\"name\":\"Host\",\"value\":\"www.yelp.com\"},{\"name\":\"Referer\",\"value\":\"https://www.yelp.com/biz/noche-de-margaritas-new-york\"},{\"name\":\"TE\",\"value\":\"Trailers\"},{\"name\":\"User-Agent\",\"value\":\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0\"},{\"name\":\"X-Requested-By-React\",\"value\":\"true\"},{\"name\":\"X-Requested-With\",\"value\":\"XMLHttpRequest\"}]\n",
        "  headers_ok = {}\n",
        "  for header in headers:\n",
        "    temp = {\n",
        "        header['name']: header['value']\n",
        "    }\n",
        "    headers_ok.update(temp)\n",
        "  \n",
        "  x = 0\n",
        "  reviews_list = []\n",
        "  position = 1\n",
        "  url = url_business + \"/review_feed?rl=\" + lang + \"&sort_by=\" + sort + \"&q=\" + query\n",
        "  \n",
        "  while 1:\n",
        "    if x == 0:\n",
        "      page_load = requests.get(url + '&start=', headers=headers_ok)\n",
        "    else:\n",
        "      page_load = requests.get(url + '&start=' + str(x), headers=headers_ok)\n",
        "    print(page_load)\n",
        "    x = x + 20\n",
        "    reviews = page_load.json()['reviews']\n",
        "    # print(json.dumps(reviews, indent=4, sort_keys=True))\n",
        "    if not reviews:\n",
        "      break\n",
        "    if is_random:\n",
        "      # shuffle the reviews\n",
        "      random.shuffle(reviews)\n",
        "    for review in reviews:\n",
        "      reviews_list.append((position, review['userId'], review['user']['reviewCount']))\n",
        "      position = position + 1\n",
        "  print(reviews_list)\n",
        "  return reviews_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbM06dpOYYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################# EXECUTE IN LOCAL (if not blocked by yelp) #############################################\n",
        "def download_ranking_by_relevance(business_id):\n",
        "  random = False\n",
        "  ranking = get_ranking_from_call(\"https://www.yelp.com/biz/\" + business_id, \"en\", \"relevance_desc\", \"\", random)\n",
        "  print(len(ranking))\n",
        "  with open(\"ranking_by_relevance.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(ranking, fp)\n",
        "  ### TODO: if executed in local, then UPLOAD ranking_by_relevance.txt ON COLAB \n",
        "\n",
        "\n",
        "def download_ranking_by_date(business_id):\n",
        "  random = False\n",
        "  ranking = get_ranking_from_call(\"https://www.yelp.com/biz/\" + business_id, \"en\", \"date_desc\", \"\", random)\n",
        "  print(len(ranking))\n",
        "  with open(\"ranking_by_date.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(ranking, fp)\n",
        "  ### TODO: if executed in local, then UPLOAD ranking_by_date.txt ON COLAB \n",
        "\n",
        "\n",
        "def download_ranking_by_random(business_id):\n",
        "  random = True\n",
        "  ranking = get_ranking_from_call(\"https://www.yelp.com/biz/\" + business_id, \"en\", \"relevance_desc\", \"\", random)\n",
        "  print(len(ranking))\n",
        "  with open(\"ranking_by_random.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(ranking, fp)\n",
        "  ### TODO: if executed in local, then UPLOAD ranking_by_random.txt ON COLAB "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3pgmnXU9cSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_rankings(business_id):\n",
        "  # Business URL: 'https://www.yelp.com/biz/noche-de-margaritas-new-york'\n",
        "  download_ranking_by_relevance(business_id)\n",
        "  download_ranking_by_date(business_id)\n",
        "  download_ranking_by_random(business_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40JdFbw0Qqmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "d7d9ea36-1135-422a-90a0-de4c37778dad"
      },
      "source": [
        "few_revcount, many_revcount = divide_business_by_review_count()\n",
        "business_temp_ids = [few_revcount['business_id'].iloc[2], many_revcount['business_id'].iloc[1]]\n",
        "for business_id in business_temp_ids:\n",
        "  attribute = 'Review_Count'\n",
        "  download_rankings(business_id)\n",
        "  df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random = pipeline1()\n",
        "  group_list = pipeline2(attribute, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random)\n",
        "  pipeline3(attribute, group_list, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max is =  8348\n",
            "min is =  3\n",
            "                   business_id  ...                                              hours\n",
            "0       1SWheh84yJXfytovILXOAQ  ...                                               None\n",
            "1       QXAEGFB4oINsVuTFxEYKFQ  ...  {'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...\n",
            "2       gnKjwL_1w79qoiV3IC_xQQ  ...  {'Monday': '17:30-21:30', 'Wednesday': '17:30-...\n",
            "3       xvX2CttrVhyG2z1dFg_0xw  ...  {'Monday': '8:0-17:0', 'Tuesday': '8:0-17:0', ...\n",
            "4       HhyxOkGAM07SRYtlQ4wMFQ  ...  {'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...\n",
            "...                        ...  ...                                                ...\n",
            "192604  nqb4kWcOwp8bFxzfvaDpZQ  ...  {'Monday': '0:0-0:0', 'Tuesday': '0:0-0:0', 'W...\n",
            "192605  vY2nLU5K20Pee-FdG0br1g  ...                                               None\n",
            "192606  MiEyUDKTjeci5TMfxVZPpg  ...  {'Monday': '7:0-15:0', 'Tuesday': '7:0-15:0', ...\n",
            "192607  zNMupayB2jEHVDOji8sxoQ  ...  {'Tuesday': '8:30-17:30', 'Wednesday': '8:30-1...\n",
            "192608  c1f_VAX1KIK8-JoVhjbYOw  ...  {'Monday': '10:0-0:0', 'Tuesday': '10:0-0:0', ...\n",
            "\n",
            "[192598 rows x 14 columns]\n",
            "                   business_id  ...                                              hours\n",
            "1975    ujHiaprwCQ5ewziu0Vi9rw  ...  {'Monday': '7:0-22:0', 'Tuesday': '7:0-22:0', ...\n",
            "29614   iCQpiavjjPzJ5_3gPD5Ebg  ...  {'Monday': '11:0-5:0', 'Tuesday': '11:0-4:0', ...\n",
            "35199   K7lWdNUhCbcnEvI0NhGewg  ...  {'Monday': '8:0-21:0', 'Tuesday': '8:0-21:0', ...\n",
            "58852   f4x1YBxkLrZg652xt2KR5g  ...  {'Monday': '0:0-0:0', 'Tuesday': '0:0-0:0', 'W...\n",
            "82250   4JNXUYY8wbaaDmk3BPzlWw  ...  {'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...\n",
            "83950   DkYS3arLOhA8si5uUEmHOw  ...  {'Monday': '0:0-0:0', 'Tuesday': '0:0-0:0', 'W...\n",
            "89206   RESDUcs7fIiihp38-d6_6g  ...  {'Monday': '7:30-22:0', 'Tuesday': '7:30-22:0'...\n",
            "89437   5LNZ67Yw9RD6nf4_UhXOjw  ...  {'Monday': '0:0-0:0', 'Tuesday': '0:0-0:0', 'W...\n",
            "141820  cYwJA2A6I12KNkm2rtXd5g  ...  {'Monday': '11:0-0:0', 'Tuesday': '11:0-0:0', ...\n",
            "170982  SMPbvZLSMMb7KU76YNYMGg  ...  {'Monday': '0:0-0:0', 'Tuesday': '0:0-0:0', 'W...\n",
            "174616  2weQS-RnoOBhb1KsHKyoSQ  ...  {'Monday': '7:30-21:30', 'Tuesday': '7:30-21:3...\n",
            "\n",
            "[11 rows x 14 columns]\n",
            "<Response [503]>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-38337bacb826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbusiness_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbusiness_temp_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mattribute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Review_Count'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mdownload_rankings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mdf_ranking_by_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ranking_by_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ranking_by_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mgroup_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ranking_by_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ranking_by_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ranking_by_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-806863330c7f>\u001b[0m in \u001b[0;36mdownload_rankings\u001b[0;34m(business_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_rankings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# Business URL: 'https://www.yelp.com/biz/noche-de-margaritas-new-york'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdownload_ranking_by_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mdownload_ranking_by_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdownload_ranking_by_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9917859fd62b>\u001b[0m in \u001b[0;36mdownload_ranking_by_relevance\u001b[0;34m(business_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_ranking_by_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mrandom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ranking_from_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.yelp.com/biz/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbusiness_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relevance_desc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ranking_by_relevance.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#Pickling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-c4f9d1948558>\u001b[0m in \u001b[0;36mget_ranking_from_call\u001b[0;34m(url_business, lang, sort, query, is_random)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# print(json.dumps(reviews, indent=4, sort_keys=True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX8ibEsAIxCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline1():\n",
        "  ############################# EXECUTE ON COLAB #############################################\n",
        "  ### READ THE RANKING FROM FILE\n",
        "  with open(\"ranking_by_relevance.txt\", \"rb\") as fp:   # Unpickling\n",
        "    ranking_by_relevance = pickle.load(fp)\n",
        "  ###\n",
        "  ### READ THE RANKING FROM FILE\n",
        "  with open(\"ranking_by_date.txt\", \"rb\") as fp:   # Unpickling\n",
        "    ranking_by_date = pickle.load(fp)\n",
        "  ###\n",
        "  ### READ THE RANKING FROM FILE\n",
        "  with open(\"ranking_by_random.txt\", \"rb\") as fp:   # Unpickling\n",
        "    ranking_by_random = pickle.load(fp)\n",
        "  ###\n",
        "\n",
        "  # df_ranking = find_users_in_dataset(ranking)  # cannot find all users\n",
        "\n",
        "  df_ranking_by_relevance = create_df_users(ranking_by_relevance)\n",
        "  df_ranking_by_date = create_df_users(ranking_by_date)\n",
        "  df_ranking_by_random = create_df_users(ranking_by_random)\n",
        "  \n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  print(df_ranking_by_relevance)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_ranking_by_date)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_ranking_by_random)\n",
        "  print('\\n')\n",
        "\n",
        "  return df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random\n",
        "\n",
        "def pipeline2(attribute, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "  print(\"ATTRIBUTE: \" + attribute + \"\\n\")\n",
        "  group_list = create_objective_groups_by_percentile(df_ranking_by_relevance, attribute, 10) # number of groups to be created\n",
        "  # group_list = create_objective_groups_by_size(df_ranking_by_relevance, attribute, 10)\n",
        "  index = 1\n",
        "  for group in group_list:\n",
        "      print(\"\\nGROUP \" + str(index) + \":\")\n",
        "      for person in group:\n",
        "          print(person['user_id'] + \", \" + str(person['Review_Count']))\n",
        "      index = index + 1\n",
        "  return group_list\n",
        "\n",
        "def pipeline3(attribute, group_list, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random):\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\")\n",
        "  index = 0\n",
        "  yelp_exposures = []\n",
        "  for group in group_list:\n",
        "      current_exp = exposure(group, df_ranking_by_relevance)\n",
        "      yelp_exposures.append((index, current_exp))\n",
        "      print(\"\\nEXPOSURE of group \" + str(index) + \" is: \" + str(current_exp))\n",
        "      index = index + 1\n",
        "  print('\\n')\n",
        "  print(\"------------Ranking by Date:------------\")\n",
        "  index = 0\n",
        "  date_exposures = []\n",
        "  for group in group_list:\n",
        "      current_exp = exposure(group, df_ranking_by_date)\n",
        "      date_exposures.append((index, current_exp))\n",
        "      print(\"\\nEXPOSURE of group \" + str(index) + \" is: \" + str(current_exp))\n",
        "      index = index + 1\n",
        "  print('\\n')\n",
        "  print(\"------------Ranking Random:------------\")\n",
        "  index = 0\n",
        "  random_exposures = []\n",
        "  for group in group_list:\n",
        "      current_exp = exposure(group, df_ranking_by_random)\n",
        "      random_exposures.append((index, current_exp))\n",
        "      print(\"\\nEXPOSURE of group \" + str(index) + \" is: \" + str(current_exp))\n",
        "      index = index + 1\n",
        "  print('\\n')\n",
        "\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, 'plot_'+attribute+'_'+business_id)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPGOC05EljLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_plots(yelp_exposures, date_exposures, random_exposures, title):\n",
        "  width = 0.27\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5\n",
        "\n",
        "  x1 = [el[0] - width for el in yelp_exposures]\n",
        "  x2 = [el[0] for el in date_exposures]\n",
        "  x3 = [el[0] + width for el in random_exposures]\n",
        "\n",
        "  y1 = [el[1] for el in yelp_exposures]\n",
        "  y2 = [el[1] for el in date_exposures]\n",
        "  y3 = [el[1] for el in random_exposures]\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='yelp')\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date')\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0))\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0M0ETqjMrqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_df_users(ranking):\n",
        "  df_ranking = pd.DataFrame(ranking, columns=['Position', 'user_id', 'Review_Count']).sort_values('Position')\n",
        "  return df_ranking"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awAE4AWWxEFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_percentile(df_users, attribute, N_of_groups):\n",
        "  # order df_users by review_count\n",
        "  df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "  group_list = []\n",
        "  percentile_up_value = math.ceil(100/N_of_groups)\n",
        "  percentile_down_value = int(100/N_of_groups)\n",
        "  initial_percentile_value = percentile_down_value\n",
        "  array_of_values = df_users[attribute].tolist()\n",
        "  index = 0\n",
        "  prec_max_value = -1\n",
        "  while percentile_up_value <= 100 or percentile_down_value <= 100:\n",
        "    group_list.append([])\n",
        "    if percentile_up_value > 100:\n",
        "      percentile_up_value = 100\n",
        "    max_value_in_group = np.percentile(array_of_values, percentile_up_value)\n",
        "    for i, user in df_users.iterrows():\n",
        "      if user[attribute] <= max_value_in_group and user[attribute] > prec_max_value:\n",
        "        group_list[index].append(user)\n",
        "    index = index + 1\n",
        "    percentile_up_value = percentile_up_value + initial_percentile_value\n",
        "    percentile_down_value = percentile_down_value + initial_percentile_value\n",
        "    prec_max_value = max_value_in_group\n",
        "\n",
        "  return group_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbpLTiPSWuL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exposure(group, ranking):\n",
        "    return sommatory(group, ranking)/(len(group))\n",
        "\n",
        "\n",
        "def sommatory(group, ranking):  # ranking is a list of user_id\n",
        "    sum = 0\n",
        "    for user in group:\n",
        "        user_id = user['user_id']\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'Position'].tolist()[0]\n",
        "        # position = ranking.index[ranking['user_id'] == user_id].tolist()[0]\n",
        "        # position = position + 1\n",
        "        sum = sum + exp(position)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/np.log(1 + position)\n",
        "\n",
        "\n",
        "def demographic_parity(g1, g2, s):\n",
        "    ex1 = exposure(g1, s)\n",
        "    ex2 = exposure(g2, s)\n",
        "    print(\"|\" + str(ex1) + \" - \" + str(ex2) + \"| < 0.1?\")\n",
        "    return np.absolute(ex1 - ex2) < 0.1  # in our case, g1 and g2 are groups of author (reviewer)\n",
        "\n",
        "\n",
        "def app(author, df):\n",
        "    return df[df['UserId'] == author].iloc[0, -1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUr0D9xWp5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_size(df_users, attribute, range):\n",
        "    # order df_users by review_count\n",
        "    df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "    group_list = [[]]\n",
        "    i = 0\n",
        "    group_index = 0\n",
        "    prec = -1\n",
        "    for index, user in df_users.iterrows():\n",
        "        if i == range:\n",
        "            i = 0\n",
        "            group_list.append([])\n",
        "            group_index = group_index + 1\n",
        "        temp = user[attribute]\n",
        "        if temp != prec:\n",
        "            prec = temp\n",
        "            i = i + 1\n",
        "        group_list[group_index].append(user)\n",
        "    print(\"Number of groups created: \" + str(group_index+1))\n",
        "    return group_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6gUlnqBZ9eoa",
        "colab": {}
      },
      "source": [
        "# TODO: get review_count from web page\n",
        "def get_ranking(url):\n",
        "\n",
        "  i=1\n",
        "  reviews_text = []\n",
        "  reviews_date = []\n",
        "  reviews_userid = []\n",
        "  x=0\n",
        "\n",
        "  while 1:\n",
        "    if x == 0:\n",
        "      page_content = requests.get(url)\n",
        "    else:\n",
        "        page_content = requests.get(url + '?start=' + str(x))\n",
        "    x = x + 20\n",
        "    \n",
        "    tree = html.fromstring(page_content.content)\n",
        "    recommended_reviews_text = \"Recommended Reviews\"\n",
        "    reviews_list = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/*' % recommended_reviews_text)\n",
        "    if not reviews_list:\n",
        "      break\n",
        "    else:\n",
        "      # Index for ranking\n",
        "      j=1\n",
        "      for review in reviews_list:\n",
        "          reviews_text.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[p]/p/span' % (recommended_reviews_text, j))[0].text))\n",
        "          reviews_date.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[1]/div/div[2]/span' % (recommended_reviews_text, j))[0].text))\n",
        "          user_link = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div/div/div/div/div/div/a/@href' % (recommended_reviews_text, j))[0]\n",
        "          reviews_userid.append((i,user_link[user_link.find('=')+1:]))\n",
        "          i = i + 1\n",
        "          j = j + 1\n",
        "\n",
        "\n",
        "  print(reviews_text)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_text)))\n",
        "  print(reviews_date)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_date)))\n",
        "  print(reviews_userid)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_userid)))\n",
        "  return reviews_userid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHl6bORqNzeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_users_in_dataset(users):\n",
        "    data = []\n",
        "    with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            data.append(json_data)\n",
        "    df = pd.DataFrame(data)\n",
        "    df_ranking = pd.DataFrame(users, columns=['Position', 'user_id'])\n",
        "    df_merged = df_ranking.merge(df, on='user_id')\n",
        "    # temp = df[df['user_id'].isin(users)]\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def exists_in_dataset(userid):\n",
        "  found = False\n",
        "  with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            if json_data['user_id'] == userid:\n",
        "              found = True\n",
        "  if found:\n",
        "    print(\"Trovato\")\n",
        "  else:\n",
        "    print(\"Non trovato\")\n",
        "  return found\n",
        "\n",
        "\n",
        "def find_users_in_chopped_dataset(users):\n",
        "    found = False\n",
        "    data = []\n",
        "    i = 1\n",
        "    j = 100000\n",
        "    df_users = pd.DataFrame()\n",
        "    while not found:\n",
        "        path = \"user\" + str(i) + \"-\" + str(j) + \".json\"\n",
        "        try:\n",
        "            with open(path, 'r') as file:\n",
        "                for line in file:\n",
        "                    json_data = json.loads(line)\n",
        "                    data.append(json_data)\n",
        "            df = pd.DataFrame(data)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Cannot find all users into the dataset\")\n",
        "            return 0\n",
        "        else:\n",
        "            temp = df[df['user_id'].isin(users)]  # df of users of the ranking\n",
        "            df_users = df_users.append(temp)\n",
        "            if df_users.shape[0] == len(users):\n",
        "                found = True\n",
        "            else:\n",
        "                i = i + 100000\n",
        "                j = j + 100000\n",
        "    return df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}