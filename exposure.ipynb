{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exposure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/exposure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPpCkgv0271l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import datetime\n",
        "import math\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics as st\n",
        "from scipy import stats\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#!pip install balanced_kmeans\n",
        "#from balanced_kmeans import kmeans\n",
        "#from balanced_kmeans import kmeans_equal\n",
        "\n",
        "import networkx as nx\n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7PR_gGjwNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONSTANTS\n",
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n",
        "\n",
        "FOLDER = {'WbJ1LRQdOuYYlRLyTkuuxw':\n",
        "            {'FID_RANKING': '1U0Ml5puObfOk3qHCKhO9bQ06WyhJoffW',\n",
        "             'FID_G1': '1cauvqKXzF5VhiveP84aL3U0oDrXP32XV',\n",
        "             'FID_G2': '1vC1fc2owpCis_PTl1ABKT6BfG6-2yoiJ',\n",
        "             'FID_G3': '1_iKbmJkBwWEEIIh73UvR431Ss6xnGb2f',\n",
        "             'FID_G4': '1m_feamMvpx5B5mTe_lH6qlhoZFEpwlSs',\n",
        "             'FID_G5': '1qJgFbR4FUX5t7O9h5cHgp6aE6lpfbdOs',\n",
        "             'FID_G6': '1BSQrghVusO_eNbe10fFE4-bVfRK8ly0y',\n",
        "             'FID_G7': '1fJD4WIErH-u5plesyTePCAQ3IyfjoOKf',\n",
        "             'FID_G8': '1fD_fdl-1KAzXCjPOsEvFOZS3u354XhLL',\n",
        "             'FID_G9': '1ZdiaS2cZP9Ks3rheSSVaoLpw-qMYF9EA',\n",
        "             'FID_G10': '',\n",
        "             'FID_G11': '1ce7VQPdbQ9ieimmpz66RaHgbzEFa4TsT',\n",
        "             'FID_G12': '1tf-KwOYXdMGS3uYdoe8Kc4LV-rdxHY_3',\n",
        "             'FID_G13': '1zrIsEjhaSqDfGqhBBZu5cYg7_9RsXLcQ',\n",
        "             'FID_G14': '1ry0OF9h4oJVYwF6Uw4AF18SIYKJUc0vd'\n",
        "             },\n",
        "          'T2tEMLpTeSMxLKpxwFdS3g':\n",
        "            {'FID_RANKING': '136FZ0Y90Zx-4UYDyW50cX8zUa_DhN3XS',\n",
        "             'FID_G1': '1eiju1aTzsRi4QQZ8XrSouqcLtFSN-tbj',\n",
        "             'FID_G2': '1VuU8MAcXgN_E85q9SH_P1HsHVkeJakwH',\n",
        "             'FID_G3': '1L240ycUElvmF0R4l8PbnuoBbqQY6q1og',\n",
        "             'FID_G4': '1S4oTf-ZSD8q9aTOjrDEA2KeNJ99uh-2n',\n",
        "             'FID_G5': '1u9ZFeCwmol1uXRH-l8djJ2hst0B01VI3',\n",
        "             'FID_G6': '19TgwV3uHOtDDJ0a8krpvLUeooe9rhQFJ',\n",
        "             'FID_G7': '11YpFHw34cDglPD87W8yWvRM1VAOWXPtF',\n",
        "             'FID_G8': '1tR-GotUUNgiilJLBxvr7PkUpbPG5F_J7',\n",
        "             'FID_G11': '1tDHkZXXxP2vVypoxaJlyMUnK6nFB2fk6',\n",
        "             'FID_G12': '1qQKWPP3hK59ZY_Gh5E5_YzPaWPUsUnoa',\n",
        "             'FID_G14': '1pTbQYeLXVTzZTIDdHOtmaWZER5h_QYBK',\n",
        "             },\n",
        "          'ALwAlxItASeEs2vYAeLXHA':\n",
        "            {'FID_RANKING': '12zXi3XyQaNgukGHW_805cyrkHhSdF7Df',\n",
        "             'FID_G1': '1NB-isOm1cDArAlwIMDMou1Q_XAap09KA',\n",
        "             'FID_G2': '1ELMUBbKGryblHDnbh7ghiIgMdZ48pBG1',\n",
        "             'FID_G3': '1QZ90pwDjBoaU41P8wP8v_I9tsuhObEUP',\n",
        "             'FID_G4': '1MXivbV9VGB0vBGGuLhYu0zJlpv6O-2Xb',\n",
        "             'FID_G5': '1bTetFx3Jhy5QbRKYGV09muMy7D1NKmJT',\n",
        "             'FID_G6': '1F_7bdMS5MW4sIpyUbDz62ZTYJtHoouGG',\n",
        "             'FID_G7': '1uRFceNIlYk5vQXEGd2H6yanrlwrSIM9V',\n",
        "             'FID_G11': '12jqKYjkUjmiJ0Iwt1VkYptcTPx2zVI2P',\n",
        "             'FID_G12': '1CLzUDx8dhd0O_vc4LjIZiz_7f1jTaXBH',\n",
        "             'FID_G14': '1Ef87ewaZbBr6OmMdXAK8kZ3-0tIfXVj8',\n",
        "             },\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw':\n",
        "            {'FID_RANKING': '1ZyNQavpG0akr3ca3PJjjl_D89IA5wlcc',\n",
        "             'FID_G1': '1kQkAi_9V3mSld1LlP50j6uw0bwqnkLrR',\n",
        "             'FID_G2': '1hGR4eZiP3Q9Etn7RdlY58IsNsTMMTydz',\n",
        "             'FID_G3': '1YcAPHxioFpiVXWndO5tK17US1h36cWgK',\n",
        "             'FID_G4': '1NxzzH0l18fGaO4bZ35D9NMq2CeW5BRVP',\n",
        "             'FID_G5': '1HC1wZGPtbb8UvRlfbGoHl_ijl3G67Ei4',\n",
        "             'FID_G6': '1J_15pO-CFbHR_0YsHK5kDnKp49kDL0kZ',\n",
        "             'FID_G7': '1RTwCqhYzB3djzpe3-uwbQ3Ur5P8YoECa',\n",
        "             'FID_G11': '1fsugmUSzPQ9o1MjnwcYWCQlihHseMyaA',\n",
        "             'FID_G12': '129UptTzu0etAIRP-rKI5BV-JHXzKEe6B',\n",
        "             'FID_G14': '1ivduQ-XS-1sUBmJIb1j-ZDGGG5xxM5H5',\n",
        "             },\n",
        "          'Sovgwq-E-n6wLqNh3X_rXg':\n",
        "            {'FID_RANKING': '1EZqvt9x5PN07BgUad1RtNIUXTVqujA0g',\n",
        "             'FID_G1': '1pkUEBYeZ66GfIvXoTkzZ28gGFlZNWmEq',\n",
        "             'FID_G2': '1lJzHAfBlvL9_EgNizH4vnyPTn-j-Vbrx',\n",
        "             'FID_G3': '1NRYqjUsQJTwgB0JXtcCx7lQErirMH1TK',\n",
        "             'FID_G4': '1nLtZbS0NGXKAs6aETNs7EWasNs8omaw6',\n",
        "             'FID_G5': '14w-jVhOuod3s_EYSipvS92SPx_3zT_FD',\n",
        "             'FID_G6': '1tbUyiamFmyAoeA3W6j51x97h3MjUSVsd',\n",
        "             'FID_G7': '1TNAmibHwJHkvNajHhRffN1gEY0-QnBZH',\n",
        "             'FID_G11': '1-WKO0_zGQdw__C3Iq1W_2zYAy2Fv57Ky',\n",
        "             'FID_G12': '1GSvB1qJLMBwrBiVMcsWb_9Kgk7M2VGCI',\n",
        "             'FID_G14': '1MXc5Z-KchS-NhvILmmRIAk64ElB7a17f',\n",
        "             },\n",
        "          'j5nPiTwWEFr-VsePew7Sjg':\n",
        "            {'FID_RANKING': '18bQVXYZ03vIpfEFLPh8I2i7cPTlokGxv',\n",
        "             'FID_G1': '1U8STZ7irZLPcUpP1ALR6QYIFnoXB2tbE',\n",
        "             'FID_G2': '1xScKc0_DlnQZucHLeb26hLqpgjqMA3RH',\n",
        "             'FID_G3': '1JFYl2eKtDyWbsyVdBpTXsvPn2h6MO3Cr',\n",
        "             'FID_G4': '1aFYpJmymdnHxquuqbbMwXzN5wG-3Zf2O',\n",
        "             'FID_G5': '1NfHlsBTk87jscmZfagtnd--kcdpy8t_8',\n",
        "             'FID_G6': '1b88afNqrZSRptYA0pL3lCEV49mFaEP6Q',\n",
        "             'FID_G7': '15rDbY-MTUBVjHxOedbenZIjTxQ_wEAZW',\n",
        "             'FID_G11': '1gNed-nYqKpMRi28OW5GGKNvnYsD-X9Lq',\n",
        "             'FID_G12': '109w17IhYMSdyJbTsjktw6mW5uIiPd54L',\n",
        "             'FID_G14': '1B9SAbA0-BB5EIFAT1x8ZgBJI543L9woa',\n",
        "             },\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg':\n",
        "            {'FID_RANKING': '1nl6A997UnuR5ceYRZS1242JJCvK-Sf_u',\n",
        "             'FID_G1': '1hrssLlhPmnS48yVxXpwBBIuhL1YdDNhj',\n",
        "             'FID_G2': '1shLPc3aUsVaxeZhZ_SvDS3agPFlMw1dn',\n",
        "             'FID_G3': '13eUtYFVp-KKjxV6-mhCsxQGHkKFOgB82',\n",
        "             'FID_G4': '1nQVPHZT4sHpBXD32qX_x41VywkmyppFj',\n",
        "             'FID_G5': '1gJpfGhb721Q64gynq4jSGMis5pDA7qAx',\n",
        "             'FID_G6': '1WbZvolaSwambWOKHx4v4eFIV-KUzygGF',\n",
        "             'FID_G7': '1IWyQpgMjLny2JTbw0nUuHkNRPV5CCA_A',\n",
        "             'FID_G11': '1-DDmHcBg6zhDgEYJBi28UXX_ZF8pQIxl',\n",
        "             'FID_G12': '1MtfQ3Em5so4Z9o7rIxA1-pm0o7TJiMO4',\n",
        "             'FID_G14': '1sxBZUfSHX9H9YCiLH2rmZu4_3JkiFkZS',\n",
        "             },\n",
        "          'e4NQLZynhSmvwl38hC4m-A':\n",
        "            {'FID_RANKING': '1K566Y5Q2N6Lw7S6yDicKC_R-zFVRqnFr',\n",
        "             'FID_G1': '1RNxub2faGfc4NAFvau5SsMADAzBlQd2j',\n",
        "             'FID_G2': '16tjv4k5CJSOxwgvC0wDJU67MEjRaw_0o',\n",
        "             'FID_G3': '1p-9HrVncD_FZOxO0YzHiY7KTe47ugSTQ',\n",
        "             'FID_G4': '1qGpECepAyJjqyfPx8Q94odPOCHbOixtZ',\n",
        "             'FID_G5': '15AGLE1YT0v8tQ3r6i6k7j5XaNORToUct',\n",
        "             'FID_G6': '1nNbAPulqJCzO6urHNsqc4qzSM7Yih8uk',\n",
        "             'FID_G7': '1o9Ew39-Q9VO3wjEyv4qGJ9YlfRmkzTed',\n",
        "             'FID_G11': '19PkXHjCyMmNJwgKyA05YZyLV6-a8kZ1r',\n",
        "             'FID_G12': '1Yw6orSOfL16rgwR1Jx5vZ0gMET_htnaU',\n",
        "             'FID_G14': '11QOBb79y1g-sy9xQxwihoiTZrOOIKm6l',\n",
        "             },\n",
        "          'S-oLPRdhlyL5HAknBKTUcQ':\n",
        "            {'FID_RANKING': '1FJqfJgaJinXS3oIvdA53FMuO9O7bCDVn',\n",
        "             'FID_G1': '1CcLHVEA41AgSzJKl37YlcJKf-o5NaCUo',\n",
        "             'FID_G2': '1j-lp1gcu6YGTwcOV4aYiY7dh2G9_TL6J',\n",
        "             'FID_G3': '1I-Qn9oz4VopXbpj5i8_iqLvcSp8jpb2_',\n",
        "             'FID_G4': '1gsS9uFbRyI18ifF5yLp20WHhP66gdr57',\n",
        "             'FID_G5': '1LJIVm9tCZMcgzBkGilYem0nqKoUHpdpX',\n",
        "             'FID_G6': '1syB0wF48fiSo0_uW9dedS-ZlFfXUv1B8',\n",
        "             'FID_G7': '1cBPjBprJ19RxI6-Bcpb-Fifv0s1bQSV2',\n",
        "             'FID_G11': '1j19CTwpDdr6jnvUAWQ4DmgZD78auPqrm',\n",
        "             'FID_G12': '1XSdznjVER62XkIjg-g-4AhRerAGM_mut',\n",
        "             'FID_G14': '1x83XV4b38hQvTo2H6-NAZ0kriB5uBhxq',\n",
        "             }\n",
        "        }\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bw0Twhc3qgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upload_file(filename, folder_id):\n",
        "  drive = authenticate()\n",
        "  fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "  drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Check if file already exists in Google Drive (prevents duplicates)\n",
        "  for file in fileList:\n",
        "      if file['title'] == filename:  # The file already exists, then overwrite it\n",
        "          fileID = file['id']\n",
        "          drive_file = drive.CreateFile({'id': fileID, 'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Upload user picture on Google Drive\n",
        "  drive_file.SetContentFile(filename)  # path of local file content\n",
        "  drive_file.Upload()  # Upload the file.\n",
        "  return drive_file['id']\n",
        "  \n",
        "\n",
        "def create_folder_in_drive(gdrive, folder_name, parent_folder_id):\n",
        "  folder_metadata = {'title': folder_name,'mimeType': 'application/vnd.google-apps.folder',\n",
        "                    'parents': [{\"kind\": \"drive#fileLink\", \"id\": parent_folder_id}]\n",
        "                    }\n",
        "  folder = gdrive.CreateFile(folder_metadata)\n",
        "  folder.Upload()\n",
        "  print(folder)\n",
        "  # Return folder informations\n",
        "  print('title: %s, id: %s' % (folder['title'], folder['id']))\n",
        "  return folder['id']\n",
        "\n",
        "\n",
        "def set_file_destination(lst, method, id):\n",
        "  if method == 'ranking':\n",
        "    return FOLDER[id]['FID_RANKING']\n",
        "  if lst == ['review_count']:\n",
        "    return FOLDER[id]['FID_G1']\n",
        "  if lst == ['fans']:\n",
        "      return FOLDER[id]['FID_G6']\n",
        "  if method == 'kmeans':\n",
        "    if lst == ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']:\n",
        "      return FOLDER[id]['FID_G2']\n",
        "    if lst == ['loc1', 'loc2', 'loc3']:\n",
        "      return FOLDER[id]['FID_G3']\n",
        "    if lst == ['age', 'gender', 'ethnicity']: \n",
        "      return FOLDER[id]['FID_G4']\n",
        "    if lst == ['useful','funny', 'cool']:\n",
        "      return FOLDER[id]['FID_G7']\n",
        "    if lst == ['review_count', 'friend_count', 'photo_count']:\n",
        "      return FOLDER[id]['FID_G13']\n",
        "  if method == 'balanced_kmeans':\n",
        "    if lst == ['age', 'gender', 'ethnicity']:\n",
        "      return FOLDER[id]['FID_G8']\n",
        "    if lst == ['loc1', 'loc2', 'loc3']:\n",
        "      return FOLDER[id]['FID_G9']\n",
        "  if method == 'custom':\n",
        "    if lst == ['gender', 'ethnicity']:\n",
        "      return FOLDER[id]['FID_G12']\n",
        "    if lst == ['review_sentiment']:\n",
        "      return FOLDER[id]['FID_G5']\n",
        "    if lst == ['ethnicity']:\n",
        "      return FOLDER[id]['FID_G11']\n",
        "    if lst == ['gender']:\n",
        "      return gtree.loc[gtree['business_id'] == id, 'gfolder_#14'].tolist()[0]\n",
        "      #return FOLDER[id]['FID_G14']\n",
        "  if method == 'dbscan':\n",
        "    if lst == ['review_count', 'friend_count', 'photo_count']:\n",
        "      return FOLDER[id]['FID_G13']\n",
        "\n",
        "\n",
        "\n",
        "# READ JSON\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "  cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "  return df[cols]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1kK6DE97Nu4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8336edef-0c05-427a-fac5-704f7d7df7b9"
      },
      "source": [
        "# DOWNLOAD restaurants.txt FROM DRIVE\n",
        "restaurants_dataset_id = '1BqMsph8flQMGmcE_WxYV0vSC_uqi-nge'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': restaurants_dataset_id})\n",
        "download.GetContentFile('restaurants.txt')\n",
        "with open('restaurants.txt') as f:\n",
        "    content = f.readlines()\n",
        "restaurants_id_list = [x.strip() for x in content] \n",
        "\n",
        "print(restaurants_id_list)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['WbJ1LRQdOuYYlRLyTkuuxw', 'T2tEMLpTeSMxLKpxwFdS3g', 'ALwAlxItASeEs2vYAeLXHA', 'OVTZNSkSfbl3gVB9XQIJfw', 'Sovgwq-E-n6wLqNh3X_rXg', 'j5nPiTwWEFr-VsePew7Sjg', 'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A', 'S-oLPRdhlyL5HAknBKTUcQ', 'VyVIneSU7XAWgMBllI6LnQ', 'pSQFynH1VxkfSmehRXlZWw', 'JzOp695tclcNCNMuBl7oxA', 'OgJ0KxwJcJ9R5bUK0ixCbg', '3l54GTr8-E3XPbIxnF_sAA', '9a3DrZvpYxVs3k_qwlCNSw', 'frCxZS7lPhEnQRJ3UY6m7A', 'yNPh5SO-7wr8HPpVCDPbXQ', '0FUtlsQrJI7LhqDPxLumEw', 'K-uQkfSUTwu5LIwPB4b_vg', 'L2p0vO3fsS2LC6hhQo3CzA', 'd10IxZPirVJlOSpdRZJczA', 'wUKzaS1MHg94RGM6z8u9mw', 'z6-reuC5BYf_Rth9gMBfgQ', 'aiX_WP7NKPTdF9CfI-M-wg', '3C5Z9homtzkWHouH2BHXYQ', 'C8D_GU9cDDjbOJfCaGXxDQ', 'Yl05MqCs9xRzrJFkGWLpgA', 'eS29S_06lvsDW04wVrIVxg', 'IsoLzudHC50oJLiEWpwV-w', '3N9U549Zse8UP-MwKZAjAQ', '_XN-GwzZwAyIqLKJsl2htg', 'r5PLDU-4mSbde5XekTXSCA', 'Iq7NqQD-sESu3vr9iEGuTA', 'u-SJ5QUwrNquL9VnXwl8cg', 'sJNcipFYElitBrtiJx0ezQ', '7m1Oa1VYV98UUuo_6i0EZg', 'e4NQLZynhSmvwl38hC4m-A', 'k1QpHAkzKTrFYfk6u--VgQ', '-6tvduBzjLI1ISfs3F_qTg', 'jcasci3gjbsSuTEVzvDQKg', 'Ch7NAhB_MWSDwcNbcptEKg', 'eZDXz_RylvdD0tHEA8I0NA', 'tCSlpwJQ4CZsUEMZeH2SFg', 'yWUBX9Pe6pzh1PjVj04UmQ', 'k-drEjxKmfqllwfY90STfA', 'y1eeVRfJa22CCpUCeNfrSw', 'VH3WA7a-OVzFj2K_SP4BIw', 'osSwv6CJy5hDKQdOKeyTow', 'o5QDrg_kb4hgsWxV6cV_Uw', 'XgRljuEUyaHBKIpIz-PRAA', 'wArcCMVnrl_tc9MULW-0CQ', 'SHFlELFcEcAOJv_fTAKChQ', 'aBLx9JlAMq_AuW6VAImSwg', 'd8lmIZIqmBC9oPM8y1dc7Q', 'bzbNGyrTwWHAgg1CqkSgeg', 'ADgacmZ-qXrSOhMfU6bmTA', 'gChmBjLSe3qa8rzEngoDLQ', 'D2ojX9bvE0_-aIj9BhdZZA', 'hggVnGwA5042-ABxqeJX-A', '9dqdzRzjSfbnMvjUGqWBAw', 'DyOzNIeXUksPNLXYHnB-oQ', 'mC39IrCp36QIVFRZIw9PTQ', 'sdYkVaTy7EJwUkO8Ie_qPg', 'ALwAlxItASeEs2vYAeLXHA', '01fuY2NNscttoTxOYbuZXw', 'T2tEMLpTeSMxLKpxwFdS3g', 'RVQE2Z2uky4c0-njFQO66g', 'WbJ1LRQdOuYYlRLyTkuuxw', 'RAh9WCQAuocM7hYM5_6tnw', '3kdSl5mo9dWC4clrQjEDGg', '7sPNbCx7vGAaH7SbNPZ6oA', 'Cni2l-VKG_pdospJ6xliXQ', 'DkYS3arLOhA8si5uUEmHOw', 'f4x1YBxkLrZg652xt2KR5g', 'faPVqws-x-5k2CQKDNtHxw', 'fL-b760btOaGa85OJ9ut3w', 'G-5kEa6E6PD5fkBRuA7k9Q', 'g8OnV26ywJlZpezdBnOWUQ', 'HhVmDybpU7L50Kb5A0jXTg', 'hihud--QRriCYZw1zZvW4g', 'IWN2heYitkg-D4UdqfxcMA', 'JDZ6_yycNQFTpUZzLIKHUg', 'JpgVl3d20CMRNjf1DVnzGA', 'mDR12Hafvr84ctpsV6YLag', 'NvKNe9DnQavC9GstglcBJQ', 'OETh78qcgDltvHULowwhJg', 'P7pxQFqr7yBKMMI2J51udw', 'pH0BLkL4cbxKzu471VZnuA', 'QJatAcxYgK1Zp9BRZMAx7g', 'QXV3L_QFGj8r6nWX2kS2hA', 'RwMLuOkImBIqqYj4SSKSPg', 'ujHiaprwCQ5ewziu0Vi9rw', 'UPIYuRaZvknINOd1w8kqRQ', 'xkVMIk_Vqh17f48ZQ_6b0w', 'XXW_OFaYQkkGOGniujZFHg', 'XZbuPXdyA0ZtTu3AzqtQhg', 'yfxDa8RFOvJPQh0rNtakHA', 'YJ8ljUhLsz6CtT_2ORNFmg', 'ZkGDCVKSdf8m76cnnalL-A']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrfK_M25rAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD user.json FROM DRIVE\n",
        "users_dataset_id = '1s9dxkpeg0LQ7V4oNgzD-ePO6CXDpGo7y'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': users_dataset_id})\n",
        "download.GetContentFile('user.json')\n",
        "users = read_json('user.json')\n",
        "users = pd.DataFrame(users)\n",
        "\n",
        "# DOWNLOAD business.json FROM DRIVE\n",
        "business_dataset_id = '1VRpLAxR7wIhkcLMydp4J4TSTv-xSIwWd'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': business_dataset_id})\n",
        "download.GetContentFile('business.json')\n",
        "business = read_json('business.json')\n",
        "business = pd.DataFrame(business)\n",
        "\n",
        "# DOWNLOAD review.json FROM DRIVE\n",
        "review_dataset_id = '1XWywtKEh34KuIT7y-JTcq-DCK-tajb_8'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': review_dataset_id})\n",
        "download.GetContentFile('review.json')\n",
        "reviews = read_json('review.json')\n",
        "reviews = pd.DataFrame(reviews)\n",
        "\n",
        "# DOWNLOAD DEMOGRAPHICS\n",
        "'''id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A', 'S-oLPRdhlyL5HAknBKTUcQ']\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "file_id_list = {'WbJ1LRQdOuYYlRLyTkuuxw':['1uuA0QH-DJklvtiekxS-7yaXfKRi9ux06'],\n",
        "#                                          '13amOFvuku27snF8mea8Yp7rH36Mve42N',\n",
        "#                                          '1VskD_0Ijwe3_fzVbgYqK9Bk-YOCK046t',\n",
        "#                                          '1uuA0QH-DJklvtiekxS-7yaXfKRi9ux06'],\n",
        "                'T2tEMLpTeSMxLKpxwFdS3g':['102OYWJOKF54GMnFh4_AKc5e_Mz7xiXZt'],\n",
        "                                          #'11fM_aHYkCLQ2z3g7Z76aZZ3cAsIrpezN',\n",
        "                                          #'102OYWJOKF54GMnFh4_AKc5e_Mz7xiXZt',\n",
        "                                          #'1ObvjBbxRNIp1vOQipUGZxZEi_eQH7kJQ'],\n",
        "                'ALwAlxItASeEs2vYAeLXHA':['1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk'],\n",
        "                                          #'1crTNpDR2VjBdGSgavIvN4QNhwTUR3-Bk',\n",
        "                                          #'1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk',\n",
        "                                          #'1tgJnMW0ZImx1vbDzWpvFBRX0FJu1Kdbt'],\n",
        "                'OVTZNSkSfbl3gVB9XQIJfw':['12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF'],\n",
        "                                          #'1Vmkzpdi_0m9CGcyp5ggeNsUsueBzqufH',\n",
        "                                          #'12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF',\n",
        "                                          #'1qecYhCBytq1Z6lm_ueCWIuithvRG0wIq'],\n",
        "                'Sovgwq-E-n6wLqNh3X_rXg':['1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT'],\n",
        "                                          #'1VVePrvWR7e5XkRmhp5Q5qOPXPskVmnnt',\n",
        "                                          #'1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT',\n",
        "                                          #'1R5GFYo2c2YC_AOB9G0pdOfROTyglePDI'],\n",
        "                'j5nPiTwWEFr-VsePew7Sjg':['16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                                          #'1txkAzGKMRjXku18rv9222q-FqVo42A9o',\n",
        "                                          #'1e8iSM3SpyB2Y_h6rzRSVVgphjHEIJnZU',\n",
        "                                          #'16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                'aiX_WP7NKPTdF9CfI-M-wg':['1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                                          #'1VrVgBoJp5cRVMH4I-5uKvdF3b-PSfbmL',\n",
        "                                          #'1f7T7ksdCCGxhVF_RJU9zu38-r8aTTJcD',\n",
        "                                          #'1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                'e4NQLZynhSmvwl38hC4m-A':['1-myKdlTuCpUyPuK8OwHhglpQQGe7Bh-y'],\n",
        "                                          #'1yYG8ftjE1i9prP61ej5EdGfJDlweiK3C',\n",
        "                                          #'1KSdN24eXC-Bnk2_mTQR6oNaGll1KP5hO',\n",
        "                                          #'1-myKdlTuCpUyPuK8OwHhglpQQGe7Bh-y'],\n",
        "                'S-oLPRdhlyL5HAknBKTUcQ':['1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "                                          #'1qcX9pGfmogOokbBOPnZY-NbroolffAlg',\n",
        "                                          #'1YVdSND3VNit4ieORXC_mVzcv4mphGUnY',\n",
        "                                          #'1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "for business_id in id_list:\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for file_id in file_id_list[business_id]:\n",
        "    download = drive.CreateFile({'id': file_id})\n",
        "    download.GetContentFile('demographics_' + business_id + '.csv')\n",
        "    df_temp = pd.read_csv('demographics_' + business_id + '.csv')\n",
        "    df = df.append(df_temp)\n",
        "    \n",
        "  print(len(df.index))\n",
        "  df = df.sort_values(['id', 'age'], ascending=[True, False])\n",
        "  df = df.drop_duplicates('id',keep='first').reset_index(drop=True)\n",
        "  print('senza duplicati: ', len(df.index))\n",
        "  df = df.rename(columns={'id':'user_id'})\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('demographics_' + business_id + '.csv')\n",
        "  upload_file('demographics_' + business_id + '.csv',FOLDER[business_id]['FID_G4'])\n",
        "'''\n",
        "# DOWNLOAD SENTIMENT ANALYSIS\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A', 'S-oLPRdhlyL5HAknBKTUcQ']\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "file_id_list = {'WbJ1LRQdOuYYlRLyTkuuxw':[#'1z4r5wJJVHy65OGWo4VFBz0vviAebPr9D',\n",
        "                                          '1DsOTrTZepa8DAeSGf8v7suIwdc7bm8jB'],\n",
        "                                          #'19sQ-pbO2dGsax5dpGeLQBnSgX34GSu3t'],\n",
        "                'T2tEMLpTeSMxLKpxwFdS3g':['1SwQWQnchjrKmJHq6ZQIHCgsKKNvzs2B4'],\n",
        "                                          #'11fM_aHYkCLQ2z3g7Z76aZZ3cAsIrpezN',\n",
        "                                          #'1TPexYC2YHq_8ywZSuRRIJkrUCstwoxSp',\n",
        "                                          #'1ObvjBbxRNIp1vOQipUGZxZEi_eQH7kJQ'],\n",
        "                'ALwAlxItASeEs2vYAeLXHA':[#'1crTNpDR2VjBdGSgavIvN4QNhwTUR3-Bk',\n",
        "                                          '1kqjtQ5WQfPAUcOZNdpGZmn4tyTeqi3Ua'],\n",
        "                                          #'1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk',\n",
        "                                          #'1tgJnMW0ZImx1vbDzWpvFBRX0FJu1Kdbt'],\n",
        "                'OVTZNSkSfbl3gVB9XQIJfw':[#'1Vmkzpdi_0m9CGcyp5ggeNsUsueBzqufH',\n",
        "                                          '1wxP30ZDwl1hR_Ev3SNXB6Fpq9DMtBbD_'],\n",
        "                                          #'12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF',\n",
        "                                          #'1qecYhCBytq1Z6lm_ueCWIuithvRG0wIq'],\n",
        "                'Sovgwq-E-n6wLqNh3X_rXg':[#'1VVePrvWR7e5XkRmhp5Q5qOPXPskVmnnt',\n",
        "                                          '1np0WOU9aH6Iwoa-cgrlc6_EwfQRQ8pWG'],\n",
        "                                          #'1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT',\n",
        "                                          #'1R5GFYo2c2YC_AOB9G0pdOfROTyglePDI'],\n",
        "                'j5nPiTwWEFr-VsePew7Sjg':[#'1txkAzGKMRjXku18rv9222q-FqVo42A9o',\n",
        "                                          '1y76OtwUcjZ9e4Pokn55qk6ZS7e7qCtDQ'],\n",
        "                                          #'1e8iSM3SpyB2Y_h6rzRSVVgphjHEIJnZU',\n",
        "                                          #'16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                'aiX_WP7NKPTdF9CfI-M-wg':[#'1VrVgBoJp5cRVMH4I-5uKvdF3b-PSfbmL',\n",
        "                                          '1lV9wxvN4zYf2-VxpwoYVBncCpZQPxmTv'],\n",
        "                                          #'1f7T7ksdCCGxhVF_RJU9zu38-r8aTTJcD',\n",
        "                                          #'1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                'e4NQLZynhSmvwl38hC4m-A':[#'1yYG8ftjE1i9prP61ej5EdGfJDlweiK3C',\n",
        "                                          '1Ls9NjEU5FuCOyxsbbFVNEwCJioDO_e29'],\n",
        "                                          #'1KSdN24eXC-Bnk2_mTQR6oNaGll1KP5hO',\n",
        "                                          #'1PA2eYsYj2bj4Mo7ofbnvbQgYjmd2SwRG'],\n",
        "                'S-oLPRdhlyL5HAknBKTUcQ':[#'1qcX9pGfmogOokbBOPnZY-NbroolffAlg',\n",
        "                                          '1vVFHYIBf4Y_3G7neGfz7XFobNd13SUph']}\n",
        "                                          #'1YVdSND3VNit4ieORXC_mVzcv4mphGUnY',\n",
        "                                          #'1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "for business_id in id_list:\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for file_id in file_id_list[business_id]:\n",
        "    download = drive.CreateFile({'id': file_id})\n",
        "    download.GetContentFile('sentiment_' + business_id + '.csv')\n",
        "    df_temp = pd.read_csv('sentiment_' + business_id + '.csv')\n",
        "    df = df.append(df_temp)\n",
        "    \n",
        "  print(len(df.index))\n",
        "  df = df.drop_duplicates('id',keep='first').reset_index(drop=True)\n",
        "  print(len(df.index))\n",
        "  df = df.rename(columns={'id':'user_id'})\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('sentiment_' + business_id + '.csv')\n",
        "  upload_file('sentiment_' + business_id + '.csv',FOLDER[business_id]['FID_G5'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070u7qOzHsiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# JOIN USER FROM YELP WITH USER FROM DATASET AND PRINT LOST USERS\n",
        "def filter_user_from_dataset(df, users):\n",
        "  df = df[['position','user_id','date','location']] # Tolgo alcune colonne perch√© prendo quelle del dataset\n",
        "  df_merged = df.merge(users, on='user_id')\n",
        "  total_review = df['position'].max()\n",
        "  print('Utenti persi: totali ' + str(total_review) + ' - utenti nel dataset ' +\n",
        "        str(len(df_merged.index)) + ' = ' + str(total_review - len(df_merged.index)))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "  df_merged = drop_unnamed(df_merged)\n",
        "  return df_merged\n",
        "\n",
        "\n",
        "def compute_groups_percents(df_groups, N_of_groups):\n",
        "  total = len(df_groups.index)\n",
        "  percents = []\n",
        "  i = 0\n",
        "  while i < N_of_groups:\n",
        "    current_length = len(df_groups[df_groups['group_id'] == i].index)\n",
        "    percents.append((current_length/total)*100)\n",
        "    i = i + 1\n",
        "  print(percents)\n",
        "  return percents\n",
        "\n",
        "\n",
        "# ADD REVIEW INFO IN RANKING DATAFRAME\n",
        "def integrate_review_info(df, reviews, business_id):\n",
        "  business_reviews = reviews[reviews['business_id'] == business_id]\n",
        "  df_merged = business_reviews.merge(df, on='user_id')\n",
        "  del df_merged['date_y']  # it's date from yelp website (not updated in dataset)\n",
        "  df_merged = df_merged.rename(columns={'date_x':'date'})\n",
        "  # drop duplicates reviews from same user\n",
        "  df_merged = df_merged.sort_values('date').drop_duplicates('user_id',keep='last')\n",
        "  df_merged = df_merged.sort_values(by=['position']).reset_index(drop=True)\n",
        "  print('Review perse: ', len(df.index) - len(df_merged.index))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "  df_merged = drop_unnamed(df_merged)\n",
        "  df_merged = df_merged[['position', 'user_id', 'review_id', 'date', 'name', 'location', 'text', 'fans', 'average_stars', 'review_count', 'business_id']]\n",
        "  \n",
        "  return df_merged\n",
        "\n",
        "import ast\n",
        "def reorder_user_data(df):\n",
        "  df['review_count'] = np.NaN\n",
        "  df['friend_count'] = np.NaN\n",
        "  df['location'] = np.NaN\n",
        "  df['photo_count'] = np.NaN\n",
        "  df['useful_votes'] = np.NaN\n",
        "  df['funny_votes'] = np.NaN\n",
        "  df['cool_votes'] = np.NaN\n",
        "  df['text'] = \"\"\n",
        "  for i, user in df.iterrows():\n",
        "    print(i)\n",
        "    user_data = user['user']\n",
        "    user_data_converted = ast.literal_eval(user_data)\n",
        "    df.loc[i, 'review_count'] = user_data_converted[\"reviewCount\"]\n",
        "    df.loc[i, 'friend_count'] = user_data_converted[\"friendCount\"]\n",
        "    df.loc[i, 'location'] = user_data_converted['displayLocation']\n",
        "    df.loc[i, 'photo_count'] = user_data_converted['photoCount']\n",
        "    comment_data = user['comment']\n",
        "    comment_data_converted = ast.literal_eval(comment_data)\n",
        "    df.loc[i, 'text'] = comment_data_converted[\"text\"]\n",
        "    feedback_data = user['feedback']\n",
        "    feedback_data_converted = ast.literal_eval(feedback_data)\n",
        "    counts_data = feedback_data_converted['counts']\n",
        "    #counts_data_converted = ast.literal_eval(counts_data)\n",
        "    df.loc[i, 'useful_votes'] = counts_data['useful']\n",
        "    df.loc[i, 'funny_votes'] = counts_data['funny']\n",
        "    df.loc[i, 'cool_votes'] = counts_data['cool']\n",
        "  print('REORDER-----------------------------------------')\n",
        "  print(df)\n",
        "  df = drop_unnamed(df)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezxCXIZBkcAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADD USER DATA TO GROUP USERS\n",
        "def create_vectors_only_demographics(df_users, business_id, list_of_attributes, method):\n",
        "  vectors = df_users[['user_id']]\n",
        "\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "def create_vectors_yelp(df_users, business_id, list_of_attributes, method, destination):\n",
        "  vectors = df_users[['user_id', 'review_count', 'friend_count', 'location', 'photo_count']]\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  # SENTIMENT\n",
        "  #vectors = get_sentiment(vectors, business_id)\n",
        "\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "def create_vectors(df_users, reviews, business, business_id, list_of_attributes, method):\n",
        "  vectors = df_users[['user_id','name','location','text','fans','average_stars','review_count']]\n",
        "  vectors[\"useful\"] = np.NaN\n",
        "  vectors[\"funny\"] = np.NaN\n",
        "  vectors[\"cool\"] = np.NaN\n",
        "  vectors[\"loc1\"] = np.NaN\n",
        "  vectors[\"loc2\"] = np.NaN\n",
        "  vectors[\"loc3\"] = np.NaN\n",
        "  print(vectors.columns)\n",
        "  for index, user in vectors.iterrows():\n",
        "    print(index)\n",
        "    # TOP THREE LOCATION\n",
        "    location_list = get_top_three_loc(user, reviews, business)\n",
        "    i = 0\n",
        "    while i < len(location_list):\n",
        "      vectors.loc[index, 'loc'+str(i+1)] = location_list[i]\n",
        "      i = i + 1\n",
        "\n",
        "    # USEFUL FUNNY COOL\n",
        "    useful, funny, cool = get_details_user(user, reviews)\n",
        "    vectors.loc[index, 'useful'] = useful\n",
        "    vectors.loc[index, 'funny'] = funny\n",
        "    vectors.loc[index, 'cool'] = cool\n",
        "\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  #SENTIMENT\n",
        "  vectors = get_sentiment(vectors, business_id)\n",
        "\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "\n",
        "def get_sentiment(vectors, id):\n",
        "  df_sentiment = pd.read_csv('sentiment_' + id + '.csv')\n",
        "  new_vectors = pd.merge(vectors, df_sentiment, on='user_id', how='left')\n",
        "  new_vectors = drop_unnamed(new_vectors)\n",
        "  return new_vectors\n",
        "\n",
        "\n",
        "def get_demographics(vectors, id):\n",
        "  demographics_file_id = gtree.loc[gtree['business_id']==id, 'gfolder_clarifai'].tolist()[0]\n",
        "  download = drive.CreateFile({'id': demographics_file_id})\n",
        "  download.GetContentFile('demographics_'  + id + '.csv')\n",
        "  df_demographics = pd.read_csv('demographics_' + id + '.csv')\n",
        "  df_demographics = df_demographics.sort_values(['user_id', 'age'], ascending=[True, False])\n",
        "  df_demographics = df_demographics.drop_duplicates('user_id',keep='first').reset_index(drop=True)\n",
        "  new_vectors = pd.merge(vectors, df_demographics, on='user_id', how='left')\n",
        "  new_vectors = drop_unnamed(new_vectors)\n",
        "  return new_vectors\n",
        "\n",
        "\n",
        "def get_details_user(user, reviews):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  return user_reviews['useful'].sum(), user_reviews['funny'].sum(), user_reviews['cool'].sum()\n",
        "\n",
        "\n",
        "def get_top_three_loc(user, reviews, business):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  result = user_reviews.merge(business, on='business_id')\n",
        "  top_loc = result['state'].value_counts().index.tolist()[:3]\n",
        "  top_zip = []\n",
        "  for location in top_loc:\n",
        "    # exclude postal_code of Canada, that is strings\n",
        "    temp_list = result[result['state']==location]['postal_code'].value_counts().index.tolist()\n",
        "    if temp_list != []:\n",
        "      temp_list = [elem for elem in temp_list if str(elem).isdigit()]\n",
        "      if temp_list != []:\n",
        "        top_zip.append(int(str(temp_list[0])[:3]))\n",
        "  return top_zip\n",
        "\n",
        "\n",
        "\n",
        "def generate_dummies(df, text_attribute_list):\n",
        "  dummy_columns = []\n",
        "  for attr in text_attribute_list:\n",
        "    gender_dummies = pd.get_dummies(df[attr])\n",
        "    dummy_columns = dummy_columns + list(gender_dummies.columns)\n",
        "    df = pd.merge(df, gender_dummies, how=\"left\",left_index=True, right_index=True)\n",
        "    \n",
        "    # drop all the unnamed columns\n",
        "    df = drop_unnamed(df)\n",
        "  return df, dummy_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyIm6NR55FGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "drive = authenticate()\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW2AKZ8lHgoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PIPELINE\n",
        "\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "#          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A', 'S-oLPRdhlyL5HAknBKTUcQ']\n",
        "#id_list = ['S-oLPRdhlyL5HAknBKTUcQ']\n",
        "download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "download.GetContentFile('gtree.csv')\n",
        "gtree = pd.read_csv('gtree.csv')\n",
        "#list_of_attributes = ['review_count'] # alluser\n",
        "#list_of_attributes = ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool'] # dataset\n",
        "#list_of_attributes = ['loc1', 'loc2', 'loc3'] # dataset\n",
        "#list_of_attributes = ['age', 'gender', 'ethnicity'] # alluser\n",
        "#list_of_attributes = ['fans'] # dataset\n",
        "#list_of_attributes = ['useful', 'funny', 'cool'] # dataset\n",
        "#list_of_attributes = ['gender', 'ethnicity'] # alluser\n",
        "#list_of_attributes = ['ethnicity'] # alluser\n",
        "#list_of_attributes = ['review_count', 'friend_count', 'photo_count'] # alluser\n",
        "#list_of_attributes = ['review_sentiment'] # alluser\n",
        "list_of_attributes = ['gender'] # alluser\n",
        "method = 'custom'\n",
        "folder_name = '#14_custom_gender'\n",
        "gtree_folder_destination_name = 'gfolder_#14'\n",
        "N_of_groups = 3\n",
        "alluser = True\n",
        "#alluser = False\n",
        "\n",
        "index_restaurant = 42\n",
        "for id in restaurants_id_list[index_restaurant-1:index_restaurant+1]:\n",
        "\n",
        "  authenticate()\n",
        "  df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1(id, users, reviews, alluser)\n",
        "  #df_rel_ranking = pd.read_csv(\"dataset_rel_ranking_\" + id + \".csv\")\n",
        "  #df_date_ranking = pd.read_csv(\"dataset_date_ranking_\" + id + \".csv\")\n",
        "  #df_rand_ranking = pd.read_csv(\"dataset_rand_ranking_\" + id + \".csv\")\n",
        "  authenticate()\n",
        "\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "  \n",
        "  destination = create_folder_in_drive(drive, folder_name, gtree.loc[gtree['business_id']==id, 'gfolder_groups'].tolist()[0])\n",
        "  destination_copy = destination\n",
        "  gtree.loc[gtree['business_id']==id, gtree_folder_destination_name] = destination\n",
        "  gtree = drop_unnamed(gtree)\n",
        "  gtree.to_csv('gtree.csv')\n",
        "  upload_file('gtree.csv', '1eUt2wyCOULW0-LdL6vyRJi1mUs_U46DD') # id folder data\n",
        "\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                   N_of_groups, list_of_attributes, method,\n",
        "                                   alluser, destination, '')\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  authenticate()\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes, alluser, destination)\n",
        "  \n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "  # ASSUMPTIONS\n",
        "  # ALL MEN\n",
        "  print('''#####################################################################\n",
        "            ################## ASSUMPTION ALL MEN ###############################\n",
        "            #####################################################################''')\n",
        "\n",
        "  destination = destination_copy\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                   N_of_groups, list_of_attributes, method,\n",
        "                                   alluser, destination, 'all_men')\n",
        "  \n",
        "  print(percents)\n",
        "  authenticate()\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes, alluser, destination)\n",
        "  \n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  # ALL WOMEN\n",
        "  print('''#####################################################################\n",
        "            ################## ASSUMPTION ALL WOMEN #############################\n",
        "            #####################################################################''')\n",
        "  destination = destination_copy\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                   N_of_groups, list_of_attributes, method,\n",
        "                                   alluser, destination, 'all_women')\n",
        "  print(group_list)\n",
        "  print(percents)\n",
        "  authenticate()\n",
        "\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes, alluser, destination)\n",
        "  \n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  # 50%\n",
        "  print('''#####################################################################\n",
        "            ################## ASSUMPTION 50 AND 50 #############################\n",
        "            #####################################################################''')\n",
        "  destination = destination_copy\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                   N_of_groups, list_of_attributes, method,\n",
        "                                   alluser, destination, '50')\n",
        "  print(group_list)\n",
        "  print(percents)\n",
        "  authenticate()\n",
        "\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes, alluser, destination)\n",
        "\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  # EQUALLY DISTRIBUTED\n",
        "  print('''#####################################################################\n",
        "            ################## ASSUMPTION EQUALLY DISTRIBUTED ###################\n",
        "            #####################################################################''')\n",
        "  destination = destination_copy\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                   N_of_groups, list_of_attributes, method,\n",
        "                                   alluser, destination, 'equal')\n",
        "  print(group_list)\n",
        "  print(percents)\n",
        "  authenticate()\n",
        "\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes, alluser, destination)\n",
        "  \n",
        "\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  # MAINTAIN PROPORTION\n",
        "  print('''#####################################################################\n",
        "            ################## ASSUMPTION MAINTAIN PROPORTION ###################\n",
        "            #####################################################################''')\n",
        "  destination = destination_copy\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                   N_of_groups, list_of_attributes, method,\n",
        "                                   alluser, destination, 'proportioned')\n",
        "  print(group_list)\n",
        "  print(percents)\n",
        "  authenticate()\n",
        "\n",
        "  download = drive.CreateFile({'id': '1MIWS9uI9GKxsgAlzB0wXFBiutE025u2c'}) # id file gtree.csv\n",
        "  download.GetContentFile('gtree.csv')\n",
        "  gtree = pd.read_csv('gtree.csv')\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes, alluser, destination)\n",
        "\n",
        "print('FIN')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX8ibEsAIxCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline1(business_id, users, reviews, alluser):\n",
        "  ### READ THE RANKING FROM GTREE\n",
        "  local_ranking_folder = gtree.loc[gtree['business_id']==id, 'gfolder_rankings'].tolist()[0]\n",
        "  print(local_ranking_folder)\n",
        "  local_ranking_files = drive.ListFile({'q': \"'\" + local_ranking_folder + \"' in parents and trashed=false\"}).GetList()\n",
        "  download = drive.CreateFile({'id': local_ranking_files[0]['id']})\n",
        "  download.GetContentFile(local_ranking_files[0]['title'])\n",
        "  print(local_ranking_files[0]['id'])\n",
        "  df_date_ranking = pd.read_csv(local_ranking_files[0]['title'])\n",
        "\n",
        "  download = drive.CreateFile({'id': local_ranking_files[1]['id']})\n",
        "  download.GetContentFile(local_ranking_files[1]['title'])\n",
        "  print(local_ranking_files[1]['id'])\n",
        "  df_rand_ranking = pd.read_csv(local_ranking_files[1]['title'])\n",
        "\n",
        "  download = drive.CreateFile({'id': local_ranking_files[2]['id']})\n",
        "  download.GetContentFile(local_ranking_files[2]['title'])\n",
        "  print(local_ranking_files[2]['id'])\n",
        "  df_rel_ranking = pd.read_csv(local_ranking_files[2]['title'])\n",
        "  \n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = reorder_user_data(df_rel_ranking)\n",
        "  df_date_ranking = reorder_user_data(df_date_ranking)\n",
        "  df_rand_ranking = reorder_user_data(df_rand_ranking)\n",
        "\n",
        "  if not alluser:\n",
        "\n",
        "    df_rel_ranking = filter_user_from_dataset(df_rel_ranking, users)\n",
        "    df_date_ranking = filter_user_from_dataset(df_date_ranking, users)\n",
        "    df_rand_ranking = filter_user_from_dataset(df_rand_ranking, users)\n",
        "\n",
        "    df_rel_ranking = integrate_review_info(df_rel_ranking, reviews, business_id)\n",
        "    df_date_ranking = integrate_review_info(df_date_ranking, reviews, business_id)\n",
        "    df_rand_ranking = integrate_review_info(df_rand_ranking, reviews, business_id)\n",
        "\n",
        "    df_rel_ranking.to_csv('dataset_rel_ranking_' + business_id + '.csv')\n",
        "    df_date_ranking.to_csv('dataset_date_ranking_' + business_id + '.csv')\n",
        "    df_rand_ranking.to_csv('dataset_rand_ranking_' + business_id + '.csv')\n",
        "\n",
        "    upload_file('dataset_rel_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "    upload_file('dataset_date_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "    upload_file('dataset_rand_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "    \n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline2(df_ranking_by_relevance, reviews, business, id, N_of_groups,\n",
        "              local_list_of_attributes, method, alluser, destination, assumption):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "\n",
        "  if alluser:\n",
        "    df_vectors = create_vectors_yelp(df_ranking_by_relevance, id, local_list_of_attributes,\n",
        "                                     method, destination)\n",
        "  #else:\n",
        "    #df_vectors = create_vectors(df_ranking_by_relevance, reviews, business, id, local_list_of_attributes, method)\n",
        "    #df_vectors = create_vectors_only_demographics(df_ranking_by_relevance, id, local_list_of_attributes, method)\n",
        "\n",
        "  # ------ SINGLE ATTRIBUTE ---------\n",
        "  if len(local_list_of_attributes) == 1 and local_list_of_attributes[0]!='review_sentiment' and local_list_of_attributes[0]!='ethnicity' and local_list_of_attributes[0]!='gender':\n",
        "    attribute = local_list_of_attributes[0]\n",
        "    df_groups = create_groups_by_percentile(df_ranking_by_relevance, attribute, N_of_groups, id)\n",
        "    method = attribute\n",
        "  \n",
        "  # ------ MULTIPLE ATTRIBUTES ---------\n",
        "  # TO BUILD VECTORS, it saves a csv file\n",
        "  elif method == 'kmeans':\n",
        "    df_groups = create_groups_by_kmeans_clustering(df_ranking_by_relevance,N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'spectral':\n",
        "    df_groups = create_groups_by_spectral_clustering(df_ranking_by_relevance, N_of_groups, id)\n",
        "  elif method == 'dbscan':\n",
        "    df_groups = create_groups_by_dbscan_clustering(df_ranking_by_relevance,N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'balanced_kmeans':\n",
        "    df_groups = create_groups_by_balanced_kmeans_clustering(N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'custom':\n",
        "    df_groups, destination = create_groups_custom(N_of_groups,local_list_of_attributes,id, destination, assumption)\n",
        "  \n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "  \n",
        "  print(df_groups)\n",
        "  print(percents)\n",
        "  return df_groups, percents, destination\n",
        "\n",
        "\n",
        "def pipeline3(business_id, method, df_ranking_by_relevance, df_ranking_by_date, \n",
        "              df_ranking_by_random, reviews, percents, list_of_attributes,\n",
        "              alluser, destination):\n",
        "  \n",
        "  df_result = pd.DataFrame(columns=['Exposure_method', 'Context', 'Means', 'P-value'])\n",
        "  df_result.to_csv('result.csv')\n",
        "  id_new_file = upload_file('result.csv', destination)\n",
        "\n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  print(\"----------------DEMOGRAPHIC PARITY EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                    method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes, destination)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                    method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes, destination)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                      method, df_ranking_by_random,\n",
        "                                                       \"rand_\", list_of_attributes, destination)\n",
        "  \n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"demgr_yelp_\", business_id, np.arange(0,N_of_groups), destination) # group_id\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"demgr_date_\", business_id, np.arange(0,N_of_groups), destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                    'scatter_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "\n",
        "  print(\"----------------DISPARATE IMPACT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                   reviews, \"yelp_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                   reviews, \"date_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                     reviews, \"rand_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"dispimp_yelp_\", business_id, np.arange(0,N_of_groups), destination)\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"dispimp_date_\", business_id, np.arange(0,N_of_groups), destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                    'scatter_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDtdqvj_BUBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_custom(N_of_groups, list_of_attributes, id, destination, assumption):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "\n",
        "  local_list_of_attributes = list_of_attributes\n",
        "\n",
        "  if set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity', 'review_sentiment'])):\n",
        "    text_attribute_list = ['gender', 'ethnicity']\n",
        "    df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "    # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "    # subtraction\n",
        "    temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "    list_of_attributes = temp + dummy_columns_name\n",
        "    #serve per la descrizione dei gruppi, da fare\n",
        "\n",
        "    #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "    df_no_info = df_vectors[df_vectors[local_list_of_attributes[0]].isnull()]\n",
        "    df_yes_info = df_vectors[df_vectors[local_list_of_attributes[0]].notnull()]\n",
        "    print('All users =', len(df_vectors))\n",
        "    print('Users with info =',len(df_yes_info.index))\n",
        "    print('User without info =',len(df_no_info.index))\n",
        "  \n",
        "  else:\n",
        "    df_no_info = pd.DataFrame()\n",
        "    df_yes_info = df_vectors\n",
        "\n",
        "  \n",
        "  # GRUPPI PER GENERE\n",
        "  if local_list_of_attributes == ['gender']:\n",
        "    if assumption=='':\n",
        "      femmine = df_yes_info[df_yes_info['gender']=='feminine']\n",
        "      maschi = df_yes_info[df_yes_info['gender']=='masculine']\n",
        "      new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "      new_df_users['group_id'] = np.NaN\n",
        "      for j, user in femmine.iterrows():\n",
        "        new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "        \n",
        "      for j, user in maschi.iterrows():\n",
        "        new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    else:\n",
        "      new_df_users, destination = do_assumptions(df_yes_info, df_no_info,\n",
        "                                                 destination, assumption)\n",
        "      df_no_info = df_no_info.iloc[0:0]\n",
        "\n",
        "  # GRUPPI PER REVIEW SENTIMENT\n",
        "  if local_list_of_attributes == ['review_sentiment']:\n",
        "    df_yes_info = df_yes_info[['user_id', 'review_sentiment']]\n",
        "    print(df_yes_info)\n",
        "    positivi = df_yes_info[df_yes_info['review_sentiment'].str.startswith('positive')]\n",
        "    negativi = df_yes_info[df_yes_info['review_sentiment'].str.startswith('negative')]\n",
        "    neutrali = df_yes_info[df_yes_info['review_sentiment'].str.startswith('neutral')]\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in positivi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in negativi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in neutrali.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "    \n",
        "  # GRUPPI PER ETNIA\n",
        "  if local_list_of_attributes == ['ethnicity']:\n",
        "    bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "    neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "    altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in bianchi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in neri.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in altri.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "\n",
        "  # GRUPPI PER ETNIA, GENERE\n",
        "  if local_list_of_attributes == ['gender','ethnicity']:\n",
        "    bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "    b_under_39 = bianchi[(bianchi['age']<39)]\n",
        "    b_over_39 = bianchi[(bianchi['age']>=39)]\n",
        "    b_fem = bianchi[bianchi['gender']=='feminine'].reset_index(drop=True)\n",
        "    b_mas = bianchi[bianchi['gender']=='masculine'].reset_index(drop=True)\n",
        "    \n",
        "    neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "    n_under_39 = neri[(neri['age']<39)]\n",
        "    n_over_39 = neri[(neri['age']>=39)]\n",
        "    n_fem = neri[neri['gender']=='feminine'].reset_index(drop=True)\n",
        "    n_mas = neri[neri['gender']=='masculine'].reset_index(drop=True)\n",
        "    \n",
        "    altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "    a_under_39 = altri[(altri['age']<39)]\n",
        "    a_over_39 = altri[(altri['age']>=39)]\n",
        "    a_fem = altri[altri['gender']=='feminine'].reset_index(drop=True)\n",
        "    a_mas = altri[altri['gender']=='masculine'].reset_index(drop=True)\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in b_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in b_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in n_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "      \n",
        "    for j, user in n_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 4\n",
        "      \n",
        "    for j, user in a_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 5\n",
        "      \n",
        "    for j, user in a_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 6\n",
        "\n",
        "  if not df_no_info.empty:\n",
        "    # ADD THE EXCLUDED IN LAST C\n",
        "    new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "    new_df_users = drop_unnamed(new_df_users)\n",
        "    new_df_users = new_df_users.fillna(0)\n",
        "\n",
        "  #destination = set_file_destination(local_list_of_attributes, 'custom', id)\n",
        "\n",
        "  new_df_users.to_csv('groups_custom_' + id + '.csv')\n",
        "  upload_file('groups_custom_' + id + '.csv',destination)\n",
        "  return new_df_users, destination\n",
        "\n",
        "\n",
        "def do_assumptions(df_yes, df_no_info, destination, assumption):\n",
        "  total = len(df_yes.index)+len(df_no_info.index)\n",
        "  already_men = len(df_yes[df_yes['gender']=='masculine'].values.tolist())\n",
        "  # ALL IN ONE GROUP\n",
        "  if assumption=='all_men':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'All_men', destination)\n",
        "    df_no = df_no_info\n",
        "    df_no['gender'].fillna('masculine', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  if assumption=='all_women':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'All_women', destination)\n",
        "    df_no = df_no_info\n",
        "    df_no['gender'] = 'feminine'\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # 50%\n",
        "  if assumption=='50':\n",
        "    men_size = total//2\n",
        "    id_new_folder = create_folder_in_drive(drive, '50and50', destination)\n",
        "    df_no = df_no_info\n",
        "    while (already_men+len(df_no[df_no['gender']=='masculine'].values.tolist()))<=men_size:\n",
        "      index_found = df_no.index[df_no['gender'].isnull()].tolist()[0]\n",
        "      df_no.loc[index_found, 'gender'] = 'masculine'\n",
        "    df_no['gender'].fillna('feminine', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # EQUALLY DISTRIBUTED\n",
        "  if assumption=='equal':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'Equally_distributed', destination)\n",
        "    df_no = df_no_info\n",
        "    even = 0\n",
        "    for ind, row in df_no.iterrows():\n",
        "      if even==0:\n",
        "        even=1\n",
        "        df_no.loc[ind,'gender']='masculine'\n",
        "      else:\n",
        "        df_no.loc[ind,'gender']='feminine'\n",
        "        even=0\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # MAINTAIN PROPORTION\n",
        "  if assumption=='proportioned':\n",
        "    #male_size:partial=new_male_size:total\n",
        "    partial = len(df_yes.index)\n",
        "    men_size = (total*(len(df_yes[df_yes['gender']=='masculine'].reset_index(drop=True).index)))//partial\n",
        "    id_new_folder = create_folder_in_drive(drive, 'Maintaining_proportion', destination)\n",
        "    df_no = df_no_info\n",
        "    while (already_men+len(df_no[df_no['gender']=='masculine'].values.tolist()))<=men_size:\n",
        "      index_found = df_no.index[df_no['gender'].isnull()].tolist()[0]\n",
        "      df_no.loc[index_found, 'gender'] = 'masculine'\n",
        "    df_no['gender'].fillna('feminine', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  femmine = df[df['gender']=='feminine']\n",
        "  maschi = df[df['gender']=='masculine']\n",
        "\n",
        "  new_df_users = df[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in femmine.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 0\n",
        "  for j, user in maschi.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "  return new_df_users, id_new_folder\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_c2w7U3DNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_balanced_kmeans_clustering(N_of_groups, list_of_attributes, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "  df_no_info = df_vectors[df_vectors['age'].isnull()]['user_id']\n",
        "  df_yes_info = df_vectors[df_vectors['age'].notnull()]\n",
        "  number_of_users = len(df_yes_info.index)\n",
        "  five_percent = (number_of_users*5)//100\n",
        "  one_cluster_size = (number_of_users//(N_of_groups-1))-five_percent\n",
        "  clusters_size = [one_cluster_size]*(N_of_groups-1)\n",
        "\n",
        "  list_vectors = df_yes_info[list_of_attributes].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0 \n",
        "\n",
        "  # #############################################################################\n",
        "  # COMPUTE BALANCED KMEANS WITH MIN_FLOW\n",
        "  # see https://adared.ch/constrained-k-means-implementation-in-python/\n",
        "  (centroids, labels, f) = constrained_kmeans(X, clusters_size)\n",
        "  print('Centroids: ', centroids)\n",
        "  print('labels: ', labels)\n",
        "\n",
        "  # Save centroids\n",
        "  temp_df = pd.DataFrame(data=centroids)\n",
        "  temp_df.columns = list_of_attributes\n",
        "  destination = set_file_destination(local_list_of_attributes, 'balanced_kmeans', id)\n",
        "  temp_df.to_csv(\"centroids_balanced_kmeans_\" + id + \".csv\", float_format=\"%.3f\")\n",
        "  upload_file(\"centroids_balanced_kmeans_\" + id + \".csv\",destination)\n",
        "\n",
        "  new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[j, 'group_id'] = labels[j]\n",
        "  \n",
        "  # ADD THE EXCLUDED IN LAST C\n",
        "  new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "  new_df_users = new_df_users.fillna(N_of_groups-1)\n",
        "\n",
        "  new_df_users.to_csv('groups_balanced_kmeans_' + id + '.csv')\n",
        "  upload_file('groups_balanced_kmeans_' + id + '.csv',destination)\n",
        "  return new_df_users\n",
        "  # #############################################################################\n",
        "  # BALANCED KMEANS WITH PYTORCH DOESN'T WORK\n",
        "  # see https://github.com/giannisdaras/balanced_kmeans/tree/master\n",
        "  #N = len(df_users.index)\n",
        "  #cluster_size = N // N_of_groups\n",
        "  #choices, centers = kmeans_equal(X, num_clusters=N_of_groups, cluster_size=cluster_size)\n",
        "  #print(centers)\n",
        "  #print(choices)\n",
        "\n",
        "\n",
        "def constrained_kmeans(data, demand, maxiter=None, fixedprec=1e9):\n",
        "\tdata = np.array(data)\n",
        "\t\n",
        "\tmin_ = np.min(data, axis = 0)\n",
        "\tmax_ = np.max(data, axis = 0)\n",
        "\t\n",
        "\tC = min_ + np.random.random((len(demand), data.shape[1])) * (max_ - min_)\n",
        "\tM = np.array([-1] * len(data), dtype=np.int)\n",
        "\t\n",
        "\titercnt = 0\n",
        "\twhile True:\n",
        "\t\titercnt += 1\n",
        "\t\tprint(itercnt)\n",
        "\t\t# memberships\n",
        "\t\tg = nx.DiGraph()\n",
        "\t\tg.add_nodes_from(range(0, data.shape[0]), demand=-1) # points\n",
        "\t\tfor i in range(0, len(C)):\n",
        "\t\t\tg.add_node(len(data) + i, demand=demand[i])\n",
        "\t\t\n",
        "\t\t# Calculating cost...\n",
        "\t\tcost = np.array([np.linalg.norm(np.tile(data.T,\n",
        "                                          len(C)).T - np.tile(C, len(data)).reshape(len(C) * len(data),\n",
        "                                                                                            C.shape[1]), axis=1)])\n",
        "\t\t# Preparing data_to_C_edges...\n",
        "\t\tdata_to_C_edges = np.concatenate((np.tile([range(0, data.shape[0])], len(C)).T,\n",
        "                                    np.tile(np.array([range(data.shape[0], data.shape[0] + C.shape[0])]).T,\n",
        "                                            len(data)).reshape(len(C) * len(data), 1), cost.T * fixedprec),\n",
        "                                   axis=1).astype(np.uint64)\n",
        "\t\t# Adding to graph\n",
        "\t\tg.add_weighted_edges_from(data_to_C_edges)\n",
        "\t\t\n",
        "\n",
        "\t\ta = len(data) + len(C)\n",
        "\t\tg.add_node(a, demand=len(data)-np.sum(demand))\n",
        "\t\tC_to_a_edges = np.concatenate((np.array([range(len(data), len(data) + len(C))]).T, np.tile([[a]], len(C)).T), axis=1)\n",
        "\t\tg.add_edges_from(C_to_a_edges)\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t# Calculating min cost flow...\n",
        "\t\tf = nx.min_cost_flow(g)\n",
        "\t\t\n",
        "\t\t# assign\n",
        "\t\tM_new = np.ones(len(data), dtype=np.int) * -1\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tp = sorted(f[i].items(), key=lambda x: x[1])[-1][0]\n",
        "\t\t\tM_new[i] = p - len(data)\n",
        "\t\t\t\n",
        "\t\t# stop condition\n",
        "\t\tif np.all(M_new == M):\n",
        "\t\t\t# Stop\n",
        "\t\t\treturn (C, M, f)\n",
        "\t\t\t\n",
        "\t\tM = M_new\n",
        "\t\t\t\n",
        "\t\t# compute new centers\n",
        "\t\tfor i in range(len(C)):\n",
        "\t\t\tC[i, :] = np.mean(data[M==i, :], axis=0)\n",
        "\t\t\t\n",
        "\t\tif maxiter is not None and itercnt >= maxiter:\n",
        "\t\t\t# Max iterations reached\n",
        "\t\t\treturn (C, M, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQKajNw47W7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_dbscan_clustering(df_users, N_of_groups, list_of_attributes, id):\n",
        "  # #############################################################################\n",
        "  # Generate data\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  list_vectors = df_vectors[list_of_attributes].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0\n",
        "\n",
        "  # #############################################################################\n",
        "  # Compute DBSCAN\n",
        "  i = 0\n",
        "  while i<21:\n",
        "    db = DBSCAN(eps=2, min_samples=i).fit(X)\n",
        "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "    core_samples_mask[db.core_sample_indices_] = True\n",
        "    labels = db.labels_\n",
        "\n",
        "    # Number of clusters in labels, ignoring noise if present.\n",
        "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise_ = list(labels).count(-1)\n",
        "\n",
        "    print('Estimated number of clusters: %d' % n_clusters_)\n",
        "    print('Estimated number of noise points: %d' % n_noise_)\n",
        "    i = i+1\n",
        "  #print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
        "  #print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
        "  #print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
        "  #print(\"Adjusted Rand Index: %0.3f\"\n",
        "  #      % metrics.adjusted_rand_score(labels_true, labels))\n",
        "  #print(\"Adjusted Mutual Information: %0.3f\"\n",
        "  #      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
        "  print(\"Silhouette Coefficient: %0.3f\"\n",
        "        % metrics.silhouette_score(X, labels))\n",
        "  \n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[j, 'group_id'] = labels[j]\n",
        "  new_df_users.to_csv('groups_dbscan_' + id + '.csv')\n",
        "  destination = set_file_destination(local_list_of_attributes, 'dbscan', id)\n",
        "  upload_file('groups_dbscan_' + id + '.csv',destination)\n",
        "  \n",
        "  #SEARCH FOR CLUSTER DESCRIPTION AND SAME SIZE CLUSTERING\n",
        "\n",
        "  # #############################################################################\n",
        "  # Plot result\n",
        "\n",
        "  # Black removed and is used for noise instead.\n",
        "  unique_labels = set(labels)\n",
        "  colors = [plt.cm.Spectral(each)\n",
        "            for each in np.linspace(0, 1, len(unique_labels))]\n",
        "  for k, col in zip(unique_labels, colors):\n",
        "      if k == -1:\n",
        "          # Black used for noise.\n",
        "          col = [0, 0, 0, 1]\n",
        "\n",
        "      class_member_mask = (labels == k)\n",
        "\n",
        "      xy = X[class_member_mask & core_samples_mask]\n",
        "      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "              markeredgecolor='k', markersize=14)\n",
        "\n",
        "      xy = X[class_member_mask & ~core_samples_mask]\n",
        "      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "              markeredgecolor='k', markersize=6)\n",
        "\n",
        "  plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "  plt.show()\n",
        "  ####################################################################\n",
        "  return new_df_users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRs30-Z9heM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_kmeans_clustering(df_users, N_of_groups, list_of_attributes, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  #if(list_of_attributes == ['age', 'gender', 'ethnicity']):\n",
        "    #age, feminine, masculine, asian, black, hispanic, arabs, hawaiian, white\n",
        "    #init_centroids = np.array([\n",
        "    #                 [20, 1, -10, -10, -10, -10, -10, -10, 1],\n",
        "    #                 [60, -10, 1, -10, -10, -10, -10, -10, 1],\n",
        "    #                 [39, 0, 0, -10, -10, 1, 1, 1, -30],\n",
        "    #                 [60, 0, 0, 1, -10, -10, -10, -10, -30],\n",
        "    #                 [20, 0, 0, -10, 1, -10, -10, -10, -30],\n",
        "    #                 [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
        "    #                np.float64)\n",
        "\n",
        "  local_list_of_attributes = list_of_attributes\n",
        "\n",
        "  if set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    text_attribute_list = ['gender', 'ethnicity']\n",
        "    df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "    # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "    # subtraction\n",
        "    temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "    list_of_attributes = temp\n",
        "    list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  list_vectors = df_vectors[list_of_attributes].values.tolist()\n",
        "  # ex. ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']\n",
        "  # ex. ['loc1', 'loc2', 'loc3']\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0\n",
        "  kmeans = KMeans(n_clusters=N_of_groups, random_state=0).fit(X) #init=init_centroids\n",
        "  labels = kmeans.labels_\n",
        "  # kmeans.predict([[0, 0, 20], [12, 3, 5]]))\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  temp_df = pd.DataFrame(data=centroids)\n",
        "  temp_df.columns = list_of_attributes\n",
        "  destination = set_file_destination(local_list_of_attributes, 'kmeans', id)\n",
        "  temp_df.to_csv(\"centroids_kmeans_\" + id + \".csv\", float_format=\"%.3f\")\n",
        "  upload_file(\"centroids_kmeans_\" + id + \".csv\",destination)\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = labels[i]\n",
        "  new_df_users.to_csv('groups_kmeans_' + id + '.csv')\n",
        "  upload_file('groups_kmeans_' + id + '.csv',destination)\n",
        "  return new_df_users\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU_zGBsWzAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_spectral_clustering(df_users, N_of_groups, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['top_location']\n",
        "  df_vectors = one_hot_encoding(df_vectors, text_attribute_list)\n",
        "  \n",
        "  list_vectors = df_vectors[['review_count', 'fans', 'average_stars', 'top_location',\n",
        "                             'useful', 'funny', 'cool']].values.tolist()\n",
        "  # Convert list of list in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  mat = cosine_similarity(list_of_arrays)\n",
        "  group_array = SpectralClustering(n_clusters=N_of_groups, affinity='precomputed').fit_predict(mat)\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = group_array[i]\n",
        "  new_df_users.to_csv('groups_spectral_' + id + '.csv')\n",
        "  return new_df_users\n",
        "\n",
        "def one_hot_encoding(df, text_attribute_list):\n",
        "  for attr in text_attribute_list:\n",
        "    text = list(df[attr])\n",
        "    vectorizer = CountVectorizer().fit_transform(text)\n",
        "    embeddings = vectorizer.toarray()\n",
        "    for i, user in df.iterrows():\n",
        "      df.at[i, attr] = embeddings[i]\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awAE4AWWxEFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_percentile(df_users, attribute, N_of_groups, business_id):\n",
        "  values = df_users[attribute].tolist()\n",
        "\n",
        "  print(df_users)\n",
        "  # Create array of percents to define limit of group \n",
        "  scale = 100/N_of_groups\n",
        "  percents = []\n",
        "  current_percent = 0\n",
        "  while current_percent < 100:\n",
        "    current_percent = current_percent + scale\n",
        "    '''if current_percent > 100:\n",
        "      current_percent = 100\n",
        "      percents.append(current_percent)\n",
        "      break'''\n",
        "    percents.append(current_percent)\n",
        "  # ex. percents = [20, 40, 60, 80, 100]\n",
        "  print(percents)\n",
        "\n",
        "  # Create array with limit values (max) of each group\n",
        "  i = 0\n",
        "  limit_values = []\n",
        "  while i < len(percents):\n",
        "    limit_values.append(np.percentile(values, percents[i]))\n",
        "    i = i + 1\n",
        "  # ex. limit_values = [5.0, 13.0, 27.0, 73.0, 10022.0]\n",
        "  print(limit_values)\n",
        "  with open('descr_groups_' + attribute + '_' + business_id + '.txt', 'w') as output:\n",
        "    output.write(str(percents) + '\\n' + str(limit_values))\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  \n",
        "  for i, user in df_users.iterrows():\n",
        "    id_group = 0\n",
        "\n",
        "    if (user[attribute] >= 0) and (user[attribute] <= limit_values[id_group]):\n",
        "      new_df_users.loc[i, 'group_id'] = id_group\n",
        "    id_group = id_group + 1\n",
        "    while id_group < len(percents):\n",
        "      if (user[attribute] > limit_values[id_group - 1]) and (user[attribute] <= limit_values[id_group]):\n",
        "        new_df_users.loc[i, 'group_id'] = id_group\n",
        "      id_group = id_group + 1\n",
        "  \n",
        "  destination = set_file_destination([attribute], '', business_id)\n",
        "  new_df_users.to_csv('groups_' + attribute + '_' + business_id + '.csv')\n",
        "  upload_file('groups_' + attribute + '_' + business_id + '.csv',destination)\n",
        "  upload_file('descr_groups_' + attribute + '_' + business_id + '.txt',destination)\n",
        "  return new_df_users\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbpLTiPSWuL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_disparate_impact_exposure(business_id, method,  df_ranking, reviews,\n",
        "                                    filename, list_of_attributes, alluser, destination):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = disparate_impact_exposure(df_groups[df_groups['group_id'] == i], df_ranking, reviews,user_exposures, business_id, i, alluser)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  filePath = stat_significance_inter_groups(df_groups, 'dispimp_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "  '''array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  couples = list(itertools.combinations(array_group_id,2))\n",
        "  couples = [(x,y) for (x,y) in couples if x!=y]\n",
        "  print(couples) # [(0,1), (0,2), (1,2)]\n",
        "  filePath = 'stat_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  for id1,id2 in couples:\n",
        "    group1 = user_exposures[user_exposures['group_id'] == id1]['exposure']\n",
        "    group2 = user_exposures[user_exposures['group_id'] == id2]['exposure']\n",
        "    result = stats.ttest_ind(group1, group2)\n",
        "    percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "    percent2 = 100*(len(group2.index)/len(user_exposures.index))\n",
        "    print('+++++++++TEST STATISTICAL SIGNIFICANT+++++++++')\n",
        "    print('GROUP '+ str(id1) +':')\n",
        "    print(user_exposures[user_exposures['group_id'] == id1])\n",
        "    print('GROUP '+ str(id2) +':')\n",
        "    print(user_exposures[user_exposures['group_id'] == id2])\n",
        "    print('RESULT:')\n",
        "    print(result)\n",
        "    with open(filePath, 'a') as output:\n",
        "      output.write('Percent group '+ str(id1) +': ' + str(percent1) + '\\nPercent group '+\n",
        "                   str(id2) +': ' + str(percent2) + '\\nResult: ')\n",
        "      output.write(str(result)+'\\n\\n')'''\n",
        "\n",
        "  exposures.to_csv('exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "\n",
        "\n",
        "def print_demographic_parity_exposure(business_id, method, df_ranking, filename,\n",
        "                                      list_of_attributes, destination):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = demographic_parity_exposure(df_groups[df_groups['group_id'] == i], df_ranking, user_exposures, i)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  filePath = stat_significance_inter_groups(df_groups, 'demgr_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "  '''array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  couples = list(itertools.combinations(array_group_id,2))\n",
        "  couples = [(x,y) for (x,y) in couples if x!=y]\n",
        "  print(couples) # [(0,1), (0,2), (1,2)]\n",
        "  filePath = 'stat_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  for id1,id2 in couples:\n",
        "    group1 = user_exposures[user_exposures['group_id'] == id1]['exposure']\n",
        "    group2 = user_exposures[user_exposures['group_id'] == id2]['exposure']\n",
        "    result = stats.ttest_ind(group1, group2)\n",
        "    percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "    percent2 = 100*(len(group2.index)/len(user_exposures.index))\n",
        "    print('+++++++++TEST STATISTICAL SIGNIFICANT+++++++++')\n",
        "    print('GROUP '+ str(id1) +':')\n",
        "    print(user_exposures[user_exposures['group_id'] == id1])\n",
        "    print('GROUP '+ str(id2) +':')\n",
        "    print(user_exposures[user_exposures['group_id'] == id2])\n",
        "    print('RESULT:')\n",
        "    print(result)\n",
        "    with open(filePath, 'a') as output:\n",
        "      output.write('Percent group '+ str(id1) +': ' + str(percent1) + '\\nPercent group '+\n",
        "                   str(id2) +': ' + str(percent2) + '\\nResult: ')\n",
        "      output.write(str(result)+'\\n\\n')'''\n",
        "\n",
        "  exposures.to_csv('exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "\n",
        "\n",
        "def stat_significance_inter_groups(df_groups, filename, business_id, method, df_user_exposures, destination):\n",
        "  array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  couples = list(itertools.combinations(array_group_id,2))\n",
        "  couples = [(x,y) for (x,y) in couples if x!=y]\n",
        "  print(couples) # [(0,1), (0,2), (1,2)]\n",
        "  filePath = 'stat_groups_' + method + '_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  ciclo = 0\n",
        "  while ciclo < 3:\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('Without first and last ' + str(ciclo) + ' rows:\\n\\n')\n",
        "    user_exposures = df_user_exposures.iloc[ciclo:(len(df_user_exposures.index)-ciclo)]\n",
        "    print('Size:', len(user_exposures.index))\n",
        "    for id1,id2 in couples:\n",
        "      group1 = user_exposures[user_exposures['group_id'] == id1]['exposure']\n",
        "      group2 = user_exposures[user_exposures['group_id'] == id2]['exposure']\n",
        "      result = stats.ttest_ind(group1, group2)\n",
        "      percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "      percent2 = 100*(len(group2.index)/len(user_exposures.index))\n",
        "      #print('+++++++++TEST STAT SIGNIFICANCE INTER GROUPS+++++++++')\n",
        "      #print('GROUP '+ str(id1) +':')\n",
        "      #print(user_exposures[user_exposures['group_id'] == id1])\n",
        "      #print('GROUP '+ str(id2) +':')\n",
        "      #print(user_exposures[user_exposures['group_id'] == id2])\n",
        "      #print('RESULT:')\n",
        "      #print(result)\n",
        "      with open(filePath, 'a') as output:\n",
        "        output.write('Percent GROUP '+ str(id1) +': ' + str(percent1) + '\\nPercent GROUP '+\n",
        "                    str(id2) +': ' + str(percent2) + '\\nResult: ')\n",
        "        output.write(str(result)+'\\n\\n')\n",
        "      if result[1] <= 0.05:\n",
        "        update_result_table(filename[:-6], filename[-5:-1], (int(id1),int(id2)),\n",
        "                          result[1], destination)\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('----------------------------------------\\n\\n')\n",
        "    ciclo = ciclo + 1\n",
        "  return filePath\n",
        "\n",
        "def stat_significance_inter_rankings(df_user_exposures, df_random_user_exposures, method,\n",
        "                                     filename, business_id, group_ids, destination):\n",
        "  filePath = 'stat_rankings_' + method + '_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  ciclo = 0\n",
        "  while ciclo < 3:\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('Without first and last ' + str(ciclo) + ' rows:\\n\\n')\n",
        "    user_exposures = df_user_exposures.iloc[ciclo:(len(df_user_exposures.index)-ciclo)]\n",
        "    random_user_exposures = df_random_user_exposures.iloc[ciclo:(len(df_random_user_exposures.index)-ciclo)]\n",
        "    for group_id in group_ids:\n",
        "      group1 = user_exposures[user_exposures['group_id'] == group_id]['exposure']\n",
        "      group2 = random_user_exposures[random_user_exposures['group_id'] == group_id]['exposure']\n",
        "      result = stats.ttest_ind(group1, group2)\n",
        "      percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "      percent2 = 100*(len(group2.index)/len(random_user_exposures.index))\n",
        "      print('+++++++++TEST STAT SIGNIFICANCE INTER RANKINGS+++++++++')\n",
        "      #print('GROUP '+ str(group_id))\n",
        "      #print('Experiment ranking:\\n')\n",
        "      #print(user_exposures[user_exposures['group_id'] == group_id])\n",
        "      #print('Control ranking:\\n')\n",
        "      #print(random_user_exposures[random_user_exposures['group_id'] == group_id])\n",
        "      print('RESULT:')\n",
        "      print(result)\n",
        "      with open(filePath, 'a') as output:\n",
        "        output.write('Percent GROUP '+ str(group_id) +': ' + str(percent1) + '\\nResult: ')\n",
        "        output.write(str(result)+'\\n\\n')\n",
        "      if result[1] <= 0.05:\n",
        "        update_result_table(filename[:-6], group_id, (filename[-5:-1],'random'), result[1], destination)\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('----------------------------------------\\n\\n')\n",
        "    ciclo = ciclo + 1\n",
        "  return filePath\n",
        "\n",
        "\n",
        "def update_result_table(exp_method, context, means, p_value, destination):\n",
        "  file_list = drive.ListFile({'q': \"'\" + destination + \"' in parents and trashed=false\"}).GetList()\n",
        "  for file in file_list:\n",
        "    if file['title']=='result.csv':\n",
        "      id_result_file = file['id']\n",
        "  download = drive.CreateFile({'id': id_result_file}) # id file gtree.csv\n",
        "  download.GetContentFile('result.csv')\n",
        "  df = pd.read_csv('result.csv')\n",
        "  new_row = {'Exposure_method':exp_method, 'Context':context,\n",
        "             'Means':str(means[0])+' - '+str(means[1]), 'P-value':p_value}\n",
        "  df = df.append(new_row, ignore_index=True)\n",
        "  df = drop_unnamed(df)\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('result.csv')\n",
        "  upload_file('result.csv', destination) # id folder data\n",
        "\n",
        "\n",
        "def disparate_impact_exposure(df_group, ranking, reviews, user_exposures, business_id, group_index, alluser):\n",
        "  all_exposures = []\n",
        "  for i, user in df_group.iterrows():\n",
        "    user_id = user['user_id']\n",
        "\n",
        "    if alluser:\n",
        "      useful = ranking[ranking['user_id'] == user_id]['useful_votes'].values[0]\n",
        "      funny = ranking[ranking['user_id'] == user_id]['funny_votes'].values[0]\n",
        "      cool = ranking[ranking['user_id'] == user_id]['cool_votes'].values[0]\n",
        "    else:\n",
        "      #Integrate reviews info for clicks counting\n",
        "      df_temp = reviews[(reviews['business_id'] == business_id) & (reviews['user_id'] == user_id)]\n",
        "      # FUNZIONA SOLO NEL DATASET\n",
        "      useful = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['useful']\n",
        "      funny = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['funny']\n",
        "      cool = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['cool']\n",
        "    \n",
        "    counts = useful + funny + cool + 2\n",
        "    \n",
        "    base = 2  # con 10 i valori sono troppo bassi\n",
        "    counts = math.log(counts, base)\n",
        "    \n",
        "    position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "    \n",
        "    current_exp = exp(position) * counts\n",
        "    \n",
        "    all_exposures.append(current_exp)\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "  print('all_exposures size:', len(all_exposures))\n",
        "  mean = st.mean(all_exposures)\n",
        "  return mean, user_exposures\n",
        "\n",
        "  '''if len(df_group.index) == 0:\n",
        "    return 0\n",
        "  return dmp_sommatory(df_group, ranking, business_id, reviews)/(len(df_group.index))'''\n",
        "\n",
        "\n",
        "def demographic_parity_exposure(df_group, ranking, user_exposures, group_index):\n",
        "    all_exposures = []\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "        current_exp = exp(position)\n",
        "        all_exposures.append(current_exp)\n",
        "        user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "        user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "    print('all_exposures size:', len(all_exposures))\n",
        "    mean = st.mean(all_exposures)\n",
        "    return mean, user_exposures\n",
        "    \n",
        "    '''if len(df_group.index) == 0:\n",
        "      return 0\n",
        "    return demgr_sommatory(df_group, ranking)/(len(df_group.index))'''\n",
        "\n",
        "\n",
        "'''def dmp_sommatory(df_group, ranking, business_id, reviews):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        #Integrate reviews info for clicks counting\n",
        "        df_temp = reviews[(reviews['business_id'] == business_id) & (reviews['user_id'] == user_id)]\n",
        "        # FUNZIONA SOLO NEL DATASET\n",
        "        useful = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['useful']\n",
        "        funny = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['funny']\n",
        "        cool = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['cool']\n",
        "        counts = useful + funny + cool + 1\n",
        "        base = 2  # con 10 i valori sono troppo bassi\n",
        "        counts = math.log(counts, base) \n",
        "\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        addend = exp(position) * counts\n",
        "        sum = sum + addend\n",
        "    return sum\n",
        "'''\n",
        "\n",
        "'''def demgr_sommatory(df_group, ranking):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        sum = sum + exp(position)\n",
        "    return sum'''\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/(np.log(1 + position))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPGOC05EljLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_plots(yelp_exposures, date_exposures, random_exposures, percents, title,\n",
        "              list_of_attributes, method, business_id, alluser, errors, destination):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  print(y1)\n",
        "\n",
        "  '''y1_error = np.std(y1)\n",
        "  y2_error = np.std(y2)\n",
        "  y3_error = np.std(y3)'''\n",
        "  '''# I have missing user when using dataset OR using demographic attributes\n",
        "  if alluser and set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    y1_error = st.pstdev(y1)\n",
        "    y2_error = st.pstdev(y2)\n",
        "    y3_error = st.pstdev(y3)\n",
        "  else:\n",
        "    y1_error = st.stdev(y1)\n",
        "    y2_error = st.stdev(y2)\n",
        "    y3_error = st.stdev(y3)'''\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='yelp', yerr=errors[0], capsize=5)\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date', yerr=errors[1], capsize=5)\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random', yerr=errors[2], capsize=5)\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0),this_range)\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def get_scatter_plots(yelp_user_exposures, date_user_exposures, random_user_exposures, title,\n",
        "              list_of_attributes, method, business_id, alluser, destination):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 1.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_user_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_user_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_user_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_user_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_user_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_user_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  plt.scatter(x1,y1, s=10, color='red', label='yelp')\n",
        "  plt.scatter(x2,y2, s=10, color='green', label='date')\n",
        "  plt.scatter(x3,y3, s=10, color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  #this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0))\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-scx9i9b-2Fl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "outputId": "6792b35e-5cee-45e8-a187-84a44bfd78f1"
      },
      "source": [
        "# TEST THE STATISTICAL SIGNIFICANCE AND ERROR BARS\n",
        "\n",
        "#\tuser_id\t            group_id\texposure\n",
        "#  0\t4vdFVrrYxqb4ofj8RK8xvw\t2 \t1.442695041\n",
        "#  1\t7BEIyQzlskOV1OAJ0nbweQ\t1 \t0.9102392266\n",
        "#  2\tGaWT2KIjIxfIBMgGIm7HGw\t0 \t0.7213475204\n",
        "# 13\tfEYb-rRR1SYSlUnSk0ewpA\t1 \t0.3692693731\n",
        "# 14\tUWK3mKjnsVOA6sGim6zfgg\t2 \t0.3606737602\n",
        "# 15\tw8xU5z_-yCXspCfB5bWTJw\t0 \t0.3529561239\n",
        "#126\tfgRRod48nvfP4WwXiQnNSg\t2 \t0.2060992916\n",
        "#127\tWxd9_FATaozi0dhgeh3cPQ\t1 \t0.2057692596\n",
        "#128\tvQkm_xkJX-OqjZ_PF-pDwA\t0 \t0.2054428192\n",
        "\n",
        "test_ranking = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "test_ranking['user_id'] = ['4vdFVrrYxqb4ofj8RK8xvw',\n",
        "                           '7BEIyQzlskOV1OAJ0nbweQ',\n",
        "                           'GaWT2KIjIxfIBMgGIm7HGw',\n",
        "                           'fEYb-rRR1SYSlUnSk0ewpA',\n",
        "                           'UWK3mKjnsVOA6sGim6zfgg',\n",
        "                           'w8xU5z_-yCXspCfB5bWTJw',\n",
        "                           'fgRRod48nvfP4WwXiQnNSg',\n",
        "                           'Wxd9_FATaozi0dhgeh3cPQ',\n",
        "                           'vQkm_xkJX-OqjZ_PF-pDwA',\n",
        "                          ]\n",
        "test_ranking['group_id'] = [2,1,0,1,2,0,2,1,0]\n",
        "test_ranking['exposure'] = [1.442695041,0.9102392266,\t0.7213475204,\n",
        "                            0.3692693731,0.3606737602,0.3529561239,\n",
        "                            0.2060992916,0.2057692596,0.2054428192]\n",
        "\n",
        "group0 = test_ranking[test_ranking['group_id'] == 0]['exposure']\n",
        "group1 = test_ranking[test_ranking['group_id'] == 1]['exposure']\n",
        "group2 = test_ranking[test_ranking['group_id'] == 2]['exposure']\n",
        "result = stats.ttest_ind(group1, group2)\n",
        "percent1 = 100*(len(group1.index)/len(test_ranking.index))\n",
        "percent2 = 100*(len(group2.index)/len(test_ranking.index))\n",
        "print('+++++++++TEST STATISTICAL SIGNIFICANT+++++++++')\n",
        "print('GROUP 0:')\n",
        "print(test_ranking[test_ranking['group_id'] == 0])\n",
        "print('GROUP 1:')\n",
        "print(test_ranking[test_ranking['group_id'] == 1])\n",
        "print('GROUP 2:')\n",
        "print(test_ranking[test_ranking['group_id'] == 2])\n",
        "print('STATISTICAL SIGNIFICANCE:')\n",
        "print(result)\n",
        "\n",
        "test_exposure = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "test_exposure['group_id'] = [0,1,2]\n",
        "test_exposure['exposure'] = [st.mean(group0), st.mean(group1), st.mean(group2)]\n",
        "\n",
        "print(test_exposure)\n",
        "\n",
        "errors = [np.std(group0), np.std(group1), np.std(group2)]\n",
        "print(errors)\n",
        "\n",
        "x1 = [el['group_id'] for i, el in test_exposure[['group_id']].iterrows()]\n",
        "y1 = [el['exposure'] for i, el in test_exposure[['exposure']].iterrows()]\n",
        "width = 0.20\n",
        "plt.bar(x1,y1,width=width,align='center', color='red', label='yelp', yerr=errors, ecolor='black', capsize=5)\n",
        "axes = plt.gca()\n",
        "axes.yaxis.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++++++++TEST STATISTICAL SIGNIFICANT+++++++++\n",
            "GROUP 0:\n",
            "                  user_id  group_id  exposure\n",
            "2  GaWT2KIjIxfIBMgGIm7HGw         0  0.721348\n",
            "5  w8xU5z_-yCXspCfB5bWTJw         0  0.352956\n",
            "8  vQkm_xkJX-OqjZ_PF-pDwA         0  0.205443\n",
            "GROUP 1:\n",
            "                  user_id  group_id  exposure\n",
            "1  7BEIyQzlskOV1OAJ0nbweQ         1  0.910239\n",
            "3  fEYb-rRR1SYSlUnSk0ewpA         1  0.369269\n",
            "7  Wxd9_FATaozi0dhgeh3cPQ         1  0.205769\n",
            "GROUP 2:\n",
            "                  user_id  group_id  exposure\n",
            "0  4vdFVrrYxqb4ofj8RK8xvw         2  1.442695\n",
            "4  UWK3mKjnsVOA6sGim6zfgg         2  0.360674\n",
            "6  fgRRod48nvfP4WwXiQnNSg         2  0.206099\n",
            "RESULT:\n",
            "Ttest_indResult(statistic=-0.3940338377664993, pvalue=0.713659758991346)\n",
            "   group_id  exposure\n",
            "0         0  0.426582\n",
            "1         1  0.495093\n",
            "2         2  0.669823\n",
            "[0.21695623132558242, 0.30104606585764787, 0.550134565950428]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHR1vvEz-yk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "223b2a67-824a-4ecc-8c0c-aaab5374131a"
      },
      "source": [
        "def print_statistics_of_demographics(id):\n",
        "  df = pd.read_csv('demographics_' + id + '.csv')\n",
        "  all_users = len(df.index)\n",
        "  print(all_users)\n",
        "\n",
        "  print('-------------' + id + '--------------')\n",
        "\n",
        "  u_0_20 = len(df[(df['age']>=0) & (df['age']<=20)])\n",
        "  print('Utenti in fascia et√† 0-20: %d (%.2f%%)' % (u_0_20, (u_0_20/all_users)*100))\n",
        "\n",
        "  u_20_40 = len(df[(df['age']>20) & (df['age']<=40)])\n",
        "  print('Utenti in fascia et√† 20-40: %d (%.2f%%)' % (u_20_40, (u_20_40/all_users)*100))\n",
        "\n",
        "  u_40_60 = len(df[(df['age']>40) & (df['age']<=60)])\n",
        "  print('Utenti in fascia et√† 40-60: %d (%.2f%%)' % (u_40_60, (u_40_60/all_users)*100))\n",
        "\n",
        "  u_60_90 = len(df[(df['age']>60) & (df['age']<=90)])\n",
        "  print('Utenti in fascia et√† 60-90: %d (%.2f%%)' % (u_60_90, (u_60_90/all_users)*100))\n",
        "\n",
        "  u_over_90 = len(df[(df['age']>90)])\n",
        "  print('Utenti in fascia et√† >90: %d (%.2f%%)' % (u_over_90, (u_over_90/all_users)*100))\n",
        "\n",
        "  #temp = df[(df['age']>=0) & (df['age']<=20)][['user_id', 'age']]\n",
        "  #temp = drop_unnamed(temp)\n",
        "  #print(temp)\n",
        "\n",
        "  print('--------------------')\n",
        "\n",
        "  u_fem = len(df[df['gender']=='feminine'])\n",
        "  print('Utenti femmine: %d (%.2f%%)' % (u_fem, (u_fem/all_users)*100))\n",
        "\n",
        "  u_mas = len(df[df['gender']=='masculine'])\n",
        "  print('Utenti maschi: %d (%.2f%%)' % (u_mas, (u_mas/all_users)*100))\n",
        "\n",
        "  print('--------------------')\n",
        "\n",
        "  u_white = len(df[df['ethnicity']=='white'].reset_index(drop=True))\n",
        "  print('Utenti bianchi: %d (%.2f%%)' % (u_white, (u_white/all_users)*100))\n",
        "  bianchi = df[df['ethnicity']=='white'].reset_index(drop=True)\n",
        "  b_under_39 = len(bianchi[(bianchi['age']<39)])\n",
        "  print('Bianchi in fascia et√† <39: %d (%.2f%%)' % (b_under_39, (b_under_39/all_users)*100))\n",
        "  b_over_39 = len(bianchi[(bianchi['age']>=39)])\n",
        "  print('Bianchi in fascia et√† >39: %d (%.2f%%)' % (b_over_39, (b_over_39/all_users)*100))\n",
        "  b_fem = len(bianchi[bianchi['gender']=='feminine'])\n",
        "  print('Bianchi femmine: %d (%.2f%%)' % (b_fem, (b_fem/all_users)*100))\n",
        "  b_mas = len(bianchi[bianchi['gender']=='masculine'])\n",
        "  print('Bianchi maschi: %d (%.2f%%)' % (b_mas, (b_mas/all_users)*100))\n",
        "\n",
        "\n",
        "  u_black = len(df[df['ethnicity']=='black or african american'])\n",
        "  print('Utenti neri: %d (%.2f%%)' % (u_black, (u_black/all_users)*100))\n",
        "  neri = df[df['ethnicity']=='black or african american']\n",
        "  n_under_39 = len(neri[(neri['age']<39)])\n",
        "  print('Neri in fascia et√† <39: %d (%.2f%%)' % (n_under_39, (n_under_39/all_users)*100))\n",
        "  n_over_39 = len(neri[(neri['age']>=39)])\n",
        "  print('Neri in fascia et√† >39: %d (%.2f%%)' % (n_over_39, (n_over_39/all_users)*100))\n",
        "  n_fem = len(neri[neri['gender']=='feminine'])\n",
        "  print('Neri femmine: %d (%.2f%%)' % (n_fem, (n_fem/all_users)*100))\n",
        "  n_mas = len(neri[neri['gender']=='masculine'])\n",
        "  print('Neri maschi: %d (%.2f%%)' % (n_mas, (n_mas/all_users)*100))\n",
        "\n",
        "  u_altro = len(df[(df['ethnicity']=='hispanic, latino, or spanish origin') | (df['ethnicity']=='asian') | (df['ethnicity']=='middle eastern or north african') | (df['ethnicity']=='native hawaiian or pacific islander')])\n",
        "  print('Utenti altre etnie: %d (%.2f%%)' % (u_altro, (u_altro/all_users)*100))\n",
        "  altri = df[(df['ethnicity']=='hispanic, latino, or spanish origin') | (df['ethnicity']=='asian') | (df['ethnicity']=='middle eastern or north african') | (df['ethnicity']=='native hawaiian or pacific islander')]\n",
        "  a_under_39 = len(altri[(altri['age']<39)])\n",
        "  print('Altri in fascia et√† <39: %d (%.2f%%)' % (a_under_39, (a_under_39/all_users)*100))\n",
        "  a_over_39 = len(altri[(altri['age']>=39)])\n",
        "  print('Altri in fascia et√† >39: %d (%.2f%%)' % (a_over_39, (a_over_39/all_users)*100))\n",
        "  a_fem = len(altri[altri['gender']=='feminine'])\n",
        "  print('Altri femmine: %d (%.2f%%)' % (a_fem, (a_fem/all_users)*100))\n",
        "  a_mas = len(altri[altri['gender']=='masculine'])\n",
        "  print('Altri maschi: %d (%.2f%%)' % (a_mas, (a_mas/all_users)*100))\n",
        "\n",
        "  '''u_latino = len(df[df['ethnicity']=='hispanic, latino, or spanish origin'])\n",
        "  print('Utenti latini: %d (%.2f%%)' % (u_latino, (u_latino/all_users)*100))\n",
        "\n",
        "  u_asian = len(df[df['ethnicity']=='asian'])\n",
        "  print('Utenti asiatici: %d (%.2f%%)' % (u_asian, (u_asian/all_users)*100))\n",
        "\n",
        "  u_arabs = len(df[df['ethnicity']=='middle eastern or north african'])\n",
        "  print('Utenti arabi: %d (%.2f%%)' % (u_arabs, (u_arabs/all_users)*100))\n",
        "\n",
        "  u_hawa = len(df[df['ethnicity']=='native hawaiian or pacific islander'])\n",
        "  print('Utenti hawaiiani: %d (%.2f%%)' % (u_hawa, (u_hawa/all_users)*100))'''\n",
        "\n",
        "\n",
        "\n",
        "  print('---------ALTRE MISURE-----------')\n",
        "\n",
        "  u_0_35 = len(df[(df['age']>=0) & (df['age']<35)])\n",
        "  print('Utenti in fascia et√† 0-35: %d (%.2f%%)' % (u_0_35, (u_0_35/all_users)*100))\n",
        "  \n",
        "  u_35_40 = len(df[(df['age']>=35) & (df['age']<=40)])\n",
        "  print('Utenti in fascia et√† 35-40: %d (%.2f%%)' % (u_35_40, (u_35_40/all_users)*100))\n",
        "\n",
        "  u_over_40 = len(df[(df['age']>40)])\n",
        "  print('Utenti in fascia et√† >40: %d (%.2f%%)' % (u_over_40, (u_over_40/all_users)*100))\n",
        "\n",
        "  print('---------ALTRE MISURE 2-----------')\n",
        "\n",
        "  u_under_39 = len(df[(df['age']<39)])\n",
        "  print('Utenti in fascia et√† <39: %d (%.2f%%)' % (u_under_39, (u_under_39/all_users)*100))\n",
        "  \n",
        "  u_over_39 = len(df[(df['age']>=39)])\n",
        "  print('Utenti in fascia et√† >39: %d (%.2f%%)' % (u_over_39, (u_over_39/all_users)*100))\n",
        "\n",
        "\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "for id in id_list:\n",
        "  print_statistics_of_demographics(id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1209\n",
            "-------------WbJ1LRQdOuYYlRLyTkuuxw--------------\n",
            "Utenti in fascia et√† 0-20: 25 (2.07%)\n",
            "Utenti in fascia et√† 20-40: 865 (71.55%)\n",
            "Utenti in fascia et√† 40-60: 293 (24.23%)\n",
            "Utenti in fascia et√† 60-90: 24 (1.99%)\n",
            "Utenti in fascia et√† >90: 2 (0.17%)\n",
            "--------------------\n",
            "Utenti femmine: 775 (64.10%)\n",
            "Utenti maschi: 434 (35.90%)\n",
            "--------------------\n",
            "Utenti bianchi: 693 (57.32%)\n",
            "Bianchi in fascia et√† <39: 222 (18.36%)\n",
            "Bianchi in fascia et√† >39: 471 (38.96%)\n",
            "Bianchi femmine: 422 (34.90%)\n",
            "Bianchi maschi: 271 (22.42%)\n",
            "Utenti neri: 245 (20.26%)\n",
            "Neri in fascia et√† <39: 111 (9.18%)\n",
            "Neri in fascia et√† >39: 134 (11.08%)\n",
            "Neri femmine: 178 (14.72%)\n",
            "Neri maschi: 67 (5.54%)\n",
            "Utenti altre etnie: 270 (22.33%)\n",
            "Altri in fascia et√† <39: 127 (10.50%)\n",
            "Altri in fascia et√† >39: 143 (11.83%)\n",
            "Altri femmine: 174 (14.39%)\n",
            "Altri maschi: 96 (7.94%)\n",
            "---------ALTRE MISURE-----------\n",
            "Utenti in fascia et√† 0-35: 306 (25.31%)\n",
            "Utenti in fascia et√† 35-40: 584 (48.30%)\n",
            "Utenti in fascia et√† >40: 319 (26.39%)\n",
            "---------ALTRE MISURE 2-----------\n",
            "Utenti in fascia et√† <39: 460 (38.05%)\n",
            "Utenti in fascia et√† >39: 749 (61.95%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Iq3ciMMcZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO UPDATE CENTROIDS\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg'] \n",
        "method = 'kmeans'\n",
        "for id in id_list:\n",
        "  pd.options.display.float_format = '{:.3f}'.format\n",
        "  centroids = pd.read_csv('centroids_' + method + '_' + id + '.csv')\n",
        "  values = [float(x) for x in centroids.columns.values]\n",
        "  centroids.loc[-1] = values # adding a row\n",
        "  centroids.index = centroids.index + 1  # shifting index\n",
        "  centroids.sort_index(inplace=True)\n",
        "  centroids.columns = ['loc1', 'loc2', 'loc3']  \n",
        "  centroids.to_csv('centroids_' + method + '_' + id + '.csv', float_format='%.3f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22E0H3eY8Vb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO UPDATE EXPOSURE\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA']\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg'] #RICALCOLARE DA CAPO\n",
        "# id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "for id in id_list:\n",
        "  df_rel_ranking = pd.read_csv('dataset_rel_ranking_'+id+'.csv')\n",
        "  df_date_ranking = pd.read_csv('dataset_date_ranking_'+id+'.csv')\n",
        "  df_rand_ranking = pd.read_csv('dataset_rand_ranking_'+id+'.csv')\n",
        "\n",
        "  method = 'kmeans'\n",
        "  N_of_groups = 5\n",
        "  df_groups = pd.read_csv('groups_' + method + '_'+id+'.csv')\n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews, percents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FcLo9mPIeHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "33fd5056-7082-48a9-a124-d3f466d7fbcb"
      },
      "source": [
        "# TO UPDATE PLOTS\n",
        "id = 'WbJ1LRQdOuYYlRLyTkuuxw'\n",
        "method = 'custom'\n",
        "list_of_attributes = ['ethnicity']\n",
        "exp = 'dispimp'\n",
        "alluser = False\n",
        "yelp_exposures = pd.read_csv('exp_' + exp + '_' + method + '_yelp_' + id + '.csv')\n",
        "date_exposures = pd.read_csv('exp_' + exp + '_' + method + '_date_' + id + '.csv')\n",
        "random_exposures = pd.read_csv('exp_' + exp + '_' + method + '_random_' + id + '.csv')\n",
        "\n",
        "yelp_user_exposures = pd.read_csv('user_exp_' + exp + '_' + method + '_yelp_' + id + '.csv')\n",
        "date_user_exposures = pd.read_csv('user_exp_' + exp + '_' + method + '_date_' + id + '.csv')\n",
        "rand_user_exposures = pd.read_csv('user_exp_' + exp + '_' + method + '_random_' + id + '.csv')\n",
        "\n",
        "\n",
        "if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "\n",
        "df_groups = pd.read_csv('groups_' + method + '_' + id + '.csv')\n",
        "N_of_groups = 5\n",
        "percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "\n",
        "get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_' + exp + '_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error])\n",
        "get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                  'scatter_' + exp + '_' + method + '_' + business_id, list_of_attributes,\n",
        "                  method, business_id, alluser)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[21.27659574468085, 20.10981468771448, 18.874399450926564, 19.766643788606725, 19.97254632807138]\n",
            "[0.15198681815318926, 0.15441688991439814, 0.15644457679065246, 0.1655399171123573, 0.2021502908541474]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-8fb25b50a9d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n\u001b[1;32m     33\u001b[0m                   \u001b[0;34m'scatter_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbusiness_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                   method, business_id, alluser)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-ba29cc803c3d>\u001b[0m in \u001b[0;36mget_scatter_plots\u001b[0;34m(yelp_user_exposures, date_user_exposures, random_user_exposures, title, list_of_attributes, method, business_id, alluser)\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'destination' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3xU9Z3/8deHcImRBCpe6tZLgCoIVO7aegVtU0v3F9cCW6lri5etu0JXa9vdbf09vC7bdtvtulW6q62XWi1UsIWppT/TVvGyVbkosIigAralF7HYQiAQAvn8/jiTGEJmcnKSk3Nm5v18POYx5ztzzswnZzLzOZfv93PM3RERkdLVJ+kAREQkWUoEIiIlTolARKTEKRGIiJQ4JQIRkRLXN+kAuuroo4/26urqpMMQESkoq1ev/qO7H9PRcwWXCKqrq1m1alXSYYiIFBQz+1Wu53RoSESkxCkRiIiUOCUCEZESV3DnCEQAmpqa2LZtG/v27Us6lFQrLy/nhBNOoF+/fkmHIimmRCAFadu2bVRWVlJdXY2ZJR1OKrk7O3bsYNu2bQwdOjTpcCTFdGgoJpkMzJ0b3EvP27dvH0OGDFESyMPMGDJkiPaa4hLxS57ZlGHusrlkNqXnx0GJIAaZDMyaBfPnB/dKBvFQEuic1lFMIn7JM5syzHp0FvNXzmfWo7NSkwyUCGJQVwcNDcF0Q0PQFpEiEvFLXre5joamYLmGpgbqNqfjx0GJIAY1NVBREUxXVARtkYEDByYdgvSUiF/ymuE1VPQLlqvoV0HN8HT8OOhkcQxqa2HBgmAjoaYmaItIEYn4Ja8dUcuC6Quo21xHzfAaakek48fB4rpCmZndB/wlsN3dx+SZbzLwHHCpuy/u7HUnTZrkKjEhr7zyCqeddlqiMdx0000cddRRXH/99QDceOONHHvssezfv59HHnmExsZGLrnkEm699VYg2CPYvXs3y5cv56abbqKyspLXX3+dqVOn8q1vfYs+feLZQU/DupLkmdlqd5/U0XNxHhp6ALgo3wxmVgZ8FUjHgTIpbj3clevKK6/kwQcfBKC5uZmFCxfy7ne/m9dee40VK1awZs0aVq9ezdNPP33YsitWrODOO+9kw4YNbN68mR/+8Ic9EpNIFLElAnd/Gni7k9k+AzwKbI8rDhEglq5c1dXVDBkyhJdeeom6ujrGjx/PypUrW6cnTJjAxo0bee211w5b9owzzmDYsGGUlZUxa9Ysnn322W7HIxJVYucIzOw9wCXAVGByJ/N+Gvg0wHHHHcfy5ctjj0/SbdCgQdTX14eef8BPfkL/Nr089v/kJzROndrtOC677DLuuecetm/fzqxZs3jqqaf47Gc/y5VXXnnIfC2x1tfX09DQQHNzc+tj+/bto6mpqUt/T1fs27dP3xnJz91juwHVwPoczy0C3p+dfgCYEeY1J06c6CIbNmzo2gJLl7pXVLhDcL90aY/E0djY6KeeeqoPHTrUDxw44I8//rifccYZXl9f7+7u27Zt8zfffNPd3Y888kh3d3/yySe9vLzct2zZ4gcPHvSamhpfvHhxj8TTkS6vKylKwCrP8buaZK+hScDC7ICXo4FpZnbA3ZckGJMUq5i6cvXv35+pU6cyePBgysrKqKmp4ZVXXuEDH/gAEJwgfuihhzj22GMPWW7y5MnMnTu39WTxJZdc0iPxiESRWCJw99biJ2b2APCYkoDEqra2x/vyNjc38/zzz7No0aLWx6677jquu+66w+bdvXt363RVVRWPPfZYj8YiElVsJ4vNbAFBt9ARZrbNzK4ys78zs7+L6z1FetOGDRt473vfy4UXXsgpp5ySdDgikcW2R+Dus7ow7+y44hCJy6hRo9iyZUuXl5syZQpTpkzp+YBEIlKJCRGREqdEICJS4pQIRERKnBKBiEiJUyIQ6SG33HILX//613M+v2TJEjZs2NCLEYmEE2f30fvMbLuZrc/x/MVmts7M1pjZKjM7J65YRNJAiUDSKsnqo78Axrr7OOBK4DsxxiISi3nz5nHqqadyzjnnsGnTJgC+/e1vM3nyZMaOHcv06dNpaGjgl7/8JZlMhi984QuMGzeOzZs3s3nzZi666CImTpzIueeey8aNGxP+a6RUJVZ91N13Z+tfABwJxHNhBJGsnr5o+OrVq1m4cCFr1qxh2bJlrFy5EoCPfexjrFy5krVr13Laaadx7733ctZZZ1FbW8vXvvY11qxZw/Dhw/n0pz/NnXfeyerVq/n617/Otdde2yNxiXRVolcoM7NLgC8DxwIfzTOfqo/KIbpafXTZ5mVc8ZMr2HtgL/e9dB/3f/R+pg2f1q0YfvaznzFt2jQOHjyImXHRRRfR2NjIihUruP3229m5cyd79uzhwgsvpL6+nqamJvbu3Ut9fT27d+/ml7/8JdOnT299vcbGxlgqkKr6qHQm0UTg7j8CfmRm5wG3Ax/MMd89wD0QXKFMozLllVdeobKyMvT8z/zuGfYe2AvA3gN7eeZ3z/DxcR/vVgzl5eUMGDCgNY7+/fszYMAArr32WpYsWcLYsWN54IEHWL58OZWVlfTr148jjjiCyspK3J3Bgwezbt26bsUQNs7x48fH/j5SuFLRayh7GGmYmR2ddCxSnOK4aPh5553HkiVLWrfyf/zjHwPBNQeOP/54mpqaePjhh1vnr6ysbN3ir6qqYujQoa3F6tydtWvXdjsmkSgSSwRm9l7L1qA2swnAAGBHUvFIcWu5aPicyXNYMH1Bj1w0fMKECXz84x9n7NixfOQjH2Hy5OD6SrfffjtnnnkmZ599NiNHjmyd/9JLL+VrX/sa48ePZ/PmzTz88MPce++9jB07ltGjR7N06dJuxyQSRZwXr18ATCG41sCbwM1APwB3/28z+yfgk0ATsBf4grt3er0+XbxeQBdk7wqtK4H8F69PrPqou3+V4ML1IiKSoFScIxARkeQoEYiIlDglAhGREqdEICJS4pQIRERKnBKBSEpUV1fzxz/+MekwpAQlWYb6smwZ6v81s1+a2di4YhGJm7vT3NycdBgikSRZhnorcL67v4+gztA9McYi0uPeeOMNRowYwSc/+UnGjBnDVVddxaRJkxg9ejQ333xz63zV1dXcfPPNTJgwgfe9732t5aZ37NhBTU0No0eP5uqrr6bt4M5vfOMbjBkzhjFjxnDHHXe0vt/IkSOZPXs2p556Kpdddhk///nPOfvssznllFNYsWJF764AKR7uHtsNqAbWh5jvXcBvw7zmxIkTXWTDhg1dXmbpUvc5c4L7nrB161Y3M3/uuefc3X3Hjh3u7n7gwAE///zzfe3ate7ufvLJJ/s3v/lNd3efP3++X3XVVe7u/pnPfMZvvfVWd3d/7LHHHPC33nrLV61a5WPGjPHdu3d7fX29jxo1yl988UXfunWrl5WV+bp16/zgwYM+YcIEv+KKK7y5udmXLFniF198cYdxRllXUnyAVZ7jdzXR6qNtXAX8NNeTKkMt7XW5DPWyMq644gj27jXuu8+5//69TJt2sFsx7N69m5NOOonRo0dTX1/Pgw8+yAMPPMCBAwf4wx/+wOrVqxk6dCjuTk1NDfX19YwcOZJFixZRX1/P8uXLeeihh6ivr+e8885j8ODB7N69m5///OdMmzat9VDTRz/60daS1yeffDLV1dXs2bOHU089lbPOOovdu3czdOhQtmzZ0uE6URlq6UziicDMphIkgpyXqnSVoZZ2ulyG+hnYG1ShZu9e45lnKvh496pQM3DgQAYOHEhlZSVbt27lrrvuYuXKlbzrXe9i9uzZmBmVlZWYGUOGDKGyspKqqircncrKSvr06dO6PICZMXDgwMPKWw8YMIDy8nIGDhzYWsa65fHBgwe3vm5zc3OH60RlqKUzifYaMrPTCS5RebG7q/KoxKamBiqCKtRUVATtnrRr1y6OPPJIBg0axJtvvslPf5pzB7fVeeedx/e//30AfvrTn/KnP/0JgHPPPZclS5bQ0NDAnj17+NGPfsS5557bswGLtJHYHoGZnQT8ELjc3V9NKg4pDbW1sGAB1NUFSaC2+1WoDzF27FjGjx/PyJEjOfHEEzn77LM7Xebmm29m1qxZjB49mrPOOouTTjoJCMpbz549mzPOOAOAq6++mvHjx/PGG2/0bNAiWUmWof4OMB34VXaRA56jRGpbKkMtoNLKXaF1JZDeMtRXA1fH9f4iIhKORhaLiJQ4JQIpWHEd1iwmWkcShhKBFKTy8nJ27NihH7o83J0dO3ZQXl6edCiScomPIxCJ4oQTTmDbtm289dZbSYeSauXl5ZxwwglJhyEpp0QgBalfv34MHTo06TBEikKS1UdHmtlzZtZoZp+PKw4REckvyeqjbwP/AHw9xhhERKQTsSUCd3+a4Mc+1/Pb3X0l0BRXDCIi0rmCOEeg6qMiIvEpiESg6qMiIvHROAIRkRKnRCAiUuJiOzTUtvqomW3j8Oqj7wZWAVVAs5ldD4xy911xxSQiIodLsvroHwANeRQRSZgODYmIlDglAhGREqdEICJS4pQIRERKnBKBiEiJUyIQESlxSZahNjP7ppm9bmbrzGxCXLGIiEhuSZah/ghwSvb2aeC/YoxFRERySKwMNXAx8KAHngcGm9nxccUjIiIdS7L66HuA37Rpb8s+9vv2M6oMtYhIfFSGWkSkxCXZa+i3wIlt2idkHxMRkV6UZCLIAJ/M9h56P7DT3Q87LCQiIvFKrAw1sAyYBrwONABXxBWLiIjklmQZagfmxPX+IiISjkYWi4iUOCUCEZESp0QgIlLilAhEREqcEoGISImLNRGY2UVmtilbYfSfO3j+ZDP7Rbb66HIz08XsC0BmU4a5y+aS2ZRJOpSCl8nA3LnBvRSYYvrw3D2WG1AGbAaGAf2BtcCodvMsAj6Vnb4A+F5nrztx4kSX5CzduNQr5lU4t+AV8yp86calSYdUsJYuda+ocIfgfqlWZeEowA8PWOU5flfj3CM4A3jd3be4+35gIUHF0bZGAU9kp5/s4HlJmbrNdTQ0NQDQ0NRA3ea6hCMqXHV10BCsShoagrYUiCL78OIsOtdRddEz282zFvgY8J/AJUClmQ1x9x1tZ1L10fT4UNmHGD5iOM3eTB/rw7CyYfo8IvrQh2D4cGhuhj59YNgw0KosEEX24SVdffTzwF1mNht4mqDo3MH2M7mqj6ZKZlOGus111AyvoXZEbdLhFLRMJtiYrKmBWq3KwlJEH54Fh45CzGh2DnCKu99vZscAA919a575PwDc4u4fzra/CODuX84x/0Bgo7vnPWE8adIkX7VqVaiYRUQkYGar3X1SR8+FOkdgZjcD/wR8MftQP+ChThZbCZxiZkPNrD9wKUHF0bave7SZtcTwReC+MPGIiEjPCXuy+BKgFtgD4O6/AyrzLeDuB4C5wOPAK8Aj7v6ymd1mZi37UVOATWb2KnAcMK/Lf4GIiHRL2HME+93dzcwBzOzIMAu5+zKCctNtH7upzfRiYHHIGEREJAZh9wgeMbO7CS4w/7fAz4FvxxeWiIj0lk73CMzMgB8AI4FdwAjgJnf/WcyxiYhIL+g0EWQPCS1z9/cB+vEXESkyYQ8NvWhmk2ONREREEhH2ZPGZwGVm9iuCnkNGsLNwemyRiYhIrwibCD4caxQiIpKYsIeGPMctrxBlqE8ysyfN7KVsKeppXQleRES6L+wewU8IfvgNKAeGApuA0bkWMLMyYD7wIYKCcyvNLOPuG9rM9n8JBpr9l5mNIhhzUN3VP0JERKILlQiyPYZamdkE4NpOFmstQ51dpqUMddtE4EBVdnoQ8Lsw8YiISM+JVH3U3V80s/YlpdsLU4b6FqDOzD4DHAl8sKMXUhlqEZH4hEoEZnZDm2YfYCI9s/U+C3jA3f89W630e2Y2xt2b286kMtQiIvEJu0fQtsDcAeAx4NFOlvktcGKb9gnZx9q6CrgIwN2fM7Ny4Ghge8i4RESkm8KeI7i1ZTpbNnqgu+/rZLHWMtQECeBS4BPt5vk1cCHwgJmdRnAi+q2QsYuISA8Iez2C75tZVbbq6Hpgg5l9Id8yIctQfw74WzNbCywAZnvYK+WIiEiPCHWFMjNb4+7jzOwyYALwz8DqJEYW6wplIiJd1+0rlAH9zKwf8FdAxt2bCDGgTERE0i9sIrgbeIOgi+fTZnYyQUlqEREpcGFPFn8T+Gabh35lZlPjCUlERHpT2JPFg8zsG2a2Knv7d4K9A8khk4G5c4N7KQ2ZTRnmLptLZpM+9JJQRF/ysCeLHyXoLfTd7EOXA2Pd/WMxxtahQjhZnMnArFnQ0AAVFbBgAdTWdr6cFK7MpgyzHp1FQ1MDFf0qWDB9AbUj9KEXrQL8kvfEyeLh7n6zu2/J3m4FhoV4486qj/6Hma3J3l41sz+HjCfV6uqC/w8I7uvqko1H4le3uY6GpuBDb2hqoG6zPvSiVmRf8rCJYK+ZndPSMLOzgb35FmhTffQjwChgVrbCaCt3/6y7j3P3ccCdwA+7Enxa1dQEGwkQ3NfUJBuPxK9meA0V/YIPvaJfBTXD9aEXtSL7koctMfH3wHfNbBBBKeq3gU91skyY6qNtzQJuDhlPqtXWBnuKdXXB/0fK9xilB9SOqGXB9AXUba6jZniNDgsVuyL7koc6R9A6s1kVgLt32nXUzGYAF7n71dn25cCZ7j63g3lPBp4HTnD3gx0837b66MSFCxeGjllERGDq1Kk5zxGErT46hGBr/RzAzexZ4DZ339FDMV4KLO4oCYCqj4qIxCnsOYKFBMXgpgMzstM/6GSZMNVHW1xKUGtIRER6WdhEcLy73+7uW7O3fwGO62SZ1uqjZtaf4Mf+sA63ZjYSeBfwXFcCFxGRnhE2EdSZ2aVm1id7+2uCqqI5haw+CkGCWKiqoyIiyQg7oKyeYCRxyzH8MmBPdtrdvarDBWNQCAPKRETSJt+AsrC1hio7n0tERApR2FpDV7Vrl5lZUfT5FxEpdWHPEVxoZsvM7HgzG0PQ5197CSIiRSDsoaFPmNnHgf8lODfwCXf/n1gjExGRXhH20NApwHXAo8CvgMvNrCLOwApdEVWoFZGORPySp7FcedheQxuBOe7+CzMz4AbgSncfHXeA7RVCr6ECrFArIl0R8UueZLnynihDfYa7/wKCvqLu/u/AJSHeOG8Z6uw8f21mG8zsZTP7fsh4Uq3IKtSKSHsRv+RpLVeeNxGY2T9CUGTOzGa2e3p2J8t2WoY6e8jpi8DZ2b2L67sUfUoVWYVaEWkv4pc8reXK8x4aMrMX3X1C++mO2h0s+wHgFnf/cLb9RQB3/3Kbef4NeNXdvxM24EI4NATBnmORVKgVkY5E/JJnNmUSKVfenQFllmO6o3Z77wF+06a9DTiz3TynZgP8H4LRyre4+/87LIhDy1CzfPnyTt46eVVVMGNGMF0A4YpIV0X8kldRxYyKGfB7WP778MvFqbNE4DmmO2pHff9TgCkE1UmfNrP3ufshl6xUGWoRkfh0lgjGmtkugq3/I7LTZNvlnSwbpgz1NuAFd28CtprZqwSJYWWY4EVEpPvynix29zJ3r3L3Snfvm51uaffr5LXDlKFeQrA3gJkdTXCoaEukv0RERCIJ2320y0KWoX4c2GFmG4AngS/04FXPJCZpHBBTqDTwsIAV0YfXpWsWp0Gh9BoqVkkOiCk2GnhYwArww+uJAWUiQHoHxBQiDTwsYEX24SkRSJekdUBMIdLAwwJWZB+eDg1JlyU1IKYYaeBhASuwDy/foSElAhGREqBzBAkoog4FItKRIipDHWsi6Kz6qJnNNrO3zGxN9nZ1nPH0lpYOBfPnB/dKBiJFJuKXvKXX3fyV85n16KzUJIPYEkGY6qNZP3D3cdlb6OJzaVZkHQpEpL1SKkPdTWcAr7v7FnffDywELo7x/VKjyDoUiEh7pVSGulsvbDYDuMjdr862LwfOdPe5beaZDXwZeAt4Ffisu/+mg9dqW3104sKFC2OJuSft3Am7dgUFCgcNSjoaEelxEb/kOxt3sqtxF1UDqhg0oPd+HKZOnRq5DHXcfgwscPdGM7sG+C5wQfuZVH1URFInk4EXXgj2Brrwm5TZlOGFzS9QU13DlBFdWC7G3qpxHhrqtPqou+9w98Zs8zvAxBjjERHpGb18sjjuDihxJoJOq4+a2fFtmrUExelERNKtl08Wx90BJenqo/+QvWj9WuAf6OQ6yCIiqdDLJ4vj7oCikcUiCSqwKgXS1o03Bh9gbS3Mmxd+sSduJLMpQ+2IWuZdEH657v6vqMSESAoVYCVjaRHxw0uyjLtKTIikkAYeFjANKBORjnS1hkxNDfTvH0z376+BhwUl4oeX1gFlSY8jECkKbXf5719zf+hdfrND76WARPjwakfUsmD6gtSVcdceQUxUfbS0RNnlr6uDxuwomsZGHRoqKEX24SkRxEDVR0tPlF1+1aQqYO1LSoQsMVFy1Ueh8zLUbeabbmZuZh2e0U5SlNrhOglYelp2+edMnhP6sFBtLVx/PYwZE9ynuseQdnEPtXNn/nYOaT1ZjLvHcgPKgM3AMKA/sBYY1cF8lcDTwPPApM5ed+LEid5blm5c6hXzKpxb8Ip5Fb5049Jwyy11r6hwh+B+abjFpMQUzP9JwQTaiyKuk6i/KT0BWOU5flfTUIb6duCrwL4YY4kkavaurQ26Fc+Zo77hklvB7DkWTKC9KOLuXO2IWq5///WMOXYM17//+tScLE66DPUE4EZ3n25my4HPu/tho8WSKkO9s3EnW/60hWZvpo/1Ydi7hvVq2Vgpbr/7Hfz+9++0jz8e/uIvkosnp507YcsWaG6GPn1g2DDVVo+4TpL8TUllGWoz6wN8gxD1hTzBMtSZTZnUdfWS4jB3btChoMWcOXDXXcnFk1fE+gZF+/2J+OHNXTaX+ZveWW7O5DncNSXch16sZagrgTHAcjN7A3g/kEnbCePaEbXcNe2u4von7qY0Xny7EBXUgLLa2uCHrotJII09ZHpExF5DUQeUFW0Zanff6e5Hu3u1u1cTnCyu7ejQkKRHUX+5E1DMA8pS20OmJ6xZk7+dQ5TeZVD8ZahTT1u/hyrqL3cvK7IxSYdJazmFpEU5yhD33mOs4wjcfZm7n+ruw919Xvaxm9z9sF9Vd5+Str0Bbf0eTl/unlNQh4YiiLr1WxDGjcvfziPqxmWce48aWZxHd7Z+i3X8TVq7vxWqYj40BEV8ji3ioaGoG5dx7z0qEeRRM7yG/mXBJlv/sv6pObGTpMymDHc8fwfrt6/njufv0F5SG13d0iv2Q0PdkfoNqTffzN/OIerGZdzlSJQIOtHszYfch1HM4290jqBjUbb0InY8SUYv/jJnMjBzZrAhNXNmSpPBn/+cv51D1EOrcQ9SVSLI4+5Vd3Og+QAAB5oPcPequ0MtV8zFxKLuJRW7KAkyYrma3tfLv8x33w379wfT+/cH7dQ5cCB/O4e0njdRIohBQRUTi8CwQ+6lyKuPduOXuWh73V12Wf52DyvkcQSdVh81s78zs/81szVm9qyZjYoznq66ZtI1DCgbAMCAsgFcM+maUMtlMnDHHbB+fXCfyl3biOo219F4MDiw3XiwUSfQs6JWHy3mmlRRT4x2o0NO6mU2ZZj+yHTmr5zP9Eemd+l8UkGOIzCzMmA+8BFgFDCrgx/677v7+9x9HPBvBCUnUqN2RC2PzHyEOZPn8MjMR1Iz+CNJaR0ZWaheeAGeeiq4T61rroG+2Wo0ffsG7RCink+K2CGnd91zT/52Drc9ddshh5tve+q2UMvFvfcYZ62h1uqjAGbWUn10Q8sM7r6rzfxHAvFUwOuG2hG1XT6OV1MD998fJIFU7/JHEPVSex0lx2LaAs5syjBz0Uz2H9zPt1/8NotmLup03dx4I/zrvwbT69cH9/PmxRxoVGVlwXHwsrLQi9QMr+H+NffT0NRQfGNOWg6V5Wrn8Oudv87bzqXlcHMmE0z39Hcn0eqj2cfnADcQXLPgAnd/rYPXSqT6KATVAnc17qJqQFWXqgTu3Am7dkFVVcp7g/SSnTth82ZwD/rMDx+e4vUS4cN7/e3X2dn4ztneQQMG8d6j3pt3mQ0bYO/ed9pHHAGjUnVwNOs3v4Ht299pH3ssnHhi7vnbiPL9KYiqrOvWQVPTO+1+/eD00ztdbP329a2HViE45Dzm2DGdLtcTBWDzVR+N88I0M4DvtGlfDtyVZ/5PAN/t7HV7+8I0/W/v79yC97+9f5cuIrF0qfucObqGR4ulS9379g2u49G3b4rXS8QLjkx7aJpzC623aQ9N63SZGTOCt2m5zZjR3eBj8qUvHRrol74U69tNm3bo203rfFX2vmOOOTTIY44Jtdjw/xx+yP/J8P8cHmq5OXMOfbs5c7oeMgldmKaz6qPtLQT+KsZ4uuzuVXez/2Cwy7f/4P7Q3UcLoh90iwhncKP0BLn77nd62B04kNIugRD5BE+UjgXHHZe/nRoFcdC+l7U/RBbykNlbe97K284l7jEniVUfBTCzU9o0PwocdlioEBVEP2iIdAa35Vj4/JXzmbloZvF1C4xYAKh2RC2fO+tzjDl2DJ8763Ohzp0Ue60hiLbR0HJSNFc7FVo2FnK1c2hqbsrbziXuMSdJVx+da2Yvm9kagvMEn4orniiidh8tGBG2fqPuJV1zTXBsE4L7kB1PkhGhAFBmU4avPPsV1m9fz1ee/UroH76CqDUU8Zc56kbDihX526nQ1JS/3cNeeil/u7sSrT7q7te5+2h3H+fuU9395Tjj6aqo3UcLph90L45oeuGF4EQXBPep7S4ZsQDQDY/fcEg5khsevyGut+p9EX+Zo240HHNM/nYqHDyYv53DwP4D87ZzWbcuf7u7NLK4E1GqJxbMIdXaWpg2DY46KrgP0SftmknX0LdP0Ou4b5++XRpkl6+dGhGT42/rf5u33ZGIdct6X79++ds5bNqxKW87lw9/OH87FSJ2Hx1SMSRvO5e+ffO3u0uJoBMzF81kyFeHMHPRzNDLvPhi/nZq3HgjLF4Mb78d3N94Y6jFyqzskPswRo7M306NCMkRoLysPG+7Ixs35m+nxuDB+ds57GrclbedS8FsSEXwxp/eyAWUsQsAAAkESURBVNvOJWIuDq10EkGE3jEzF81k8YbFvL3vbRZvWBw6GezYkb+dGhE206OWmCiYHjIRk2PbvuEdtTtSVZW/XeiqBlTlbeeyaVP+diGzdieD2rdzidhJKbTSSAQR6xs8seWJvO1c2n+2qT0R2H5rN8TW76DyQXnbucR9sqvHPPRQ/nYOLWUDcrU78uqr+duFbkfDjrztXHbtyt8uZC2dT3K1c4l4SiK00kgEEfuGXzDsgrztXAqmzvy8eTBjRnAYZMaMUPUNfrD+B3nbuaxcmb+dGhGP/fbv0z9vuyNvv52/nRq//W3+dg5/bvxz3nYuvdwhp1ftadqTt53LSSflb3dXaSSCiL/Ml59++SElly8//fJQyw0Zkr+dGpkMLFkS/AItWRJqT2nrn7bmbedSMF/uiJte+w7uy9vuSHNz/nZqtC0v0VG7h+3bl79dyLxdSZ/27Vz27Mnf7q6ky1DfYGYbzGydmf3CzE6OJZDHH8/fzuG2p27Ds3XwHA9dKXDz5vzt1LjttkOH+97W+d/XTHPedsGL+2BsGwMG5G+nRi9nrIjXfCkI7a90GPbKh3GfN0m6DPVLwCR3Px1YTFCKuuf9+tf527kWi1gpsGC2fjdsyN8uRRGP10TZ0iv2k8VRFXMi8HYFltu3cy7n+dvdFeceQWsZanffT1BL6OK2M7j7k+7eMjb7eYJ6RD0v6nDwg0152wWvsTF/uxRFPEcQZU+pYHqXSY9pf1W/tFzlL/Ey1G3mvwv4g7v/SwfPda8M9erVhz82cWLni/3+8OUmHh9iuWhv1/siBKp1kmOxCOtF66RH3653FeA6yVeGOhWJwMz+hqAu0fnunnezdNKkSb5q1aquBnP4YyH+brv18OX85hDLRXu73hchUK2THItFWC9aJz36dr2rANeJmeVMBHFeoSxUGWoz+yBwIyGSgIiI9Lyky1CPB+4Gat093j5pIiLSoaTLUH8NGAgsMrM1ZpbWUmQiIkUrzkNDuPsyYFm7x25qM/3BON9fREQ6Vxoji0VEJCclAhGREqdEICJS4pQIRERKnBKBiEhX9emTv11gkq4+ep6ZvWhmB7IjkeMRsWJT+xF/YUYAduPtel+EQLVOciwWYb1onfTo2/Wugwff+fHv0yd0ufK0rpM4S0yUAa8CHwK2EQwwm+XuG9rMUw1UAZ8HMu6+uLPXjVRiQkSkxCVVYqK1+mg2iJbqo62JwN3fyD5XZEXtRUQKR5yJ4D3Ab9q0twFnRnmhdtVHWb58ebeDExGRQKwji3uKu98D3APBoaEpU6YkG5CISBGJ82RxqOqjIiKSrESrj4qISPISrT5qZpPNbBswE7jbzF6OKx4REelY0tVHVxLXdYpFRCSUwh4OJyIi3aZEICJS4pQIRERKnBKBiEiJUyIQESlxSgQiIiUu6TLUA8zsB9nnX8hWIxURkV4UWyLIlqGeD3wEGAXMMrNR7Wa7CviTu78X+A/gq3HFIyIiHYtzj6C1DLW77wdaylC3dTHw3ez0YuBCM7MYYxIRkXaSLkPdOo+7HzCzncAQ4I9tZ2pbhhrYbWabYok4t6PbxyRaJzlovRxO6+RwSayTk3M9UXBlqJNgZqtyXdmnVGmddEzr5XBaJ4dL2zpJugx16zxm1hcYBOyIMSYREWkn6TLUGeBT2ekZwBMe10WURUSkQ7EdGsoe828pQ10G3NdShhpY5e4Z4F7ge2b2OvA2QbJIo8QOS6WY1knHtF4Op3VyuFStE9MGuIhIadPIYhGREqdEICJS4pQIOtFZmYxSY2b3mdl2M1ufdCxpYWYnmtmTZrbBzF42s+uSjilpZlZuZivMbG12ndyadExpYWZlZvaSmT2WdCwtlAjyCFkmo9Q8AFyUdBApcwD4nLuPAt4PzNH/CY3ABe4+FhgHXGRm7084prS4juA67qmhRJBfmDIZJcXdnybo4SVZ7v57d38xO11P8CV/T7JRJcsDu7PNftlbyfdMMbMTgI8C30k6lraUCPLrqExGSX/BJb9sBd3xwAvJRpK87CGQNcB24GfuXvLrBLgD+EegOelA2lIiEOkhZjYQeBS43t13JR1P0tz9oLuPI6gqcIaZjUk6piSZ2V8C2919ddKxtKdEkF+YMhkimFk/giTwsLv/MOl40sTd/ww8ic4tnQ3UmtkbBIeZLzCzh5INKaBEkF+YMhlS4rKl0+8FXnH3byQdTxqY2TFmNjg7fQTwIWBjslEly92/6O4nuHs1wW/JE+7+NwmHBSgR5OXuB4CWMhmvAI+4+8vJRpUsM1sAPAeMMLNtZnZV0jGlwNnA5QRbeGuyt2lJB5Ww44EnzWwdwQbVz9w9Nd0l5VAqMSEiUuK0RyAiUuKUCERESpwSgYhIiVMiEBEpcUoEIiIlTolASpaZHWdm3zezLWa22syeM7NLejmG28zsgx08PiVN1SmluMV2qUqRNMsOAlsCfNfdP5F97GSgtoN5+2bHlPQ4d78pjtcV6QrtEUipugDY7+7/3fKAu//K3e8EMLPZZpYxsyeAX5jZUWa2xMzWmdnzZnZ6dr5bzOzzLa9hZuvNrDp722hmD5vZK2a22Mwq2gdhZg+Y2Yzs9EXZZV4EPhbz3y/SSolAStVo4MVO5pkAzHD384FbgZfc/XTgS8CDId5jBPAtdz8N2AVcm2tGMysHvg38H2Ai8O4Qry/SI5QIRAAzm5+9mtbKNg//zN1brr1wDvA9AHd/AhhiZlWdvOxv3P1/stMPZV8jl5HAVnd/zYPh/qkoRialQYlAStXLBFv8ALj7HOBC4Jg28+wJ8ToHOPR7VN5mun39FtVzkVRSIpBS9QRQbmZ/3+axw47ht/EMcBkEPXqAP2avOfAG2YRiZhOAoW2WOcnMPpCd/gTwbJ7X3whUm9nwbHtWuD9DpPuUCKQkZQ+//BVwvpltNbMVwHeBf8qxyC3AxGw1za8An8o+/ihwlJm9TFCp9tU2y2wiuH7xK8C7gP/KE88+4NPAT7Ini7dH/dtEukrVR0VikL1k5WPuXtJX5ZLCoD0CEZESpz0CEZESpz0CEZESp0QgIlLilAhEREqcEoGISIlTIhARKXH/H0LaCbJk8YHdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdV4KHSkePdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ALTERNATIVE METHOD TO SHUFFLE THE RANKING FOR RANDOM\n",
        "#see https://gist.github.com/cadrev/6b91985a1660f26c2742\n",
        "business_id = 'WbJ1LRQdOuYYlRLyTkuuxw'\n",
        "df_rel_ranking = pd.read_csv('rel_ranking_' + business_id + '.csv')\n",
        "df_date_ranking = df_rel_ranking.sort_values(by=['Date']).reset_index(drop=True)\n",
        "df_date_ranking['Position'] = df_date_ranking.index + 1\n",
        "df_rand_ranking = df_rel_ranking\n",
        "\n",
        "#random.shuffle(df_rand_ranking) DOESN'T WORK KEY ERROR\n",
        "df_rand_ranking = df_rand_ranking.reindex(np.random.permutation(df_rand_ranking.index))\n",
        "\n",
        "df_rand_ranking = df_rand_ranking.reset_index(drop=True)\n",
        "\n",
        "# drop all the unnamed columns\n",
        "df_rel_ranking = drop_unnamed(df_rel_ranking)\n",
        "df_rand_ranking = drop_unnamed(df_rand_ranking)\n",
        "df_date_ranking = drop_unnamed(df_date_ranking)\n",
        "\n",
        "df_rand_ranking['Position'] = df_rand_ranking.index + 1\n",
        "\n",
        "print(df_rand_ranking)\n",
        "\n",
        "df_rel_ranking.to_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "df_date_ranking.to_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "df_rand_ranking.to_csv(\"rand_ranking_\" + business_id + \".csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40JdFbw0Qqmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FILTER RESTAURANTS\n",
        "is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "restaurants = business[is_restaurant]\n",
        "\n",
        "# Filter closed restaurants\n",
        "restaurants = restaurants[restaurants['is_open'] == 1]\n",
        "\n",
        "# Order restaurants by review_count\n",
        "restaurants = restaurants.sort_values('review_count')\n",
        "\n",
        "restaurants.to_csv('restaurants_input.csv')\n",
        "# NOW CHOOSE THE RESTAURANTS AND SAVE THEIR IDS INTO TXT FILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUr0D9xWp5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_size(df_users, attribute, range):\n",
        "    # order df_users by review_count\n",
        "    df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "    group_list = [[]]\n",
        "    i = 0\n",
        "    group_index = 0\n",
        "    prec = -1\n",
        "    for index, user in df_users.iterrows():\n",
        "        if i == range:\n",
        "            i = 0\n",
        "            group_list.append([])\n",
        "            group_index = group_index + 1\n",
        "        temp = user[attribute]\n",
        "        if temp != prec:\n",
        "            prec = temp\n",
        "            i = i + 1\n",
        "        group_list[group_index].append(user)\n",
        "    print(\"Number of groups created: \" + str(group_index+1))\n",
        "    return group_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYumnpYHecF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e54f518e-730b-47d2-c8dd-a165910cde70"
      },
      "source": [
        "### UNIQUE VALUE OF state, city AND categories OF BUSINESS\n",
        "# To filter only RESTAURANTS\n",
        "# is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "# restaurants = business[is_restaurant]\n",
        "\n",
        "print('UNIQUE VALUE OF \\'state\\' ATTRIBUTE IN RESTAURANTS = ', business['state'].nunique())\n",
        "print('UNIQUE VALUE OF \\'city\\' ATTRIBUTE IN RESTAURANTS = ', business['city'].nunique())\n",
        "df_cat = business[['categories']]\n",
        "distinct_categories_list = []\n",
        "for index, row in df_cat.iterrows():\n",
        "  if row['categories'] != None:\n",
        "    lst = [item.strip() for item in row['categories'].split(',')]\n",
        "    distinct_categories_list = list(set(distinct_categories_list + lst))\n",
        "print('UNIQUE VALUE OF \\'categories\\' ATTRIBUTE distinct values ', len(distinct_categories_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIQUE VALUE OF 'state' ATTRIBUTE IN RESTAURANTS =  36\n",
            "UNIQUE VALUE OF 'city' ATTRIBUTE IN RESTAURANTS =  1204\n",
            "UNIQUE VALUE OF 'categories' ATTRIBUTE distinct values  1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05AeyBn2gGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRINT THE state OF EACH user FOR HIS RESTAURANT reviews\n",
        "def statistics(users, reviews, business):\n",
        "  res = pd.DataFrame()\n",
        "  cont = 0\n",
        "  prec_user_id = '0'\n",
        "\n",
        "  for i, user in users.iterrows():\n",
        "    if cont == 1000:\n",
        "      break\n",
        "    user_id = user['user_id']  # 4, 9, 16, 21!, 22!, 23, 24\n",
        "    # Get all reviews of one user\n",
        "    # 'https://www.yelp.com/user_details_reviews_self?userid=NQffx45eJaeqhFcMadKUQA&rec_pagestart=90'??? no\n",
        "    user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "    # Get all restaurants\n",
        "    business_ids = user_reviews[['business_id']]\n",
        "    result = business_ids.merge(business)\n",
        "    is_restaurant = result['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "    user_restaurant_reviews = result[is_restaurant]\n",
        "    user_restaurant_reviews['user_id'] = user_id\n",
        "    #user_restaurant_reviews.to_csv('user_restaurant_reviews' + str(cont) + '.csv')\n",
        "    res = res.append(user_restaurant_reviews.groupby(['user_id', 'state'])['user_id'].count().reset_index(name=\"review_count\"))\n",
        "    if (user_id != prec_user_id) and not user_restaurant_reviews.empty:\n",
        "      cont = cont + 1\n",
        "    # print(res)\n",
        "    prec_user_id = user_id\n",
        "\n",
        "  res.to_csv('result.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0M0ETqjMrqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_df_users(ranking):\n",
        "  df_ranking = pd.DataFrame(ranking, columns=['position', 'user_id', 'review_count', 'date']).sort_values('position')\n",
        "  return df_ranking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6gUlnqBZ9eoa",
        "colab": {}
      },
      "source": [
        "# TODO: get review_count from web page\n",
        "def get_ranking(url):\n",
        "\n",
        "  i=1\n",
        "  reviews_text = []\n",
        "  reviews_date = []\n",
        "  reviews_userid = []\n",
        "  x=0\n",
        "\n",
        "  while 1:\n",
        "    if x == 0:\n",
        "      page_content = requests.get(url)\n",
        "    else:\n",
        "        page_content = requests.get(url + '?start=' + str(x))\n",
        "    x = x + 20\n",
        "    \n",
        "    tree = html.fromstring(page_content.content)\n",
        "    recommended_reviews_text = \"Recommended Reviews\"\n",
        "    reviews_list = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/*' % recommended_reviews_text)\n",
        "    if not reviews_list:\n",
        "      break\n",
        "    else:\n",
        "      # Index for ranking\n",
        "      j=1\n",
        "      for review in reviews_list:\n",
        "          reviews_text.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[p]/p/span' % (recommended_reviews_text, j))[0].text))\n",
        "          reviews_date.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[1]/div/div[2]/span' % (recommended_reviews_text, j))[0].text))\n",
        "          user_link = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div/div/div/div/div/div/a/@href' % (recommended_reviews_text, j))[0]\n",
        "          reviews_userid.append((i,user_link[user_link.find('=')+1:]))\n",
        "          i = i + 1\n",
        "          j = j + 1\n",
        "\n",
        "\n",
        "  print(reviews_text)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_text)))\n",
        "  print(reviews_date)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_date)))\n",
        "  print(reviews_userid)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_userid)))\n",
        "  return reviews_userid\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHl6bORqNzeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_users_in_dataset(users):\n",
        "    data = []\n",
        "    with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            data.append(json_data)\n",
        "    df = pd.DataFrame(data)\n",
        "    df_ranking = pd.DataFrame(users, columns=['Position', 'user_id'])\n",
        "    df_merged = df_ranking.merge(df, on='user_id')\n",
        "    # temp = df[df['user_id'].isin(users)]\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def exists_in_dataset(userid):\n",
        "  found = False\n",
        "  with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            if json_data['user_id'] == userid:\n",
        "              found = True\n",
        "  if found:\n",
        "    print(\"Trovato\")\n",
        "  else:\n",
        "    print(\"Non trovato\")\n",
        "  return found\n",
        "\n",
        "\n",
        "def find_users_in_chopped_dataset(users):\n",
        "    found = False\n",
        "    data = []\n",
        "    i = 1\n",
        "    j = 100000\n",
        "    df_users = pd.DataFrame()\n",
        "    while not found:\n",
        "        path = \"user\" + str(i) + \"-\" + str(j) + \".json\"\n",
        "        try:\n",
        "            with open(path, 'r') as file:\n",
        "                for line in file:\n",
        "                    json_data = json.loads(line)\n",
        "                    data.append(json_data)\n",
        "            df = pd.DataFrame(data)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Cannot find all users into the dataset\")\n",
        "            return 0\n",
        "        else:\n",
        "            temp = df[df['user_id'].isin(users)]  # df of users of the ranking\n",
        "            df_users = df_users.append(temp)\n",
        "            if df_users.shape[0] == len(users):\n",
        "                found = True\n",
        "            else:\n",
        "                i = i + 100000\n",
        "                j = j + 100000\n",
        "    return df_users\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}