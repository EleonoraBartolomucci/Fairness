{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exposure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP2foS3hRz4gwOBLK8nbnF6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/exposure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPpCkgv0271l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import datetime\n",
        "import math\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7PR_gGjwNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONSTANTS\n",
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyIm6NR55FGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrfK_M25rAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD user.json FROM DRIVE\n",
        "users_dataset_id = '1JokoV68YD5Iq2l4Y_IV2RJzBpD_mSCyq'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': users_dataset_id})\n",
        "download.GetContentFile('user.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMGOW-SHRrP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD business.json FROM DRIVE\n",
        "business_dataset_id = '1Qoy132gb205xAIFjkBbZ2CyYDeaz9yqU'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': business_dataset_id})\n",
        "download.GetContentFile('business.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tkQRFfw16x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD review.json FROM DRIVE\n",
        "review_dataset_id = '1mW1WbpMFjN0qQpLnM-R_TzNpAkFsBLHQ'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': review_dataset_id})\n",
        "download.GetContentFile('review.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcwnWi7G4w5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "users = read_json('user.json')\n",
        "users = pd.DataFrame(users)\n",
        "business = read_json('business.json')\n",
        "business = pd.DataFrame(business)\n",
        "reviews = read_json('review.json')\n",
        "reviews = pd.DataFrame(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40JdFbw0Qqmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter restaurants\n",
        "is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "restaurants = business[is_restaurant]\n",
        "\n",
        "# Filter closed restaurants\n",
        "restaurants = restaurants[restaurants['is_open'] == 1]\n",
        "\n",
        "# Order restaurants by review_count\n",
        "restaurants = restaurants.sort_values('review_count')\n",
        "\n",
        "restaurants.to_csv('restaurants_input.csv')\n",
        "# NOW CHOOSE THE RESTAURANTS AND SAVE THEIR IDS INTO TXT FILE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYSaLtPHqsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXEC IN LOCAL THE DOWNLOAD OF REVIEW RANKINGS OF EACH RESTAURANT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW2AKZ8lHgoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attribute = 'Review_Count'\n",
        "df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random = pipeline1()\n",
        "group_list = pipeline2(attribute, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random)\n",
        "pipeline3(attribute, group_list, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05AeyBn2gGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def statistics(users, reviews, business):\n",
        "  res = pd.DataFrame()\n",
        "  cont = 0\n",
        "  prec_user_id = '0'\n",
        "\n",
        "  for i, user in users.iterrows():\n",
        "    if cont == 1000:\n",
        "      break\n",
        "    user_id = user['user_id']  # 4, 9, 16, 21!, 22!, 23, 24\n",
        "    # Get all reviews of one user\n",
        "    # 'https://www.yelp.com/user_details_reviews_self?userid=NQffx45eJaeqhFcMadKUQA&rec_pagestart=90'??? no\n",
        "    user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "    # Get all restaurants\n",
        "    business_ids = user_reviews[['business_id']]\n",
        "    result = business_ids.merge(business)\n",
        "    is_restaurant = result['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "    user_restaurant_reviews = result[is_restaurant]\n",
        "    user_restaurant_reviews['user_id'] = user_id\n",
        "    #user_restaurant_reviews.to_csv('user_restaurant_reviews' + str(cont) + '.csv')\n",
        "    res = res.append(user_restaurant_reviews.groupby(['user_id', 'state'])['user_id'].count().reset_index(name=\"review_count\"))\n",
        "    if (user_id != prec_user_id) and not user_restaurant_reviews.empty:\n",
        "      cont = cont + 1\n",
        "    # print(res)\n",
        "    prec_user_id = user_id\n",
        "\n",
        "  res.to_csv('result.csv')  # PRINT THE state OF EACH user FOR HIS RESTAURANT reviews\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYumnpYHecF",
        "colab_type": "code",
        "outputId": "e54f518e-730b-47d2-c8dd-a165910cde70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "### UNIQUE VALUE OF state, city AND categories OF BUSINESS\n",
        "# To filter only RESTAURANTS\n",
        "# is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "# restaurants = business[is_restaurant]\n",
        "\n",
        "print('UNIQUE VALUE OF \\'state\\' ATTRIBUTE IN RESTAURANTS = ', business['state'].nunique())\n",
        "print('UNIQUE VALUE OF \\'city\\' ATTRIBUTE IN RESTAURANTS = ', business['city'].nunique())\n",
        "df_cat = business[['categories']]\n",
        "distinct_categories_list = []\n",
        "for index, row in df_cat.iterrows():\n",
        "  if row['categories'] != None:\n",
        "    lst = [item.strip() for item in row['categories'].split(',')]\n",
        "    distinct_categories_list = list(set(distinct_categories_list + lst))\n",
        "print('UNIQUE VALUE OF \\'categories\\' ATTRIBUTE distinct values ', len(distinct_categories_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIQUE VALUE OF 'state' ATTRIBUTE IN RESTAURANTS =  36\n",
            "UNIQUE VALUE OF 'city' ATTRIBUTE IN RESTAURANTS =  1204\n",
            "UNIQUE VALUE OF 'categories' ATTRIBUTE distinct values  1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqp91T-ezPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def divide_business_by_review_count():\n",
        "  data = read_json('business.json')\n",
        "  df = pd.DataFrame(data)\n",
        "  max = df['review_count'].max()\n",
        "  min = df['review_count'].min()\n",
        "  print('max is = ', max)\n",
        "  print('min is = ', min)\n",
        "  median = (max-min)/2  # 4172,5\n",
        "  business_with_few_review_count = df[df['review_count']<median]\n",
        "\n",
        "  # print(business_with_few_review_count)\n",
        "\n",
        "  business_with_many_review_count = df[df['review_count']>=median]\n",
        "\n",
        "  # print(business_with_many_review_count)\n",
        "  business_with_few_review_count.to_csv('few_review_business.csv')\n",
        "  business_with_many_review_count.to_csv(r'many_review_business.csv')\n",
        "  return (business_with_few_review_count, business_with_many_review_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irlisv9418xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ranking_from_call(url_business, lang, sort, query):\n",
        "    headers = [{\"name\": \"Accept\", \"value\": \"*/*\"}, {\"name\": \"Accept-Encoding\", \"value\": \"gzip, deflate, br\"},\n",
        "               {\"name\": \"Accept-Language\", \"value\": \"it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3\"},\n",
        "               {\"name\": \"Connection\", \"value\": \"keep-alive\"},\n",
        "               {\"name\": \"Content-Type\", \"value\": \"application/x-www-form-urlencoded; charset=utf-8\"}, {\"name\": \"Cookie\",\n",
        "                                                                                                       \"value\": \"qntcst=D; hl=en_US; wdi=1|3C26116D69138F61|0x1.78d019f71a444p+30|a7756ff94751d3a9; _ga=GA1.2.3C26116D69138F61; location=%7B%22city%22%3A+%22New+York%22%2C+%22state%22%3A+%22NY%22%2C+%22country%22%3A+%22US%22%2C+%22latitude%22%3A+40.713%2C+%22longitude%22%3A+-74.0072%2C+%22max_latitude%22%3A+40.8523%2C+%22min_latitude%22%3A+40.5597%2C+%22max_longitude%22%3A+-73.7938%2C+%22min_longitude%22%3A+-74.1948%2C+%22zip%22%3A+%22%22%2C+%22address1%22%3A+%22%22%2C+%22address2%22%3A+%22%22%2C+%22address3%22%3A+null%2C+%22neighborhood%22%3A+null%2C+%22borough%22%3A+null%2C+%22provenance%22%3A+%22YELP_GEOCODING_ENGINE%22%2C+%22display%22%3A+%22New+York%2C+NY%22%2C+%22unformatted%22%3A+%22New+York%2C+NY%2C+US%22%2C+%22accuracy%22%3A+4.0%2C+%22language%22%3A+null%7D; xcj=1|Ptt9P03gfc75x_PBT9zmqCkUuSuyB7PR-wWUBvABNi4; __qca=P0-60561249-1581956668708; G_ENABLED_IDPS=google; __cfduid=db8764ff59d8028a6c2e1b214867927d81583160194; _gid=GA1.2.2014867238.1583835527; bse=05dcd9d5de304ef0b1d9a76fa768b10f; sc=8a1ca0dbc2; pid=505721aa4569e7bb\"},\n",
        "               {\"name\": \"Host\", \"value\": \"www.yelp.com\"},\n",
        "               {\"name\": \"Referer\", \"value\": \"https://www.yelp.com/biz/noche-de-margaritas-new-york\"},\n",
        "               {\"name\": \"TE\", \"value\": \"Trailers\"}, {\"name\": \"User-Agent\",\n",
        "                                                     \"value\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0\"},\n",
        "               {\"name\": \"X-Requested-By-React\", \"value\": \"true\"},\n",
        "               {\"name\": \"X-Requested-With\", \"value\": \"XMLHttpRequest\"}]\n",
        "    headers_ok = {}\n",
        "    for header in headers:\n",
        "        temp = {\n",
        "            header['name']: header['value']\n",
        "        }\n",
        "        headers_ok.update(temp)\n",
        "\n",
        "    x = 0\n",
        "    reviews_list = []\n",
        "    position = 1\n",
        "    url = url_business + \"/review_feed?rl=\" + lang + \"&sort_by=\" + sort + \"&q=\" + query\n",
        "\n",
        "    while 1:\n",
        "        if x == 0:\n",
        "            page_load = requests.get(url + '&start=', headers=headers_ok)\n",
        "        else:\n",
        "            page_load = requests.get(url + '&start=' + str(x), headers=headers_ok)\n",
        "        # print(page_load)\n",
        "        x = x + 20\n",
        "        reviews = page_load.json()['reviews']\n",
        "        # print(json.dumps(reviews, indent=4, sort_keys=True))\n",
        "        if not reviews:\n",
        "            break\n",
        "        for review in reviews:\n",
        "            reviews_list.append((position, review['userId'], review['user']['reviewCount'],\n",
        "                                 datetime.datetime.strptime(review['localizedDate'], '%m/%d/%Y')))\n",
        "            position = position + 1\n",
        "    return reviews_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbM06dpOYYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################# EXECUTE IN LOCAL (if not blocked by yelp) #############################################\n",
        "def retrieve_rankings(business_id):\n",
        "    rel_ranking = get_ranking_from_call(\"https://www.yelp.com/biz/\" + business_id, \"en\", \"relevance_desc\", \"\")\n",
        "    date_ranking = sorted(rel_ranking, key=lambda tup: tup[3], reverse=True)\n",
        "    random_ranking = random.sample(rel_ranking, len(rel_ranking))\n",
        "\n",
        "    print(rel_ranking)\n",
        "    print(date_ranking)\n",
        "    print(random_ranking)\n",
        "\n",
        "    with open(\"ranking_rel_\" + business_id + \".txt\", \"wb\") as fp:   # Pickling\n",
        "        pickle.dump(rel_ranking, fp)\n",
        "    with open(\"ranking_date_\" + business_id + \".txt\", \"wb\") as fp:  # Pickling\n",
        "        pickle.dump(date_ranking, fp)\n",
        "    with open(\"ranking_rand_\" + business_id + \".txt\", \"wb\") as fp:  # Pickling\n",
        "        pickle.dump(random_ranking, fp)\n",
        "\n",
        "    # TODO: if executed in local, then UPLOAD ranking_by_relevance.txt ON COLAB\n",
        "\n",
        "\n",
        "rest_ids = []\n",
        "\n",
        "with open('rest_ids.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    print(line)\n",
        "    rest_ids.append(line)\n",
        "\n",
        "for id in rest_ids:\n",
        "    retrieve_rankings(id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3pgmnXU9cSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_rankings(business_id):\n",
        "  # Business URL: 'https://www.yelp.com/biz/noche-de-margaritas-new-york'\n",
        "  download_ranking_by_relevance(business_id)\n",
        "  download_ranking_by_date(business_id)\n",
        "  download_ranking_by_random(business_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX8ibEsAIxCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline1():\n",
        "  ### READ THE RANKING FROM FILE\n",
        "  with open(\"ranking_by_relevance.txt\", \"rb\") as fp:   # Unpickling\n",
        "    ranking_by_relevance = pickle.load(fp)\n",
        "  ###\n",
        "  ### READ THE RANKING FROM FILE\n",
        "  with open(\"ranking_by_date.txt\", \"rb\") as fp:   # Unpickling\n",
        "    ranking_by_date = pickle.load(fp)\n",
        "  ###\n",
        "  ### READ THE RANKING FROM FILE\n",
        "  with open(\"ranking_by_random.txt\", \"rb\") as fp:   # Unpickling\n",
        "    ranking_by_random = pickle.load(fp)\n",
        "  ###\n",
        "\n",
        "  # df_ranking = find_users_in_dataset(ranking)  # cannot find all users\n",
        "\n",
        "  df_ranking_by_relevance = create_df_users(ranking_by_relevance)\n",
        "  df_ranking_by_date = create_df_users(ranking_by_date)\n",
        "  df_ranking_by_random = create_df_users(ranking_by_random)\n",
        "  \n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  print(df_ranking_by_relevance)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_ranking_by_date)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_ranking_by_random)\n",
        "  print('\\n')\n",
        "\n",
        "  return df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random\n",
        "\n",
        "\n",
        "def pipeline2(attribute, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "  print(\"ATTRIBUTE: \" + attribute + \"\\n\")\n",
        "  group_list = create_objective_groups_by_percentile(df_ranking_by_relevance, attribute, 10) # number of groups to be created\n",
        "  # group_list = create_objective_groups_by_size(df_ranking_by_relevance, attribute, 10)\n",
        "  index = 1\n",
        "  for group in group_list:\n",
        "      print(\"\\nGROUP \" + str(index) + \":\")\n",
        "      for person in group:\n",
        "          print(person['user_id'] + \", \" + str(person['Review_Count']))\n",
        "      index = index + 1\n",
        "  return group_list\n",
        "\n",
        "\n",
        "def pipeline3(attribute, group_list, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random):\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\")\n",
        "  index = 0\n",
        "  yelp_exposures = []\n",
        "  for group in group_list:\n",
        "      current_exp = exposure(group, df_ranking_by_relevance)\n",
        "      yelp_exposures.append((index, current_exp))\n",
        "      print(\"\\nEXPOSURE of group \" + str(index) + \" is: \" + str(current_exp))\n",
        "      index = index + 1\n",
        "  print('\\n')\n",
        "  print(\"------------Ranking by Date:------------\")\n",
        "  index = 0\n",
        "  date_exposures = []\n",
        "  for group in group_list:\n",
        "      current_exp = exposure(group, df_ranking_by_date)\n",
        "      date_exposures.append((index, current_exp))\n",
        "      print(\"\\nEXPOSURE of group \" + str(index) + \" is: \" + str(current_exp))\n",
        "      index = index + 1\n",
        "  print('\\n')\n",
        "  print(\"------------Ranking Random:------------\")\n",
        "  index = 0\n",
        "  random_exposures = []\n",
        "  for group in group_list:\n",
        "      current_exp = exposure(group, df_ranking_by_random)\n",
        "      random_exposures.append((index, current_exp))\n",
        "      print(\"\\nEXPOSURE of group \" + str(index) + \" is: \" + str(current_exp))\n",
        "      index = index + 1\n",
        "  print('\\n')\n",
        "\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, 'plot_'+attribute+'_'+business_id)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPGOC05EljLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_plots(yelp_exposures, date_exposures, random_exposures, title):\n",
        "  width = 0.27\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5\n",
        "\n",
        "  x1 = [el[0] - width for el in yelp_exposures]\n",
        "  x2 = [el[0] for el in date_exposures]\n",
        "  x3 = [el[0] + width for el in random_exposures]\n",
        "\n",
        "  y1 = [el[1] for el in yelp_exposures]\n",
        "  y2 = [el[1] for el in date_exposures]\n",
        "  y3 = [el[1] for el in random_exposures]\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='yelp')\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date')\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0))\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0M0ETqjMrqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_df_users(ranking):\n",
        "  df_ranking = pd.DataFrame(ranking, columns=['Position', 'user_id', 'Review_Count']).sort_values('Position')\n",
        "  return df_ranking"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awAE4AWWxEFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_percentile(df_users, attribute, N_of_groups):\n",
        "  # order df_users by review_count\n",
        "  df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "  group_list = []\n",
        "  percentile_up_value = math.ceil(100/N_of_groups)\n",
        "  percentile_down_value = int(100/N_of_groups)\n",
        "  initial_percentile_value = percentile_down_value\n",
        "  array_of_values = df_users[attribute].tolist()\n",
        "  index = 0\n",
        "  prec_max_value = -1\n",
        "  while percentile_up_value <= 100 or percentile_down_value <= 100:\n",
        "    group_list.append([])\n",
        "    if percentile_up_value > 100:\n",
        "      percentile_up_value = 100\n",
        "    max_value_in_group = np.percentile(array_of_values, percentile_up_value)\n",
        "    for i, user in df_users.iterrows():\n",
        "      if user[attribute] <= max_value_in_group and user[attribute] > prec_max_value:\n",
        "        group_list[index].append(user)\n",
        "    index = index + 1\n",
        "    percentile_up_value = percentile_up_value + initial_percentile_value\n",
        "    percentile_down_value = percentile_down_value + initial_percentile_value\n",
        "    prec_max_value = max_value_in_group\n",
        "\n",
        "  return group_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbpLTiPSWuL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exposure(group, ranking):\n",
        "    return sommatory(group, ranking)/(len(group))\n",
        "\n",
        "\n",
        "def sommatory(group, ranking):  # ranking is a list of user_id\n",
        "    sum = 0\n",
        "    for user in group:\n",
        "        user_id = user['user_id']\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'Position'].tolist()[0]\n",
        "        # position = ranking.index[ranking['user_id'] == user_id].tolist()[0]\n",
        "        # position = position + 1\n",
        "        sum = sum + exp(position)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/np.log(1 + position)\n",
        "\n",
        "\n",
        "def demographic_parity(g1, g2, s):\n",
        "    ex1 = exposure(g1, s)\n",
        "    ex2 = exposure(g2, s)\n",
        "    print(\"|\" + str(ex1) + \" - \" + str(ex2) + \"| < 0.1?\")\n",
        "    return np.absolute(ex1 - ex2) < 0.1  # in our case, g1 and g2 are groups of author (reviewer)\n",
        "\n",
        "\n",
        "def app(author, df):\n",
        "    return df[df['UserId'] == author].iloc[0, -1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JgvcYwsfpy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# READ JSON\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUr0D9xWp5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_size(df_users, attribute, range):\n",
        "    # order df_users by review_count\n",
        "    df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "    group_list = [[]]\n",
        "    i = 0\n",
        "    group_index = 0\n",
        "    prec = -1\n",
        "    for index, user in df_users.iterrows():\n",
        "        if i == range:\n",
        "            i = 0\n",
        "            group_list.append([])\n",
        "            group_index = group_index + 1\n",
        "        temp = user[attribute]\n",
        "        if temp != prec:\n",
        "            prec = temp\n",
        "            i = i + 1\n",
        "        group_list[group_index].append(user)\n",
        "    print(\"Number of groups created: \" + str(group_index+1))\n",
        "    return group_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6gUlnqBZ9eoa",
        "colab": {}
      },
      "source": [
        "# TODO: get review_count from web page\n",
        "def get_ranking(url):\n",
        "\n",
        "  i=1\n",
        "  reviews_text = []\n",
        "  reviews_date = []\n",
        "  reviews_userid = []\n",
        "  x=0\n",
        "\n",
        "  while 1:\n",
        "    if x == 0:\n",
        "      page_content = requests.get(url)\n",
        "    else:\n",
        "        page_content = requests.get(url + '?start=' + str(x))\n",
        "    x = x + 20\n",
        "    \n",
        "    tree = html.fromstring(page_content.content)\n",
        "    recommended_reviews_text = \"Recommended Reviews\"\n",
        "    reviews_list = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/*' % recommended_reviews_text)\n",
        "    if not reviews_list:\n",
        "      break\n",
        "    else:\n",
        "      # Index for ranking\n",
        "      j=1\n",
        "      for review in reviews_list:\n",
        "          reviews_text.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[p]/p/span' % (recommended_reviews_text, j))[0].text))\n",
        "          reviews_date.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[1]/div/div[2]/span' % (recommended_reviews_text, j))[0].text))\n",
        "          user_link = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div/div/div/div/div/div/a/@href' % (recommended_reviews_text, j))[0]\n",
        "          reviews_userid.append((i,user_link[user_link.find('=')+1:]))\n",
        "          i = i + 1\n",
        "          j = j + 1\n",
        "\n",
        "\n",
        "  print(reviews_text)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_text)))\n",
        "  print(reviews_date)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_date)))\n",
        "  print(reviews_userid)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_userid)))\n",
        "  return reviews_userid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHl6bORqNzeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_users_in_dataset(users):\n",
        "    data = []\n",
        "    with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            data.append(json_data)\n",
        "    df = pd.DataFrame(data)\n",
        "    df_ranking = pd.DataFrame(users, columns=['Position', 'user_id'])\n",
        "    df_merged = df_ranking.merge(df, on='user_id')\n",
        "    # temp = df[df['user_id'].isin(users)]\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def exists_in_dataset(userid):\n",
        "  found = False\n",
        "  with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            if json_data['user_id'] == userid:\n",
        "              found = True\n",
        "  if found:\n",
        "    print(\"Trovato\")\n",
        "  else:\n",
        "    print(\"Non trovato\")\n",
        "  return found\n",
        "\n",
        "\n",
        "def find_users_in_chopped_dataset(users):\n",
        "    found = False\n",
        "    data = []\n",
        "    i = 1\n",
        "    j = 100000\n",
        "    df_users = pd.DataFrame()\n",
        "    while not found:\n",
        "        path = \"user\" + str(i) + \"-\" + str(j) + \".json\"\n",
        "        try:\n",
        "            with open(path, 'r') as file:\n",
        "                for line in file:\n",
        "                    json_data = json.loads(line)\n",
        "                    data.append(json_data)\n",
        "            df = pd.DataFrame(data)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Cannot find all users into the dataset\")\n",
        "            return 0\n",
        "        else:\n",
        "            temp = df[df['user_id'].isin(users)]  # df of users of the ranking\n",
        "            df_users = df_users.append(temp)\n",
        "            if df_users.shape[0] == len(users):\n",
        "                found = True\n",
        "            else:\n",
        "                i = i + 100000\n",
        "                j = j + 100000\n",
        "    return df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}