{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exposure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/exposure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPpCkgv0271l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import datetime\n",
        "import math\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#!pip install balanced_kmeans\n",
        "#from balanced_kmeans import kmeans\n",
        "#from balanced_kmeans import kmeans_equal\n",
        "\n",
        "import networkx as nx\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkoXGrGIEQB3",
        "colab_type": "code",
        "outputId": "52c07dbf-9541-45a7-9f3d-2deb9748e833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COSfvffRFRRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "root_path = 'gdrive/My Drive/Tesi/Fairness/data/fairness_data/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7PR_gGjwNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONSTANTS\n",
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n",
        "FOLDER = {'WbJ1LRQdOuYYlRLyTkuuxw':\n",
        "            {'FID_RANKING': '1U0Ml5puObfOk3qHCKhO9bQ06WyhJoffW',\n",
        "             'FID_G1': '1cauvqKXzF5VhiveP84aL3U0oDrXP32XV',\n",
        "             'FID_G2': '1vC1fc2owpCis_PTl1ABKT6BfG6-2yoiJ',\n",
        "             'FID_G3': '1_iKbmJkBwWEEIIh73UvR431Ss6xnGb2f',\n",
        "             'FID_G4': '1m_feamMvpx5B5mTe_lH6qlhoZFEpwlSs',\n",
        "             'FID_G5': '1qJgFbR4FUX5t7O9h5cHgp6aE6lpfbdOs',\n",
        "             'FID_G6': '1BSQrghVusO_eNbe10fFE4-bVfRK8ly0y',\n",
        "             'FID_G7': '1fJD4WIErH-u5plesyTePCAQ3IyfjoOKf',\n",
        "             'FID_G8': '1fD_fdl-1KAzXCjPOsEvFOZS3u354XhLL',\n",
        "             'FID_G9': '1ZdiaS2cZP9Ks3rheSSVaoLpw-qMYF9EA',\n",
        "             'FID_G10': '',\n",
        "             'FID_G11': '1ce7VQPdbQ9ieimmpz66RaHgbzEFa4TsT'\n",
        "             },\n",
        "          'T2tEMLpTeSMxLKpxwFdS3g':\n",
        "            {'FID_RANKING': '136FZ0Y90Zx-4UYDyW50cX8zUa_DhN3XS',\n",
        "             'FID_G1': '1eiju1aTzsRi4QQZ8XrSouqcLtFSN-tbj',\n",
        "             'FID_G2': '1VuU8MAcXgN_E85q9SH_P1HsHVkeJakwH',\n",
        "             'FID_G3': '1L240ycUElvmF0R4l8PbnuoBbqQY6q1og',\n",
        "             'FID_G4': '1S4oTf-ZSD8q9aTOjrDEA2KeNJ99uh-2n',\n",
        "             'FID_G5': '1u9ZFeCwmol1uXRH-l8djJ2hst0B01VI3',\n",
        "             'FID_G6': '19TgwV3uHOtDDJ0a8krpvLUeooe9rhQFJ',\n",
        "             'FID_G7': '11YpFHw34cDglPD87W8yWvRM1VAOWXPtF',\n",
        "             'FID_G8': '1tR-GotUUNgiilJLBxvr7PkUpbPG5F_J7',\n",
        "             },\n",
        "          'ALwAlxItASeEs2vYAeLXHA':\n",
        "            {'FID_RANKING': '12zXi3XyQaNgukGHW_805cyrkHhSdF7Df',\n",
        "             'FID_G1': '1NB-isOm1cDArAlwIMDMou1Q_XAap09KA',\n",
        "             'FID_G2': '1ELMUBbKGryblHDnbh7ghiIgMdZ48pBG1',\n",
        "             'FID_G3': '1QZ90pwDjBoaU41P8wP8v_I9tsuhObEUP',\n",
        "             'FID_G4': '1MXivbV9VGB0vBGGuLhYu0zJlpv6O-2Xb',\n",
        "             'FID_G5': '1bTetFx3Jhy5QbRKYGV09muMy7D1NKmJT',\n",
        "             'FID_G6': '1F_7bdMS5MW4sIpyUbDz62ZTYJtHoouGG',\n",
        "             'FID_G7': '1uRFceNIlYk5vQXEGd2H6yanrlwrSIM9V',\n",
        "             },\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw':\n",
        "            {'FID_RANKING': '1ZyNQavpG0akr3ca3PJjjl_D89IA5wlcc',\n",
        "             'FID_G1': '1kQkAi_9V3mSld1LlP50j6uw0bwqnkLrR',\n",
        "             'FID_G2': '1hGR4eZiP3Q9Etn7RdlY58IsNsTMMTydz',\n",
        "             'FID_G3': '1YcAPHxioFpiVXWndO5tK17US1h36cWgK',\n",
        "             'FID_G4': '1NxzzH0l18fGaO4bZ35D9NMq2CeW5BRVP',\n",
        "             'FID_G5': '1HC1wZGPtbb8UvRlfbGoHl_ijl3G67Ei4',\n",
        "             'FID_G6': '1J_15pO-CFbHR_0YsHK5kDnKp49kDL0kZ',\n",
        "             'FID_G7': '1RTwCqhYzB3djzpe3-uwbQ3Ur5P8YoECa',\n",
        "             },\n",
        "          'Sovgwq-E-n6wLqNh3X_rXg':\n",
        "            {'FID_RANKING': '1EZqvt9x5PN07BgUad1RtNIUXTVqujA0g',\n",
        "             'FID_G1': '1pkUEBYeZ66GfIvXoTkzZ28gGFlZNWmEq',\n",
        "             'FID_G2': '1lJzHAfBlvL9_EgNizH4vnyPTn-j-Vbrx',\n",
        "             'FID_G3': '1NRYqjUsQJTwgB0JXtcCx7lQErirMH1TK',\n",
        "             'FID_G4': '1nLtZbS0NGXKAs6aETNs7EWasNs8omaw6',\n",
        "             'FID_G5': '14w-jVhOuod3s_EYSipvS92SPx_3zT_FD',\n",
        "             'FID_G6': '1tbUyiamFmyAoeA3W6j51x97h3MjUSVsd',\n",
        "             'FID_G7': '1TNAmibHwJHkvNajHhRffN1gEY0-QnBZH',\n",
        "             },\n",
        "          'j5nPiTwWEFr-VsePew7Sjg':\n",
        "            {'FID_RANKING': '18bQVXYZ03vIpfEFLPh8I2i7cPTlokGxv',\n",
        "             'FID_G1': '1U8STZ7irZLPcUpP1ALR6QYIFnoXB2tbE',\n",
        "             'FID_G2': '1xScKc0_DlnQZucHLeb26hLqpgjqMA3RH',\n",
        "             'FID_G3': '1JFYl2eKtDyWbsyVdBpTXsvPn2h6MO3Cr',\n",
        "             'FID_G4': '1aFYpJmymdnHxquuqbbMwXzN5wG-3Zf2O',\n",
        "             'FID_G5': '1NfHlsBTk87jscmZfagtnd--kcdpy8t_8',\n",
        "             'FID_G6': '1b88afNqrZSRptYA0pL3lCEV49mFaEP6Q',\n",
        "             'FID_G7': '15rDbY-MTUBVjHxOedbenZIjTxQ_wEAZW',\n",
        "             },\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg':\n",
        "            {'FID_RANKING': '1nl6A997UnuR5ceYRZS1242JJCvK',\n",
        "             'FID_G1': '1hrssLlhPmnS48yVxXpwBBIuhL1YdDNhj',\n",
        "             'FID_G2': '1shLPc3aUsVaxeZhZ_SvDS3agPFlMw1dn',\n",
        "             'FID_G3': '13eUtYFVp-KKjxV6-mhCsxQGHkKFOgB82',\n",
        "             'FID_G4': '1nQVPHZT4sHpBXD32qX_x41VywkmyppFj',\n",
        "             'FID_G5': '1gJpfGhb721Q64gynq4jSGMis5pDA7qAx',\n",
        "             'FID_G6': '1WbZvolaSwambWOKHx4v4eFIV-KUzygGF',\n",
        "             'FID_G7': '1IWyQpgMjLny2JTbw0nUuHkNRPV5CCA_A',\n",
        "             },\n",
        "          'e4NQLZynhSmvwl38hC4m-A':\n",
        "            {'FID_RANKING': '1K566Y5Q2N6Lw7S6yDicKC_R-zFVRqnFr',\n",
        "             'FID_G1': '1RNxub2faGfc4NAFvau5SsMADAzBlQd2j',\n",
        "             'FID_G2': '16tjv4k5CJSOxwgvC0wDJU67MEjRaw_0o',\n",
        "             'FID_G3': '1p-9HrVncD_FZOxO0YzHiY7KTe47ugSTQ',\n",
        "             'FID_G4': '1qGpECepAyJjqyfPx8Q94odPOCHbOixtZ',\n",
        "             'FID_G5': '15AGLE1YT0v8tQ3r6i6k7j5XaNORToUct',\n",
        "             'FID_G6': '1nNbAPulqJCzO6urHNsqc4qzSM7Yih8uk',\n",
        "             'FID_G7': '1o9Ew39-Q9VO3wjEyv4qGJ9YlfRmkzTed',\n",
        "             },\n",
        "          'S-oLPRdhlyL5HAknBKTUcQ':\n",
        "            {'FID_RANKING': '1FJqfJgaJinXS3oIvdA53FMuO9O7bCDVn',\n",
        "             'FID_G1': '1CcLHVEA41AgSzJKl37YlcJKf-o5NaCUo',\n",
        "             'FID_G2': '1j-lp1gcu6YGTwcOV4aYiY7dh2G9_TL6J',\n",
        "             'FID_G3': '1I-Qn9oz4VopXbpj5i8_iqLvcSp8jpb2_',\n",
        "             'FID_G4': '1gsS9uFbRyI18ifF5yLp20WHhP66gdr57',\n",
        "             'FID_G5': '1LJIVm9tCZMcgzBkGilYem0nqKoUHpdpX',\n",
        "             'FID_G6': '1syB0wF48fiSo0_uW9dedS-ZlFfXUv1B8',\n",
        "             'FID_G7': '1cBPjBprJ19RxI6-Bcpb-Fifv0s1bQSV2',\n",
        "             }\n",
        "        }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bw0Twhc3qgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upload_file(filename, folder_id):\n",
        "  drive = authenticate()\n",
        "  fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "  drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Check if file already exists in Google Drive (prevents duplicates)\n",
        "  for file in fileList:\n",
        "      if file['title'] == filename:  # The file already exists, then overwrite it\n",
        "          fileID = file['id']\n",
        "          drive_file = drive.CreateFile({'id': fileID, 'title': filename, 'parents': [{'id': folder_id}]})\n",
        "\n",
        "  # Create a local copy of user picture\n",
        "  # Already created!\n",
        "\n",
        "  # Upload user picture on Google Drive\n",
        "  drive_file.SetContentFile(filename)  # path of local file content\n",
        "  drive_file.Upload()  # Upload the file.\n",
        "  \n",
        "  # Delete local user pictures\n",
        "  #if os.path.exists(filename):\n",
        "  #    os.remove(filename)\n",
        "  #else:\n",
        "  #    print(\"The file does not exist\")\n",
        "\n",
        "def set_file_destination(lst, method, id):\n",
        "  if lst == ['review_count']:\n",
        "    return FOLDER[id]['FID_G1']\n",
        "  if lst == ['fans']:\n",
        "      return FOLDER[id]['FID_G6']\n",
        "  if method == 'kmeans':\n",
        "    if lst == ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']:\n",
        "      return FOLDER[id]['FID_G2']\n",
        "    if lst == ['loc1', 'loc2', 'loc3']:\n",
        "      return FOLDER[id]['FID_G3']\n",
        "    if lst == ['age', 'gender', 'ethnicity']: \n",
        "      return FOLDER[id]['FID_G4']\n",
        "    if lst == ['review_sentiment']:\n",
        "      return FOLDER[id]['FID_G5']\n",
        "    if lst == ['useful','funny', 'cool']:\n",
        "      return FOLDER[id]['FID_G7']\n",
        "  if method == 'balanced_kmeans':\n",
        "    if lst == ['age', 'gender', 'ethnicity']:\n",
        "      return FOLDER[id]['FID_G8']\n",
        "    if lst == ['loc1', 'loc2', 'loc3']:\n",
        "      return FOLDER[id]['FID_G9']\n",
        "  if method == 'custom':\n",
        "    return FOLDER[id]['FID_G11']\n",
        "\n",
        "\n",
        "\n",
        "# READ JSON\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "  cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "  return df[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyIm6NR55FGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "drive = authenticate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrfK_M25rAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD user.json FROM DRIVE\n",
        "users_dataset_id = '1JokoV68YD5Iq2l4Y_IV2RJzBpD_mSCyq'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': users_dataset_id})\n",
        "download.GetContentFile('user.json')\n",
        "users = read_json('user.json')\n",
        "users = pd.DataFrame(users)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMGOW-SHRrP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD business.json FROM DRIVE\n",
        "business_dataset_id = '1Qoy132gb205xAIFjkBbZ2CyYDeaz9yqU'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': business_dataset_id})\n",
        "download.GetContentFile('business.json')\n",
        "business = read_json('business.json')\n",
        "business = pd.DataFrame(business)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tkQRFfw16x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD review.json FROM DRIVE\n",
        "review_dataset_id = '1mW1WbpMFjN0qQpLnM-R_TzNpAkFsBLHQ'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': review_dataset_id})\n",
        "download.GetContentFile('review.json')\n",
        "reviews = read_json('review.json')\n",
        "reviews = pd.DataFrame(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYF2ESayjKVg",
        "colab_type": "code",
        "outputId": "c4b81362-9206-4c33-802b-8f57da761c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# DOWNLOAD DEMOGRAPHICS\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A', 'S-oLPRdhlyL5HAknBKTUcQ']\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "file_id_list = {'WbJ1LRQdOuYYlRLyTkuuxw':['1uuA0QH-DJklvtiekxS-7yaXfKRi9ux06'],\n",
        "#                                          '13amOFvuku27snF8mea8Yp7rH36Mve42N',\n",
        "#                                          '1VskD_0Ijwe3_fzVbgYqK9Bk-YOCK046t',\n",
        "#                                          '1uuA0QH-DJklvtiekxS-7yaXfKRi9ux06'],\n",
        "                'T2tEMLpTeSMxLKpxwFdS3g':['102OYWJOKF54GMnFh4_AKc5e_Mz7xiXZt'],\n",
        "                                          #'11fM_aHYkCLQ2z3g7Z76aZZ3cAsIrpezN',\n",
        "                                          #'102OYWJOKF54GMnFh4_AKc5e_Mz7xiXZt',\n",
        "                                          #'1ObvjBbxRNIp1vOQipUGZxZEi_eQH7kJQ'],\n",
        "                'ALwAlxItASeEs2vYAeLXHA':['1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk'],\n",
        "                                          #'1crTNpDR2VjBdGSgavIvN4QNhwTUR3-Bk',\n",
        "                                          #'1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk',\n",
        "                                          #'1tgJnMW0ZImx1vbDzWpvFBRX0FJu1Kdbt'],\n",
        "                'OVTZNSkSfbl3gVB9XQIJfw':['12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF'],\n",
        "                                          #'1Vmkzpdi_0m9CGcyp5ggeNsUsueBzqufH',\n",
        "                                          #'12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF',\n",
        "                                          #'1qecYhCBytq1Z6lm_ueCWIuithvRG0wIq'],\n",
        "                'Sovgwq-E-n6wLqNh3X_rXg':['1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT'],\n",
        "                                          #'1VVePrvWR7e5XkRmhp5Q5qOPXPskVmnnt',\n",
        "                                          #'1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT',\n",
        "                                          #'1R5GFYo2c2YC_AOB9G0pdOfROTyglePDI'],\n",
        "                'j5nPiTwWEFr-VsePew7Sjg':['16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                                          #'1txkAzGKMRjXku18rv9222q-FqVo42A9o',\n",
        "                                          #'1e8iSM3SpyB2Y_h6rzRSVVgphjHEIJnZU',\n",
        "                                          #'16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                'aiX_WP7NKPTdF9CfI-M-wg':['1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                                          #'1VrVgBoJp5cRVMH4I-5uKvdF3b-PSfbmL',\n",
        "                                          #'1f7T7ksdCCGxhVF_RJU9zu38-r8aTTJcD',\n",
        "                                          #'1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                'e4NQLZynhSmvwl38hC4m-A':['1-myKdlTuCpUyPuK8OwHhglpQQGe7Bh-y'],\n",
        "                                          #'1yYG8ftjE1i9prP61ej5EdGfJDlweiK3C',\n",
        "                                          #'1KSdN24eXC-Bnk2_mTQR6oNaGll1KP5hO',\n",
        "                                          #'1-myKdlTuCpUyPuK8OwHhglpQQGe7Bh-y'],\n",
        "                'S-oLPRdhlyL5HAknBKTUcQ':['1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "                                          #'1qcX9pGfmogOokbBOPnZY-NbroolffAlg',\n",
        "                                          #'1YVdSND3VNit4ieORXC_mVzcv4mphGUnY',\n",
        "                                          #'1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "for business_id in id_list:\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for file_id in file_id_list[business_id]:\n",
        "    download = drive.CreateFile({'id': file_id})\n",
        "    download.GetContentFile('demographics_' + business_id + '.csv')\n",
        "    df_temp = pd.read_csv('demographics_' + business_id + '.csv')\n",
        "    df = df.append(df_temp)\n",
        "    \n",
        "  print(len(df.index))\n",
        "  df = df.sort_values(['id', 'age'], ascending=[True, False])\n",
        "  df = df.drop_duplicates('id',keep='first').reset_index(drop=True)\n",
        "  print('senza duplicati: ', len(df.index))\n",
        "  df = df.rename(columns={'id':'user_id'})\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('demographics_' + business_id + '.csv')\n",
        "  upload_file('demographics_' + business_id + '.csv',FOLDER[business_id]['FID_G4'])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1564\n",
            "senza duplicati:  1209\n",
            "1034\n",
            "senza duplicati:  832\n",
            "952\n",
            "senza duplicati:  779\n",
            "1070\n",
            "senza duplicati:  862\n",
            "972\n",
            "senza duplicati:  769\n",
            "1064\n",
            "senza duplicati:  885\n",
            "1514\n",
            "senza duplicati:  1161\n",
            "953\n",
            "senza duplicati:  740\n",
            "965\n",
            "senza duplicati:  772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki8CKUSFkv4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD SENTIMENT ANALYSIS\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "#          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A']\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "file_id_list = {'WbJ1LRQdOuYYlRLyTkuuxw':['1lvZnVuA4QN5KmnJj6IJ8BgxWdcyQixEd',\n",
        "                                          '1w4sWZYFmWLMywS43TuleneQPKQwauPQK',\n",
        "                                          '1sIHc1BZcBWT3Iqd94-PaA4BF5elxcEYi'],\n",
        "                'T2tEMLpTeSMxLKpxwFdS3g':['11fM_aHYkCLQ2z3g7Z76aZZ3cAsIrpezN',\n",
        "                                          '1TPexYC2YHq_8ywZSuRRIJkrUCstwoxSp',\n",
        "                                          '1ObvjBbxRNIp1vOQipUGZxZEi_eQH7kJQ'],\n",
        "                'ALwAlxItASeEs2vYAeLXHA':['1crTNpDR2VjBdGSgavIvN4QNhwTUR3-Bk',\n",
        "                                          '1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk',\n",
        "                                          '1tgJnMW0ZImx1vbDzWpvFBRX0FJu1Kdbt'],\n",
        "                'OVTZNSkSfbl3gVB9XQIJfw':['1Vmkzpdi_0m9CGcyp5ggeNsUsueBzqufH',\n",
        "                                          '12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF',\n",
        "                                          '1qecYhCBytq1Z6lm_ueCWIuithvRG0wIq'],\n",
        "                'Sovgwq-E-n6wLqNh3X_rXg':['1VVePrvWR7e5XkRmhp5Q5qOPXPskVmnnt',\n",
        "                                          '1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT',\n",
        "                                          '1R5GFYo2c2YC_AOB9G0pdOfROTyglePDI'],\n",
        "                'j5nPiTwWEFr-VsePew7Sjg':['1txkAzGKMRjXku18rv9222q-FqVo42A9o',\n",
        "                                          '1e8iSM3SpyB2Y_h6rzRSVVgphjHEIJnZU',\n",
        "                                          '16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                'aiX_WP7NKPTdF9CfI-M-wg':['1VrVgBoJp5cRVMH4I-5uKvdF3b-PSfbmL',\n",
        "                                          '1f7T7ksdCCGxhVF_RJU9zu38-r8aTTJcD',\n",
        "                                          '1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                'e4NQLZynhSmvwl38hC4m-A':['1yYG8ftjE1i9prP61ej5EdGfJDlweiK3C',\n",
        "                                          '1KSdN24eXC-Bnk2_mTQR6oNaGll1KP5hO',\n",
        "                                          '1PA2eYsYj2bj4Mo7ofbnvbQgYjmd2SwRG'],\n",
        "                'S-oLPRdhlyL5HAknBKTUcQ':['1qcX9pGfmogOokbBOPnZY-NbroolffAlg',\n",
        "                                          '1YVdSND3VNit4ieORXC_mVzcv4mphGUnY',\n",
        "                                          '1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "for business_id in id_list:\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for file_id in file_id_list[business_id]:\n",
        "    download = drive.CreateFile({'id': file_id})\n",
        "    download.GetContentFile('sentiment_' + business_id + '.csv')\n",
        "    df_temp = pd.read_csv('sentiment_' + business_id + '.csv')\n",
        "    df = df.append(df_temp)\n",
        "    \n",
        "  print(len(df.index))\n",
        "  df = df.drop_duplicates('id',keep='first').reset_index(drop=True)\n",
        "  print(len(df.index))\n",
        "  df = df.rename(columns={'id':'user_id'})\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('sentiment_' + business_id + '.csv')\n",
        "  upload_file('sentiment_' + business_id + '.csv',FOLDER[business_id]['FID_G5'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbM06dpOYYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXEC IN LOCAL THE DOWNLOAD OF REVIEW RANKINGS OF EACH RESTAURANT\n",
        "# UPLOAD CSV IN COLAB\n",
        "############################# EXECUTE IN LOCAL (if not blocked by yelp) #############################################\n",
        "def get_ranking_from_call(url_business, lang, sort, query):\n",
        "    headers = [{\"name\": \"Accept\", \"value\": \"*/*\"}, {\"name\": \"Accept-Encoding\", \"value\": \"gzip, deflate, br\"},\n",
        "               {\"name\": \"Accept-Language\", \"value\": \"it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3\"},\n",
        "               {\"name\": \"Connection\", \"value\": \"keep-alive\"},\n",
        "               {\"name\": \"Content-Type\", \"value\": \"application/x-www-form-urlencoded; charset=utf-8\"}, {\"name\": \"Cookie\",\n",
        "                                                                                                       \"value\": \"qntcst=D; hl=en_US; wdi=1|3C26116D69138F61|0x1.78d019f71a444p+30|a7756ff94751d3a9; _ga=GA1.2.3C26116D69138F61; location=%7B%22city%22%3A+%22New+York%22%2C+%22state%22%3A+%22NY%22%2C+%22country%22%3A+%22US%22%2C+%22latitude%22%3A+40.713%2C+%22longitude%22%3A+-74.0072%2C+%22max_latitude%22%3A+40.8523%2C+%22min_latitude%22%3A+40.5597%2C+%22max_longitude%22%3A+-73.7938%2C+%22min_longitude%22%3A+-74.1948%2C+%22zip%22%3A+%22%22%2C+%22address1%22%3A+%22%22%2C+%22address2%22%3A+%22%22%2C+%22address3%22%3A+null%2C+%22neighborhood%22%3A+null%2C+%22borough%22%3A+null%2C+%22provenance%22%3A+%22YELP_GEOCODING_ENGINE%22%2C+%22display%22%3A+%22New+York%2C+NY%22%2C+%22unformatted%22%3A+%22New+York%2C+NY%2C+US%22%2C+%22accuracy%22%3A+4.0%2C+%22language%22%3A+null%7D; xcj=1|Ptt9P03gfc75x_PBT9zmqCkUuSuyB7PR-wWUBvABNi4; __qca=P0-60561249-1581956668708; G_ENABLED_IDPS=google; __cfduid=db8764ff59d8028a6c2e1b214867927d81583160194; _gid=GA1.2.2014867238.1583835527; bse=05dcd9d5de304ef0b1d9a76fa768b10f; sc=8a1ca0dbc2; pid=505721aa4569e7bb\"},\n",
        "               {\"name\": \"Host\", \"value\": \"www.yelp.com\"},\n",
        "               {\"name\": \"Referer\", \"value\": \"https://www.yelp.com/biz/noche-de-margaritas-new-york\"},\n",
        "               {\"name\": \"TE\", \"value\": \"Trailers\"}, {\"name\": \"User-Agent\",\n",
        "                                                     \"value\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0\"},\n",
        "               {\"name\": \"X-Requested-By-React\", \"value\": \"true\"},\n",
        "               {\"name\": \"X-Requested-With\", \"value\": \"XMLHttpRequest\"}]\n",
        "    headers_ok = {}\n",
        "    for header in headers:\n",
        "        temp = {\n",
        "            header['name']: header['value']\n",
        "        }\n",
        "        headers_ok.update(temp)\n",
        "\n",
        "    x = 0\n",
        "    reviews_list = []\n",
        "    position = 1\n",
        "    url = url_business + \"/review_feed?rl=\" + lang + \"&sort_by=\" + sort + \"&q=\" + query\n",
        "\n",
        "    while 1:\n",
        "        if x == 0:\n",
        "            page_load = requests.get(url + '&start=', headers=headers_ok)\n",
        "        else:\n",
        "            page_load = requests.get(url + '&start=' + str(x), headers=headers_ok)\n",
        "        print(page_load)\n",
        "        x = x + 20\n",
        "        reviews = page_load.json()['reviews']\n",
        "        # print(json.dumps(reviews, indent=4, sort_keys=True))\n",
        "        if not reviews:\n",
        "            break\n",
        "        for review in reviews:\n",
        "            reviews_list.append((position, review['userId'], review['user'],#['reviewCount'],\n",
        "                                 datetime.datetime.strptime(review['localizedDate'], '%m/%d/%Y')))\n",
        "            position = position + 1\n",
        "    df_reviews = pd.DataFrame(reviews_list, columns=[\"position\", \"user_id\", \"user\", \"date\"])\n",
        "    return df_reviews\n",
        "\n",
        "\n",
        "def retrieve_rankings(business_id):\n",
        "    df_rel_ranking = get_ranking_from_call(\"https://www.yelp.com/biz/\" + business_id, \"en\", \"relevance_desc\", \"\")\n",
        "    df_date_ranking = df_rel_ranking.sort_values(by=['date']).reset_index(drop=True)\n",
        "    df_date_ranking['position'] = df_date_ranking.index + 1\n",
        "    df_rand_ranking = df_rel_ranking.sample(frac=1).reset_index(drop=True)\n",
        "    df_rand_ranking['position'] = df_rand_ranking.index + 1\n",
        "\n",
        "    df_rel_ranking.to_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "    df_date_ranking.to_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "    df_rand_ranking.to_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "\n",
        "\n",
        "rest_ids = []\n",
        "\n",
        "with open('rest_ids.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    rest_ids.append(line[:-1])\n",
        "\n",
        "print(rest_ids)\n",
        "for id in rest_ids:\n",
        "    retrieve_rankings(id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdV4KHSkePdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ALTERNATIVE METHOD TO SHUFFLE THE RANKING FOR RANDOM\n",
        "#see https://gist.github.com/cadrev/6b91985a1660f26c2742\n",
        "business_id = 'WbJ1LRQdOuYYlRLyTkuuxw'\n",
        "df_rel_ranking = pd.read_csv('rel_ranking_' + business_id + '.csv')\n",
        "df_date_ranking = df_rel_ranking.sort_values(by=['Date']).reset_index(drop=True)\n",
        "df_date_ranking['Position'] = df_date_ranking.index + 1\n",
        "df_rand_ranking = df_rel_ranking\n",
        "\n",
        "#random.shuffle(df_rand_ranking) DOESN'T WORK KEY ERROR\n",
        "df_rand_ranking = df_rand_ranking.reindex(np.random.permutation(df_rand_ranking.index))\n",
        "\n",
        "df_rand_ranking = df_rand_ranking.reset_index(drop=True)\n",
        "\n",
        "# drop all the unnamed columns\n",
        "df_rel_ranking = drop_unnamed(df_rel_ranking)\n",
        "df_rand_ranking = drop_unnamed(df_rand_ranking)\n",
        "df_date_ranking = drop_unnamed(df_date_ranking)\n",
        "\n",
        "df_rand_ranking['Position'] = df_rand_ranking.index + 1\n",
        "\n",
        "print(df_rand_ranking)\n",
        "\n",
        "df_rel_ranking.to_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "df_date_ranking.to_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "df_rand_ranking.to_csv(\"rand_ranking_\" + business_id + \".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070u7qOzHsiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# JOIN USER FROM YELP WITH USER FROM DATASET AND PRINT LOST USERS\n",
        "def filter_user_from_dataset(df, users):\n",
        "  df = df.rename(columns={'Position':'position','User_Id': 'user_id',\n",
        "                          'Date':'date'})\n",
        "  df = df[['position','user_id','date']]\n",
        "  df_merged = df.merge(users, on='user_id')\n",
        "  total_review = df['position'].max()\n",
        "  print('Utenti persi: totali ' + str(total_review) + ' - utenti nel dataset ' +\n",
        "        str(len(df_merged.index)) + ' = ' + str(total_review - len(df_merged.index)))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "\n",
        "  cols = df_merged.columns.tolist()\n",
        "\n",
        "  df_merged = df_merged[['position', 'user_id', 'date', 'fans', 'average_stars', 'review_count']]\n",
        "\n",
        "  return df_merged\n",
        "\n",
        "\n",
        "def compute_groups_percents(df_groups, N_of_groups):\n",
        "  total = len(df_groups.index)\n",
        "  percents = []\n",
        "  i = 0\n",
        "  while i < N_of_groups:\n",
        "    current_length = len(df_groups[df_groups['group_id'] == i].index)\n",
        "    percents.append((current_length/total)*100)\n",
        "    i = i + 1\n",
        "  print(percents)\n",
        "  return percents\n",
        "\n",
        "\n",
        "# ADD REVIEW INFO IN RANKING DATAFRAME\n",
        "def integrate_review_info(df, reviews, business_id):\n",
        "  business_reviews = reviews[reviews['business_id'] == business_id]\n",
        "  df_merged = business_reviews.merge(df, on='user_id')\n",
        "  del df_merged['date_y']  # it's date from yelp website (not updated in dataset)\n",
        "  df_merged = df_merged.rename(columns={'date_x':'date'})\n",
        "  # drop duplicates reviews from same user\n",
        "  df_merged = df_merged.sort_values('date').drop_duplicates('user_id',keep='last')\n",
        "  df_merged = df_merged.sort_values(by=['position']).reset_index(drop=True)\n",
        "  print('Review perse: ', len(df.index) - len(df_merged.index))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "\n",
        "  df_merged = df_merged[['position', 'user_id', 'review_id', 'date', 'text', 'fans', 'average_stars', 'review_count', 'business_id']]\n",
        "  \n",
        "  return df_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezxCXIZBkcAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADD USER DATA TO GROUP USERS\n",
        "def create_vectors_only_demographics(df_users, business_id, list_of_attributes, method):\n",
        "  vectors = df_users[['user_id']]\n",
        "\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "def create_vectors(df_users, reviews, business, business_id, list_of_attributes, method):\n",
        "  vectors = df_users[['user_id', 'review_count', 'fans' , 'average_stars']]\n",
        "  vectors[\"useful\"] = np.NaN\n",
        "  vectors[\"funny\"] = np.NaN\n",
        "  vectors[\"cool\"] = np.NaN\n",
        "  vectors[\"loc1\"] = np.NaN\n",
        "  vectors[\"loc2\"] = np.NaN\n",
        "  vectors[\"loc3\"] = np.NaN\n",
        "  print(vectors.columns)\n",
        "  for index, user in vectors.iterrows():\n",
        "    print(index)\n",
        "    # TOP THREE LOCATION\n",
        "    location_list = get_top_three_loc(user, reviews, business)\n",
        "    i = 0\n",
        "    while i < len(location_list):\n",
        "      vectors.loc[index, 'loc'+str(i+1)] = location_list[i]\n",
        "      i = i + 1\n",
        "\n",
        "    # USEFUL FUNNY COOL\n",
        "    useful, funny, cool = get_details_user(user, reviews)\n",
        "    vectors.loc[index, 'useful'] = useful\n",
        "    vectors.loc[index, 'funny'] = funny\n",
        "    vectors.loc[index, 'cool'] = cool\n",
        "\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "\n",
        "def get_demographics(vectors, id):\n",
        "  df_demographics = pd.read_csv('demographics_' + id + '.csv')\n",
        "  new_vectors = pd.merge(vectors, df_demographics, on='user_id', how='left')\n",
        "  new_vectors = drop_unnamed(new_vectors)\n",
        "  return new_vectors\n",
        "\n",
        "\n",
        "def get_details_user(user, reviews):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  return user_reviews['useful'].sum(), user_reviews['funny'].sum(), user_reviews['cool'].sum()\n",
        "\n",
        "\n",
        "def get_top_three_loc(user, reviews, business):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  result = user_reviews.merge(business, on='business_id')\n",
        "  top_loc = result['state'].value_counts().index.tolist()[:3]\n",
        "  top_zip = []\n",
        "  for location in top_loc:\n",
        "    # exclude postal_code of Canada, that is strings\n",
        "    temp_list = result[result['state']==location]['postal_code'].value_counts().index.tolist()\n",
        "    if temp_list != []:\n",
        "      temp_list = [elem for elem in temp_list if str(elem).isdigit()]\n",
        "      if temp_list != []:\n",
        "        top_zip.append(int(str(temp_list[0])[:3]))\n",
        "  return top_zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHR1vvEz-yk4",
        "colab_type": "code",
        "outputId": "d23fb1cf-0b34-4380-cfcf-577f2bd61463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "def print_statistics_of_demographics(id):\n",
        "  df = pd.read_csv('demographics_' + id + '.csv')\n",
        "  all_users = len(df.index)\n",
        "  print(all_users)\n",
        "\n",
        "  print('-------------' + id + '--------------')\n",
        "\n",
        "  u_0_20 = len(df[(df['age']>=0) & (df['age']<=20)])\n",
        "  print('Utenti in fascia età 0-20: %d (%.2f%%)' % (u_0_20, (u_0_20/all_users)*100))\n",
        "\n",
        "  u_20_40 = len(df[(df['age']>20) & (df['age']<=40)])\n",
        "  print('Utenti in fascia età 20-40: %d (%.2f%%)' % (u_20_40, (u_20_40/all_users)*100))\n",
        "\n",
        "  u_40_60 = len(df[(df['age']>40) & (df['age']<=60)])\n",
        "  print('Utenti in fascia età 40-60: %d (%.2f%%)' % (u_40_60, (u_40_60/all_users)*100))\n",
        "\n",
        "  u_60_90 = len(df[(df['age']>60) & (df['age']<=90)])\n",
        "  print('Utenti in fascia età 60-90: %d (%.2f%%)' % (u_60_90, (u_60_90/all_users)*100))\n",
        "\n",
        "  u_over_90 = len(df[(df['age']>90)])\n",
        "  print('Utenti in fascia età >90: %d (%.2f%%)' % (u_over_90, (u_over_90/all_users)*100))\n",
        "\n",
        "  #temp = df[(df['age']>=0) & (df['age']<=20)][['user_id', 'age']]\n",
        "  #temp = drop_unnamed(temp)\n",
        "  #print(temp)\n",
        "\n",
        "  print('--------------------')\n",
        "\n",
        "  u_fem = len(df[df['gender']=='feminine'])\n",
        "  print('Utenti femmine: %d (%.2f%%)' % (u_fem, (u_fem/all_users)*100))\n",
        "\n",
        "  u_mas = len(df[df['gender']=='masculine'])\n",
        "  print('Utenti maschi: %d (%.2f%%)' % (u_mas, (u_mas/all_users)*100))\n",
        "\n",
        "  print('--------------------')\n",
        "\n",
        "  u_white = len(df[df['ethnicity']=='white'].reset_index(drop=True))\n",
        "  print('Utenti bianchi: %d (%.2f%%)' % (u_white, (u_white/all_users)*100))\n",
        "  bianchi = df[df['ethnicity']=='white'].reset_index(drop=True)\n",
        "  b_under_39 = len(bianchi[(bianchi['age']<39)])\n",
        "  print('Bianchi in fascia età <39: %d (%.2f%%)' % (b_under_39, (b_under_39/all_users)*100))\n",
        "  b_over_39 = len(bianchi[(bianchi['age']>=39)])\n",
        "  print('Bianchi in fascia età >39: %d (%.2f%%)' % (b_over_39, (b_over_39/all_users)*100))\n",
        "  b_fem = len(bianchi[bianchi['gender']=='feminine'])\n",
        "  print('Bianchi femmine: %d (%.2f%%)' % (b_fem, (b_fem/all_users)*100))\n",
        "  b_mas = len(bianchi[bianchi['gender']=='masculine'])\n",
        "  print('Bianchi maschi: %d (%.2f%%)' % (b_mas, (b_mas/all_users)*100))\n",
        "\n",
        "\n",
        "  u_black = len(df[df['ethnicity']=='black or african american'])\n",
        "  print('Utenti neri: %d (%.2f%%)' % (u_black, (u_black/all_users)*100))\n",
        "  neri = df[df['ethnicity']=='black or african american']\n",
        "  n_under_39 = len(neri[(neri['age']<39)])\n",
        "  print('Neri in fascia età <39: %d (%.2f%%)' % (n_under_39, (n_under_39/all_users)*100))\n",
        "  n_over_39 = len(neri[(neri['age']>=39)])\n",
        "  print('Neri in fascia età >39: %d (%.2f%%)' % (n_over_39, (n_over_39/all_users)*100))\n",
        "  n_fem = len(neri[neri['gender']=='feminine'])\n",
        "  print('Neri femmine: %d (%.2f%%)' % (n_fem, (n_fem/all_users)*100))\n",
        "  n_mas = len(neri[neri['gender']=='masculine'])\n",
        "  print('Neri maschi: %d (%.2f%%)' % (n_mas, (n_mas/all_users)*100))\n",
        "\n",
        "  u_altro = len(df[(df['ethnicity']=='hispanic, latino, or spanish origin') | (df['ethnicity']=='asian') | (df['ethnicity']=='middle eastern or north african') | (df['ethnicity']=='native hawaiian or pacific islander')])\n",
        "  print('Utenti altre etnie: %d (%.2f%%)' % (u_altro, (u_altro/all_users)*100))\n",
        "  altri = df[(df['ethnicity']=='hispanic, latino, or spanish origin') | (df['ethnicity']=='asian') | (df['ethnicity']=='middle eastern or north african') | (df['ethnicity']=='native hawaiian or pacific islander')]\n",
        "  a_under_39 = len(altri[(altri['age']<39)])\n",
        "  print('Altri in fascia età <39: %d (%.2f%%)' % (a_under_39, (a_under_39/all_users)*100))\n",
        "  a_over_39 = len(altri[(altri['age']>=39)])\n",
        "  print('Altri in fascia età >39: %d (%.2f%%)' % (a_over_39, (a_over_39/all_users)*100))\n",
        "  a_fem = len(altri[altri['gender']=='feminine'])\n",
        "  print('Altri femmine: %d (%.2f%%)' % (a_fem, (a_fem/all_users)*100))\n",
        "  a_mas = len(altri[altri['gender']=='masculine'])\n",
        "  print('Altri maschi: %d (%.2f%%)' % (a_mas, (a_mas/all_users)*100))\n",
        "\n",
        "  '''u_latino = len(df[df['ethnicity']=='hispanic, latino, or spanish origin'])\n",
        "  print('Utenti latini: %d (%.2f%%)' % (u_latino, (u_latino/all_users)*100))\n",
        "\n",
        "  u_asian = len(df[df['ethnicity']=='asian'])\n",
        "  print('Utenti asiatici: %d (%.2f%%)' % (u_asian, (u_asian/all_users)*100))\n",
        "\n",
        "  u_arabs = len(df[df['ethnicity']=='middle eastern or north african'])\n",
        "  print('Utenti arabi: %d (%.2f%%)' % (u_arabs, (u_arabs/all_users)*100))\n",
        "\n",
        "  u_hawa = len(df[df['ethnicity']=='native hawaiian or pacific islander'])\n",
        "  print('Utenti hawaiiani: %d (%.2f%%)' % (u_hawa, (u_hawa/all_users)*100))'''\n",
        "\n",
        "\n",
        "\n",
        "  print('---------ALTRE MISURE-----------')\n",
        "\n",
        "  u_0_35 = len(df[(df['age']>=0) & (df['age']<35)])\n",
        "  print('Utenti in fascia età 0-35: %d (%.2f%%)' % (u_0_35, (u_0_35/all_users)*100))\n",
        "  \n",
        "  u_35_40 = len(df[(df['age']>=35) & (df['age']<=40)])\n",
        "  print('Utenti in fascia età 35-40: %d (%.2f%%)' % (u_35_40, (u_35_40/all_users)*100))\n",
        "\n",
        "  u_over_40 = len(df[(df['age']>40)])\n",
        "  print('Utenti in fascia età >40: %d (%.2f%%)' % (u_over_40, (u_over_40/all_users)*100))\n",
        "\n",
        "  print('---------ALTRE MISURE 2-----------')\n",
        "\n",
        "  u_under_39 = len(df[(df['age']<39)])\n",
        "  print('Utenti in fascia età <39: %d (%.2f%%)' % (u_under_39, (u_under_39/all_users)*100))\n",
        "  \n",
        "  u_over_39 = len(df[(df['age']>=39)])\n",
        "  print('Utenti in fascia età >39: %d (%.2f%%)' % (u_over_39, (u_over_39/all_users)*100))\n",
        "\n",
        "\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "for id in id_list:\n",
        "  print_statistics_of_demographics(id)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1209\n",
            "-------------WbJ1LRQdOuYYlRLyTkuuxw--------------\n",
            "Utenti in fascia età 0-20: 25 (2.07%)\n",
            "Utenti in fascia età 20-40: 865 (71.55%)\n",
            "Utenti in fascia età 40-60: 293 (24.23%)\n",
            "Utenti in fascia età 60-90: 24 (1.99%)\n",
            "Utenti in fascia età >90: 2 (0.17%)\n",
            "--------------------\n",
            "Utenti femmine: 775 (64.10%)\n",
            "Utenti maschi: 434 (35.90%)\n",
            "--------------------\n",
            "Utenti bianchi: 693 (57.32%)\n",
            "Bianchi in fascia età <39: 222 (18.36%)\n",
            "Bianchi in fascia età >39: 471 (38.96%)\n",
            "Bianchi femmine: 422 (34.90%)\n",
            "Bianchi maschi: 271 (22.42%)\n",
            "Utenti neri: 245 (20.26%)\n",
            "Neri in fascia età <39: 111 (9.18%)\n",
            "Neri in fascia età >39: 134 (11.08%)\n",
            "Neri femmine: 178 (14.72%)\n",
            "Neri maschi: 67 (5.54%)\n",
            "Utenti altre etnie: 270 (22.33%)\n",
            "Altri in fascia età <39: 127 (10.50%)\n",
            "Altri in fascia età >39: 143 (11.83%)\n",
            "Altri femmine: 174 (14.39%)\n",
            "Altri maschi: 96 (7.94%)\n",
            "---------ALTRE MISURE-----------\n",
            "Utenti in fascia età 0-35: 306 (25.31%)\n",
            "Utenti in fascia età 35-40: 584 (48.30%)\n",
            "Utenti in fascia età >40: 319 (26.39%)\n",
            "---------ALTRE MISURE 2-----------\n",
            "Utenti in fascia età <39: 460 (38.05%)\n",
            "Utenti in fascia età >39: 749 (61.95%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW2AKZ8lHgoB",
        "colab_type": "code",
        "outputId": "34bca9a7-3b0d-4bf8-be91-f8ed5b737b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PIPELINE\n",
        "N_of_groups = 7\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "#          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A']\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "\n",
        "#list_of_attributes = ['review_count']\n",
        "#list_of_attributes = ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']\n",
        "#list_of_attributes = ['loc1', 'loc2', 'loc3']\n",
        "list_of_attributes = ['age', 'gender', 'ethnicity']\n",
        "#list_of_attributes = ['fans']\n",
        "#list_of_attributes = ['useful', 'funny', 'cool']\n",
        "#list_of_attributes = ['age', 'ethnicity']\n",
        "method = 'custom'\n",
        "\n",
        "for id in id_list:\n",
        "  authenticate()\n",
        "  df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1_yelp(id, users, reviews)\n",
        "  #df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1(id, users, reviews)\n",
        "  #df_rel_ranking = pd.read_csv(\"dataset_rel_ranking_\" + id + \".csv\")\n",
        "  #df_date_ranking = pd.read_csv(\"dataset_date_ranking_\" + id + \".csv\")\n",
        "  #df_rand_ranking = pd.read_csv(\"dataset_rand_ranking_\" + id + \".csv\")\n",
        "  group_list, percents = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                   N_of_groups, list_of_attributes, method)\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes)\n",
        "  "
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "++++++++++++++++ RANKING ++++++++++++++++++\n",
            "\n",
            "Ranking by Yelp filter:\n",
            "      position                 user_id  Review_count        Date\n",
            "0            1  4vdFVrrYxqb4ofj8RK8xvw            70  2020-02-16\n",
            "1            2  7BEIyQzlskOV1OAJ0nbweQ            95  2020-02-16\n",
            "2            3  GaWT2KIjIxfIBMgGIm7HGw           690  2020-02-13\n",
            "3            4  IDwjyXR4zY1fICjuG-1idw           270  2020-01-29\n",
            "4            5  PMEK5fq3B_YAMfR8urwBtg            91  2020-01-19\n",
            "...        ...                     ...           ...         ...\n",
            "2086      2087  03_OcS8SbBcO4RTx_jYAUw             5  2016-03-26\n",
            "2087      2088  vpCdKYNXUjs5o-tVSmwGAg             3  2014-01-06\n",
            "2088      2089  vVBNWuQO9m5Utm69PGoRCw             7  2013-12-27\n",
            "2089      2090  h7zHYM8LbPWmXx0ZEqOOAw            20  2015-11-23\n",
            "2090      2091  4wbMeUS9tp2QGqyNhj3YTg            47  2014-09-01\n",
            "\n",
            "[2091 rows x 4 columns]\n",
            "\n",
            "\n",
            "Ranking by Date:\n",
            "      position                 user_id  Review_count        Date\n",
            "0            1  PVyZXgOkVtnU6966FDFhuw           576  2013-12-08\n",
            "1            2  aK99XES2p7yNyWKpJjpxkQ            16  2013-12-09\n",
            "2            3  f6k8dOPuuFHJaj1icCV1ZA            25  2013-12-10\n",
            "3            4  hVXj8lnaTIMLTkD_yjSj6A           189  2013-12-12\n",
            "4            5  wCDC5NVQLKdTa-OwzLG9vg            12  2013-12-14\n",
            "...        ...                     ...           ...         ...\n",
            "2086      2087  5sJlO6HA_SzC_tn1rmKy4A           105  2020-02-23\n",
            "2087      2088  FbeO2SApJ6GlOy2ZT3-88Q             7  2020-02-23\n",
            "2088      2089  sqq5uqXs5NLg_fOI7DtO9Q           460  2020-02-23\n",
            "2089      2090  EIMuP58fV0S_-FFPxLxvuA            15  2020-03-03\n",
            "2090      2091  _qohdxiWQ0LO3ph_12BWaw             7  2020-03-11\n",
            "\n",
            "[2091 rows x 4 columns]\n",
            "\n",
            "\n",
            "Ranking Random:\n",
            "      position                 user_id  Review_count        Date\n",
            "0            1  MjkbqbLYdLHAimX1PQ9UGg             5  2016-07-07\n",
            "1            2  3KvcAg61IiwDcyVjm63JkQ             2  2017-03-05\n",
            "2            3  t8v-SKJTkQVw7QYQMOJKfw             3  2017-08-10\n",
            "3            4  U8AqHy8hJfD-9X6H68KZsg             1  2018-07-28\n",
            "4            5  Hm6FZKfquhi6crLAdwY71Q            17  2017-04-07\n",
            "...        ...                     ...           ...         ...\n",
            "2086      2087  JeF43VrWyHcqQIyLo0XqOw            17  2014-04-30\n",
            "2087      2088  oftCA3e_n92XBWf453f56g            29  2015-07-14\n",
            "2088      2089  VlcasgkqiTuPi-nVT7rEtw           357  2019-01-13\n",
            "2089      2090  MWjOBJEQop__owYvB3quLg           157  2018-05-16\n",
            "2090      2091  yeI9EOQzS4r6F5GrrUsWxg            29  2016-04-20\n",
            "\n",
            "[2091 rows x 4 columns]\n",
            "\n",
            "\n",
            "\n",
            "++++++++++++++++ GROUPS CREATION ++++++++++++++++++\n",
            "\n",
            "Index(['user_id', 'age', 'gender', 'ethnicity'], dtype='object')\n",
            "df_vectors len = 2091\n",
            "1209 = 1208\n",
            "883 = 883\n",
            "                    user_id   age    gender ethnicity  feminine  masculine  \\\n",
            "0    IDwjyXR4zY1fICjuG-1idw  39.0  feminine     white         1          0   \n",
            "1    XfzLWQcULk110tiMjtmb8A  38.0  feminine     white         1          0   \n",
            "2    ofriwKiBqPO8sWlaOksing  39.0  feminine     white         1          0   \n",
            "3    al-88kr0SAphNMTjdft47Q  37.0  feminine     white         1          0   \n",
            "4    ehqxXKlMjg0H3FkxCySXiA  39.0  feminine     white         1          0   \n",
            "..                      ...   ...       ...       ...       ...        ...   \n",
            "417  riycQentXhL_Unr5ux9zyg  45.0  feminine     white         1          0   \n",
            "418  fDKwVe0dHSKqaRPVULvZSg  49.0  feminine     white         1          0   \n",
            "419  SHO8XXRSeTE6_gLtyOc_JQ  56.0  feminine     white         1          0   \n",
            "420  wCDC5NVQLKdTa-OwzLG9vg  55.0  feminine     white         1          0   \n",
            "421  bAxtOfFyhgvyHwK3gkBUrg  37.0  feminine     white         1          0   \n",
            "\n",
            "     american indian or alaska native  asian  black or african american  \\\n",
            "0                                   0      0                          0   \n",
            "1                                   0      0                          0   \n",
            "2                                   0      0                          0   \n",
            "3                                   0      0                          0   \n",
            "4                                   0      0                          0   \n",
            "..                                ...    ...                        ...   \n",
            "417                                 0      0                          0   \n",
            "418                                 0      0                          0   \n",
            "419                                 0      0                          0   \n",
            "420                                 0      0                          0   \n",
            "421                                 0      0                          0   \n",
            "\n",
            "     hispanic, latino, or spanish origin  middle eastern or north african  \\\n",
            "0                                      0                                0   \n",
            "1                                      0                                0   \n",
            "2                                      0                                0   \n",
            "3                                      0                                0   \n",
            "4                                      0                                0   \n",
            "..                                   ...                              ...   \n",
            "417                                    0                                0   \n",
            "418                                    0                                0   \n",
            "419                                    0                                0   \n",
            "420                                    0                                0   \n",
            "421                                    0                                0   \n",
            "\n",
            "     native hawaiian or pacific islander  white  \n",
            "0                                      0      1  \n",
            "1                                      0      1  \n",
            "2                                      0      1  \n",
            "3                                      0      1  \n",
            "4                                      0      1  \n",
            "..                                   ...    ...  \n",
            "417                                    0      1  \n",
            "418                                    0      1  \n",
            "419                                    0      1  \n",
            "420                                    0      1  \n",
            "421                                    0      1  \n",
            "\n",
            "[422 rows x 13 columns]\n",
            "                    user_id   age     gender ethnicity  feminine  masculine  \\\n",
            "0    4m9NXICYBC5i9t4aTt-I6w  45.0  masculine     white         0          1   \n",
            "1    zgz5B0P4LgsUySCqz7jweQ  39.0  masculine     white         0          1   \n",
            "2    z4Bvefqd1bV7hwAuM7LJbw  39.0  masculine     white         0          1   \n",
            "3    FbeO2SApJ6GlOy2ZT3-88Q  45.0  masculine     white         0          1   \n",
            "4    V5AsoF06KAsuqqMFqsxglA  39.0  masculine     white         0          1   \n",
            "..                      ...   ...        ...       ...       ...        ...   \n",
            "265  iImnb-W4UaMsLsXd9Eq3EA  36.0  masculine     white         0          1   \n",
            "266  VDFXrJXhrNRBVUH68-yW4A  58.0  masculine     white         0          1   \n",
            "267  eE1L8u2_iOV50yhMt9xyhw  27.0  masculine     white         0          1   \n",
            "268  y2T3FPC5BQgZ_C9IvhoTtg  39.0  masculine     white         0          1   \n",
            "269  Eeq2B13a-cavWckKaQJadg  39.0  masculine     white         0          1   \n",
            "\n",
            "     american indian or alaska native  asian  black or african american  \\\n",
            "0                                   0      0                          0   \n",
            "1                                   0      0                          0   \n",
            "2                                   0      0                          0   \n",
            "3                                   0      0                          0   \n",
            "4                                   0      0                          0   \n",
            "..                                ...    ...                        ...   \n",
            "265                                 0      0                          0   \n",
            "266                                 0      0                          0   \n",
            "267                                 0      0                          0   \n",
            "268                                 0      0                          0   \n",
            "269                                 0      0                          0   \n",
            "\n",
            "     hispanic, latino, or spanish origin  middle eastern or north african  \\\n",
            "0                                      0                                0   \n",
            "1                                      0                                0   \n",
            "2                                      0                                0   \n",
            "3                                      0                                0   \n",
            "4                                      0                                0   \n",
            "..                                   ...                              ...   \n",
            "265                                    0                                0   \n",
            "266                                    0                                0   \n",
            "267                                    0                                0   \n",
            "268                                    0                                0   \n",
            "269                                    0                                0   \n",
            "\n",
            "     native hawaiian or pacific islander  white  \n",
            "0                                      0      1  \n",
            "1                                      0      1  \n",
            "2                                      0      1  \n",
            "3                                      0      1  \n",
            "4                                      0      1  \n",
            "..                                   ...    ...  \n",
            "265                                    0      1  \n",
            "266                                    0      1  \n",
            "267                                    0      1  \n",
            "268                                    0      1  \n",
            "269                                    0      1  \n",
            "\n",
            "[270 rows x 13 columns]\n",
            "                    user_id   age    gender                  ethnicity  \\\n",
            "0    PMEK5fq3B_YAMfR8urwBtg  23.0  feminine  black or african american   \n",
            "1    fEYb-rRR1SYSlUnSk0ewpA  39.0  feminine  black or african american   \n",
            "2    1O7wcFf0uC6Rg1DjsYXdWQ  39.0  feminine  black or african american   \n",
            "3    LHDl09CloksIqjUrwlgiOQ  36.0  feminine  black or african american   \n",
            "4    kme90qFqjjAS_gisyONmww  39.0  feminine  black or african american   \n",
            "..                      ...   ...       ...                        ...   \n",
            "173  Ejldt4p3QUVYJFz-0dbFuQ  39.0  feminine  black or african american   \n",
            "174  YkgVUj-6eAplQRtSFmxahQ  39.0  feminine  black or african american   \n",
            "175  -8mGZ-pJi-NcjZckuz1M7A  50.0  feminine  black or african american   \n",
            "176  buEcsSARMfFhjWjdcMUE1Q  37.0  feminine  black or african american   \n",
            "177  29vtBNW7413lNjUU3tGbew  23.0  feminine  black or african american   \n",
            "\n",
            "     feminine  masculine  american indian or alaska native  asian  \\\n",
            "0           1          0                                 0      0   \n",
            "1           1          0                                 0      0   \n",
            "2           1          0                                 0      0   \n",
            "3           1          0                                 0      0   \n",
            "4           1          0                                 0      0   \n",
            "..        ...        ...                               ...    ...   \n",
            "173         1          0                                 0      0   \n",
            "174         1          0                                 0      0   \n",
            "175         1          0                                 0      0   \n",
            "176         1          0                                 0      0   \n",
            "177         1          0                                 0      0   \n",
            "\n",
            "     black or african american  hispanic, latino, or spanish origin  \\\n",
            "0                            1                                    0   \n",
            "1                            1                                    0   \n",
            "2                            1                                    0   \n",
            "3                            1                                    0   \n",
            "4                            1                                    0   \n",
            "..                         ...                                  ...   \n",
            "173                          1                                    0   \n",
            "174                          1                                    0   \n",
            "175                          1                                    0   \n",
            "176                          1                                    0   \n",
            "177                          1                                    0   \n",
            "\n",
            "     middle eastern or north african  native hawaiian or pacific islander  \\\n",
            "0                                  0                                    0   \n",
            "1                                  0                                    0   \n",
            "2                                  0                                    0   \n",
            "3                                  0                                    0   \n",
            "4                                  0                                    0   \n",
            "..                               ...                                  ...   \n",
            "173                                0                                    0   \n",
            "174                                0                                    0   \n",
            "175                                0                                    0   \n",
            "176                                0                                    0   \n",
            "177                                0                                    0   \n",
            "\n",
            "     white  \n",
            "0        0  \n",
            "1        0  \n",
            "2        0  \n",
            "3        0  \n",
            "4        0  \n",
            "..     ...  \n",
            "173      0  \n",
            "174      0  \n",
            "175      0  \n",
            "176      0  \n",
            "177      0  \n",
            "\n",
            "[178 rows x 13 columns]\n",
            "                   user_id   age     gender                  ethnicity  \\\n",
            "0   UWK3mKjnsVOA6sGim6zfgg  36.0  masculine  black or african american   \n",
            "1   vFTnHK4VM3bbPfEO0pRcmA  39.0  masculine  black or african american   \n",
            "2   D-4X3DGjuSvcSJXp-vFMRw  27.0  masculine  black or african american   \n",
            "3   jMKcvtQn8RVSyXSt2G2iAw  39.0  masculine  black or african american   \n",
            "4   iZB9l9mC7iNqHBTDq6ZH3Q  36.0  masculine  black or african american   \n",
            "..                     ...   ...        ...                        ...   \n",
            "62  uaN7sGhKZ_N8PPxDFmLdlA  39.0  masculine  black or african american   \n",
            "63  4QblvEDrT_lKUm6r_ONFLA  44.0  masculine  black or african american   \n",
            "64  6tlC85J2zsHpnrtHp3qX6Q  39.0  masculine  black or african american   \n",
            "65  9KSf1o6MmaawRPz751s7gA  27.0  masculine  black or african american   \n",
            "66  ccvQN5agFwnviWvDdG_f4Q  40.0  masculine  black or african american   \n",
            "\n",
            "    feminine  masculine  american indian or alaska native  asian  \\\n",
            "0          0          1                                 0      0   \n",
            "1          0          1                                 0      0   \n",
            "2          0          1                                 0      0   \n",
            "3          0          1                                 0      0   \n",
            "4          0          1                                 0      0   \n",
            "..       ...        ...                               ...    ...   \n",
            "62         0          1                                 0      0   \n",
            "63         0          1                                 0      0   \n",
            "64         0          1                                 0      0   \n",
            "65         0          1                                 0      0   \n",
            "66         0          1                                 0      0   \n",
            "\n",
            "    black or african american  hispanic, latino, or spanish origin  \\\n",
            "0                           1                                    0   \n",
            "1                           1                                    0   \n",
            "2                           1                                    0   \n",
            "3                           1                                    0   \n",
            "4                           1                                    0   \n",
            "..                        ...                                  ...   \n",
            "62                          1                                    0   \n",
            "63                          1                                    0   \n",
            "64                          1                                    0   \n",
            "65                          1                                    0   \n",
            "66                          1                                    0   \n",
            "\n",
            "    middle eastern or north african  native hawaiian or pacific islander  \\\n",
            "0                                 0                                    0   \n",
            "1                                 0                                    0   \n",
            "2                                 0                                    0   \n",
            "3                                 0                                    0   \n",
            "4                                 0                                    0   \n",
            "..                              ...                                  ...   \n",
            "62                                0                                    0   \n",
            "63                                0                                    0   \n",
            "64                                0                                    0   \n",
            "65                                0                                    0   \n",
            "66                                0                                    0   \n",
            "\n",
            "    white  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "..    ...  \n",
            "62      0  \n",
            "63      0  \n",
            "64      0  \n",
            "65      0  \n",
            "66      0  \n",
            "\n",
            "[67 rows x 13 columns]\n",
            "                    user_id   age    gender  \\\n",
            "0    7BEIyQzlskOV1OAJ0nbweQ  39.0  feminine   \n",
            "1    8p19CLCYzyrsfoqLCksNVA  37.0  feminine   \n",
            "2    8EVCarfyVLP14hlS4HVk1w  40.0  feminine   \n",
            "3    W1Hxr6BqhRJi2SiB99MW6g  39.0  feminine   \n",
            "4    mCrhj_CG3_pOmDFcfKotRQ  27.0  feminine   \n",
            "..                      ...   ...       ...   \n",
            "169  gZ8VNx7wHuaOd24Bx71saQ  39.0  feminine   \n",
            "170  8DSuxkm7SyX9fgQTw2tuaw  39.0  feminine   \n",
            "171  hhvkQEAUxO-lSLeJo2nUSA  23.0  feminine   \n",
            "172  _KbeQpVBtZQbxOoUg_Eveg  45.0  feminine   \n",
            "173  h-HDAty1FM4wlUTRDCBtLQ  28.0  feminine   \n",
            "\n",
            "                               ethnicity  feminine  masculine  \\\n",
            "0    hispanic, latino, or spanish origin         1          0   \n",
            "1                                  asian         1          0   \n",
            "2    hispanic, latino, or spanish origin         1          0   \n",
            "3                                  asian         1          0   \n",
            "4                                  asian         1          0   \n",
            "..                                   ...       ...        ...   \n",
            "169  hispanic, latino, or spanish origin         1          0   \n",
            "170      middle eastern or north african         1          0   \n",
            "171  hispanic, latino, or spanish origin         1          0   \n",
            "172                                asian         1          0   \n",
            "173  hispanic, latino, or spanish origin         1          0   \n",
            "\n",
            "     american indian or alaska native  asian  black or african american  \\\n",
            "0                                   0      0                          0   \n",
            "1                                   0      1                          0   \n",
            "2                                   0      0                          0   \n",
            "3                                   0      1                          0   \n",
            "4                                   0      1                          0   \n",
            "..                                ...    ...                        ...   \n",
            "169                                 0      0                          0   \n",
            "170                                 0      0                          0   \n",
            "171                                 0      0                          0   \n",
            "172                                 0      1                          0   \n",
            "173                                 0      0                          0   \n",
            "\n",
            "     hispanic, latino, or spanish origin  middle eastern or north african  \\\n",
            "0                                      1                                0   \n",
            "1                                      0                                0   \n",
            "2                                      1                                0   \n",
            "3                                      0                                0   \n",
            "4                                      0                                0   \n",
            "..                                   ...                              ...   \n",
            "169                                    1                                0   \n",
            "170                                    0                                1   \n",
            "171                                    1                                0   \n",
            "172                                    0                                0   \n",
            "173                                    1                                0   \n",
            "\n",
            "     native hawaiian or pacific islander  white  \n",
            "0                                      0      0  \n",
            "1                                      0      0  \n",
            "2                                      0      0  \n",
            "3                                      0      0  \n",
            "4                                      0      0  \n",
            "..                                   ...    ...  \n",
            "169                                    0      0  \n",
            "170                                    0      0  \n",
            "171                                    0      0  \n",
            "172                                    0      0  \n",
            "173                                    0      0  \n",
            "\n",
            "[174 rows x 13 columns]\n",
            "                   user_id   age     gender  \\\n",
            "0   4vdFVrrYxqb4ofj8RK8xvw  27.0  masculine   \n",
            "1   -D8n6z8XwLYDbzkvEqT3cA  23.0  masculine   \n",
            "2   r2P2tCwggQtDts309fs4dA  36.0  masculine   \n",
            "3   5Z5Pq5tMKzbXjATgXsKFFQ  36.0  masculine   \n",
            "4   h0TO1TaQfdaEwcHne4EGpA  39.0  masculine   \n",
            "..                     ...   ...        ...   \n",
            "91  UuM1kcMYTQXRHjUl13RQHg  39.0  masculine   \n",
            "92  EuGkxiM0fgk6eQ7NzpNjKw  23.0  masculine   \n",
            "93  sErm5Rn-LDEvoQVZPrM7yA  39.0  masculine   \n",
            "94  N1YxlI1isrMfQox0JZ746g  55.0  masculine   \n",
            "95  r4WBKoN5_F6fsyJo8OEMZA  29.0  masculine   \n",
            "\n",
            "                              ethnicity  feminine  masculine  \\\n",
            "0                                 asian         0          1   \n",
            "1   hispanic, latino, or spanish origin         0          1   \n",
            "2                                 asian         0          1   \n",
            "3   hispanic, latino, or spanish origin         0          1   \n",
            "4   native hawaiian or pacific islander         0          1   \n",
            "..                                  ...       ...        ...   \n",
            "91  hispanic, latino, or spanish origin         0          1   \n",
            "92  hispanic, latino, or spanish origin         0          1   \n",
            "93  hispanic, latino, or spanish origin         0          1   \n",
            "94  hispanic, latino, or spanish origin         0          1   \n",
            "95                                asian         0          1   \n",
            "\n",
            "    american indian or alaska native  asian  black or african american  \\\n",
            "0                                  0      1                          0   \n",
            "1                                  0      0                          0   \n",
            "2                                  0      1                          0   \n",
            "3                                  0      0                          0   \n",
            "4                                  0      0                          0   \n",
            "..                               ...    ...                        ...   \n",
            "91                                 0      0                          0   \n",
            "92                                 0      0                          0   \n",
            "93                                 0      0                          0   \n",
            "94                                 0      0                          0   \n",
            "95                                 0      1                          0   \n",
            "\n",
            "    hispanic, latino, or spanish origin  middle eastern or north african  \\\n",
            "0                                     0                                0   \n",
            "1                                     1                                0   \n",
            "2                                     0                                0   \n",
            "3                                     1                                0   \n",
            "4                                     0                                0   \n",
            "..                                  ...                              ...   \n",
            "91                                    1                                0   \n",
            "92                                    1                                0   \n",
            "93                                    1                                0   \n",
            "94                                    1                                0   \n",
            "95                                    0                                0   \n",
            "\n",
            "    native hawaiian or pacific islander  white  \n",
            "0                                     0      0  \n",
            "1                                     0      0  \n",
            "2                                     0      0  \n",
            "3                                     0      0  \n",
            "4                                     1      0  \n",
            "..                                  ...    ...  \n",
            "91                                    0      0  \n",
            "92                                    0      0  \n",
            "93                                    0      0  \n",
            "94                                    0      0  \n",
            "95                                    0      0  \n",
            "\n",
            "[96 rows x 13 columns]\n",
            "[42.27642276422765, 20.181731229076995, 12.91248206599713, 8.512673362027739, 3.204208512673362, 8.321377331420372, 4.591104734576758]\n",
            "                     user_id  group_id\n",
            "0     4vdFVrrYxqb4ofj8RK8xvw       6.0\n",
            "1     7BEIyQzlskOV1OAJ0nbweQ       5.0\n",
            "2     IDwjyXR4zY1fICjuG-1idw       1.0\n",
            "3     PMEK5fq3B_YAMfR8urwBtg       3.0\n",
            "4     8p19CLCYzyrsfoqLCksNVA       5.0\n",
            "...                      ...       ...\n",
            "2086  03_OcS8SbBcO4RTx_jYAUw       0.0\n",
            "2087  vpCdKYNXUjs5o-tVSmwGAg       0.0\n",
            "2088  vVBNWuQO9m5Utm69PGoRCw       0.0\n",
            "2089  h7zHYM8LbPWmXx0ZEqOOAw       0.0\n",
            "2090  4wbMeUS9tp2QGqyNhj3YTg       0.0\n",
            "\n",
            "[2091 rows x 2 columns]\n",
            "[42.27642276422765, 20.181731229076995, 12.91248206599713, 8.512673362027739, 3.204208512673362, 8.321377331420372, 4.591104734576758]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.148375\n",
            "1        1  0.156767\n",
            "2        2  0.153703\n",
            "3        3  0.168589\n",
            "4        4  0.163348\n",
            "5        5  0.173539\n",
            "6        6  0.174286\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.154563\n",
            "1        1  0.162192\n",
            "2        2  0.159272\n",
            "3        3  0.148259\n",
            "4        4  0.151124\n",
            "5        5  0.155251\n",
            "6        6  0.157164\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.155937\n",
            "1        1  0.156459\n",
            "2        2  0.155564\n",
            "3        3  0.153504\n",
            "4        4  0.155675\n",
            "5        5  0.154252\n",
            "6        6   0.16905\n",
            "\n",
            "\n",
            "[0.1483749725580024, 0.1567669291601412, 0.1537031231923891, 0.16858851284681098, 0.1633477648614636, 0.17353945859890268, 0.17428551424719854]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-1c03d74bee18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n\u001b[0;32m---> 28\u001b[0;31m             percents, list_of_attributes)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-101-5d49e5e1212a>\u001b[0m in \u001b[0;36mpipeline3\u001b[0;34m(business_id, method, df_ranking_by_relevance, df_ranking_by_date, df_ranking_by_random, reviews, percents, list_of_attributes)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------Ranking by Yelp filter:------------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   yelp_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_relevance,\n\u001b[0;32m--> 126\u001b[0;31m                                                    reviews, \"yelp_\", list_of_attributes)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------Ranking by Date:------------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-830141ad8d45>\u001b[0m in \u001b[0;36mprint_disparate_impact_exposure\u001b[0;34m(business_id, method, df_ranking, reviews, filename, list_of_attributes)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mexposures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exposure'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mdf_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mcurrent_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisparate_impact_exposure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ranking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbusiness_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mexposures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'group_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mexposures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exposure'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-830141ad8d45>\u001b[0m in \u001b[0;36mdisparate_impact_exposure\u001b[0;34m(df_group, ranking, reviews, business_id)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdmp_sommatory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbusiness_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-830141ad8d45>\u001b[0m in \u001b[0;36mdmp_sommatory\u001b[0;34m(df_group, ranking, business_id, reviews)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#Integrate reviews info for clicks counting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'business_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbusiness_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0museful\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'useful'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mfunny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'funny'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX8ibEsAIxCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline1_yelp(business_id, users, reviews):\n",
        "  ### READ THE RANKING FROM CSV\n",
        "  df_rel_ranking = pd.read_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "  df_date_ranking = pd.read_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "  df_rand_ranking = pd.read_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = df_rel_ranking.rename(columns={'User_Id':'user_id', 'Position':'position'})\n",
        "  df_date_ranking = df_date_ranking.rename(columns={'User_Id':'user_id', 'Position':'position'})\n",
        "  df_rand_ranking = df_rand_ranking.rename(columns={'User_Id':'user_id', 'Position':'position'})\n",
        "\n",
        "  df_rel_ranking = drop_unnamed(df_rel_ranking)\n",
        "  df_date_ranking = drop_unnamed(df_date_ranking)\n",
        "  df_rand_ranking = drop_unnamed(df_rand_ranking)\n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline1(business_id, users, reviews):\n",
        "  ### READ THE RANKING FROM CSV\n",
        "  df_rel_ranking = pd.read_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "  df_date_ranking = pd.read_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "  df_rand_ranking = pd.read_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = filter_user_from_dataset(df_rel_ranking, users)\n",
        "  df_date_ranking = filter_user_from_dataset(df_date_ranking, users)\n",
        "  df_rand_ranking = filter_user_from_dataset(df_rand_ranking, users)\n",
        "\n",
        "  df_rel_ranking = integrate_review_info(df_rel_ranking, reviews, business_id)\n",
        "  df_date_ranking = integrate_review_info(df_date_ranking, reviews, business_id)\n",
        "  df_rand_ranking = integrate_review_info(df_rand_ranking, reviews, business_id)  \n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  df_rel_ranking.to_csv('dataset_rel_ranking_' + business_id + '.csv')\n",
        "  df_date_ranking.to_csv('dataset_date_ranking_' + business_id + '.csv')\n",
        "  df_rand_ranking.to_csv('dataset_rand_ranking_' + business_id + '.csv')\n",
        "  upload_file('dataset_rel_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "  upload_file('dataset_date_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "  upload_file('dataset_rand_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline2(df_ranking_by_relevance, reviews, business, id, N_of_groups, local_list_of_attributes, method):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "\n",
        "  #df_vectors = create_vectors(df_ranking_by_relevance, reviews, business, id, local_list_of_attributes, method)\n",
        "  df_vectors = create_vectors_only_demographics(df_ranking_by_relevance, id, local_list_of_attributes, method)\n",
        "  \n",
        "\n",
        "  # ------ SINGLE ATTRIBUTE ---------\n",
        "  if len(local_list_of_attributes) == 1:\n",
        "    attribute = local_list_of_attributes[0]\n",
        "    df_groups = create_groups_by_percentile(df_ranking_by_relevance, attribute, N_of_groups, id)\n",
        "    method = attribute\n",
        "  \n",
        "  # ------ MULTIPLE ATTRIBUTES ---------\n",
        "  # TO BUILD VECTORS, it saves a csv file\n",
        "  elif method == 'kmeans':\n",
        "    df_groups = create_groups_by_kmeans_clustering(df_ranking_by_relevance,N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'spectral':\n",
        "    df_groups = create_groups_by_spectral_clustering(df_ranking_by_relevance, N_of_groups, id)\n",
        "  elif method == 'dbscan':\n",
        "    df_groups = create_groups_by_dbscan_clustering(df_ranking_by_relevance,N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'balanced_kmeans':\n",
        "    df_groups = create_groups_by_balanced_kmeans_clustering(N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'custom':\n",
        "    df_groups = create_groups_custom(N_of_groups,local_list_of_attributes,id)\n",
        "  \n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "  \n",
        "  print(df_groups)\n",
        "  print(percents)\n",
        "  return df_groups, percents\n",
        "\n",
        "\n",
        "def pipeline3(business_id, method, df_ranking_by_relevance, df_ranking_by_date, \n",
        "              df_ranking_by_random, reviews, percents, list_of_attributes):\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  print(\"----------------DEMOGRAPHIC PARITY EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                       \"random_\", list_of_attributes)\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_demgr_' + method + '_' + business_id, list_of_attributes, method, business_id)\n",
        "\n",
        "  print(\"----------------DISPARATE IMPACT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                   reviews, \"yelp_\", list_of_attributes)\n",
        "  \n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                   reviews, \"date_\", list_of_attributes)\n",
        "  \n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                     reviews, \"random_\", list_of_attributes)\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_dispimp_' + method + '_' + business_id, list_of_attributes, method, business_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDtdqvj_BUBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_custom(N_of_groups,list_of_attributes,id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  print('df_vectors len =', len(df_vectors.index))\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "  df_no_info = df_vectors[df_vectors['age'].isnull()]['user_id']\n",
        "  df_yes_info = df_vectors[df_vectors['age'].notnull()]\n",
        "  print('1209 =',len(df_yes_info.index))\n",
        "  print('883 =',len(df_no_info.index))\n",
        "\n",
        "  # ASSIGN GROUP ID\n",
        "  bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "  b_under_39 = bianchi[(bianchi['age']<39)]\n",
        "  b_over_39 = bianchi[(bianchi['age']>=39)]\n",
        "  b_fem = bianchi[bianchi['gender']=='feminine'].reset_index(drop=True)\n",
        "  b_mas = bianchi[bianchi['gender']=='masculine'].reset_index(drop=True)\n",
        "  \n",
        "  neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "  n_under_39 = neri[(neri['age']<39)]\n",
        "  n_over_39 = neri[(neri['age']>=39)]\n",
        "  n_fem = neri[neri['gender']=='feminine'].reset_index(drop=True)\n",
        "  n_mas = neri[neri['gender']=='masculine'].reset_index(drop=True)\n",
        "  \n",
        "  altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "  a_under_39 = altri[(altri['age']<39)]\n",
        "  a_over_39 = altri[(altri['age']>=39)]\n",
        "  a_fem = altri[altri['gender']=='feminine'].reset_index(drop=True)\n",
        "  a_mas = altri[altri['gender']=='masculine'].reset_index(drop=True)\n",
        "\n",
        "  print(b_fem)\n",
        "  print(b_mas)\n",
        "  print(n_fem)\n",
        "  print(n_mas)\n",
        "  print(a_fem)\n",
        "  print(a_mas)\n",
        "\n",
        "  new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in b_fem.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "    \n",
        "  for j, user in b_mas.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "  \n",
        "  for j, user in n_fem.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "    \n",
        "  for j, user in n_mas.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 4\n",
        "    \n",
        "  for j, user in a_fem.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 5\n",
        "    \n",
        "  for j, user in a_mas.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 6\n",
        "\n",
        "  # ADD THE EXCLUDED IN LAST C\n",
        "  new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "  new_df_users = drop_unnamed(new_df_users)\n",
        "  new_df_users = new_df_users.fillna(0)\n",
        "\n",
        "  destination = set_file_destination(local_list_of_attributes, 'custom', id)\n",
        "\n",
        "  new_df_users.to_csv('groups_custom_' + id + '.csv')\n",
        "  upload_file('groups_custom_' + id + '.csv',destination)\n",
        "  return new_df_users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_c2w7U3DNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_balanced_kmeans_clustering(N_of_groups, list_of_attributes, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "  df_no_info = df_vectors[df_vectors['age'].isnull()]['user_id']\n",
        "  df_yes_info = df_vectors[df_vectors['age'].notnull()]\n",
        "  number_of_users = len(df_yes_info.index)\n",
        "  five_percent = (number_of_users*5)//100\n",
        "  one_cluster_size = (number_of_users//(N_of_groups-1))-five_percent\n",
        "  clusters_size = [one_cluster_size]*(N_of_groups-1)\n",
        "\n",
        "  list_vectors = df_yes_info[list_of_attributes].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0 \n",
        "\n",
        "  # #############################################################################\n",
        "  # COMPUTE BALANCED KMEANS WITH MIN_FLOW\n",
        "  # see https://adared.ch/constrained-k-means-implementation-in-python/\n",
        "  (centroids, labels, f) = constrained_kmeans(X, clusters_size)\n",
        "  print('Centroids: ', centroids)\n",
        "  print('labels: ', labels)\n",
        "\n",
        "  # Save centroids\n",
        "  temp_df = pd.DataFrame(data=centroids)\n",
        "  temp_df.columns = list_of_attributes\n",
        "  destination = set_file_destination(local_list_of_attributes, 'balanced_kmeans', id)\n",
        "  temp_df.to_csv(\"centroids_balanced_kmeans_\" + id + \".csv\", float_format=\"%.3f\")\n",
        "  upload_file(\"centroids_balanced_kmeans_\" + id + \".csv\",destination)\n",
        "\n",
        "  new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[j, 'group_id'] = labels[j]\n",
        "  \n",
        "  # ADD THE EXCLUDED IN LAST C\n",
        "  new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "  new_df_users = new_df_users.fillna(N_of_groups-1)\n",
        "\n",
        "  new_df_users.to_csv('groups_balanced_kmeans_' + id + '.csv')\n",
        "  upload_file('groups_balanced_kmeans_' + id + '.csv',destination)\n",
        "  return new_df_users\n",
        "  # #############################################################################\n",
        "  # BALANCED KMEANS WITH PYTORCH DOESN'T WORK\n",
        "  # see https://github.com/giannisdaras/balanced_kmeans/tree/master\n",
        "  #N = len(df_users.index)\n",
        "  #cluster_size = N // N_of_groups\n",
        "  #choices, centers = kmeans_equal(X, num_clusters=N_of_groups, cluster_size=cluster_size)\n",
        "  #print(centers)\n",
        "  #print(choices)\n",
        "\n",
        "\n",
        "def constrained_kmeans(data, demand, maxiter=None, fixedprec=1e9):\n",
        "\tdata = np.array(data)\n",
        "\t\n",
        "\tmin_ = np.min(data, axis = 0)\n",
        "\tmax_ = np.max(data, axis = 0)\n",
        "\t\n",
        "\tC = min_ + np.random.random((len(demand), data.shape[1])) * (max_ - min_)\n",
        "\tM = np.array([-1] * len(data), dtype=np.int)\n",
        "\t\n",
        "\titercnt = 0\n",
        "\twhile True:\n",
        "\t\titercnt += 1\n",
        "\t\tprint(itercnt)\n",
        "\t\t# memberships\n",
        "\t\tg = nx.DiGraph()\n",
        "\t\tg.add_nodes_from(range(0, data.shape[0]), demand=-1) # points\n",
        "\t\tfor i in range(0, len(C)):\n",
        "\t\t\tg.add_node(len(data) + i, demand=demand[i])\n",
        "\t\t\n",
        "\t\t# Calculating cost...\n",
        "\t\tcost = np.array([np.linalg.norm(np.tile(data.T,\n",
        "                                          len(C)).T - np.tile(C, len(data)).reshape(len(C) * len(data),\n",
        "                                                                                            C.shape[1]), axis=1)])\n",
        "\t\t# Preparing data_to_C_edges...\n",
        "\t\tdata_to_C_edges = np.concatenate((np.tile([range(0, data.shape[0])], len(C)).T,\n",
        "                                    np.tile(np.array([range(data.shape[0], data.shape[0] + C.shape[0])]).T,\n",
        "                                            len(data)).reshape(len(C) * len(data), 1), cost.T * fixedprec),\n",
        "                                   axis=1).astype(np.uint64)\n",
        "\t\t# Adding to graph\n",
        "\t\tg.add_weighted_edges_from(data_to_C_edges)\n",
        "\t\t\n",
        "\n",
        "\t\ta = len(data) + len(C)\n",
        "\t\tg.add_node(a, demand=len(data)-np.sum(demand))\n",
        "\t\tC_to_a_edges = np.concatenate((np.array([range(len(data), len(data) + len(C))]).T, np.tile([[a]], len(C)).T), axis=1)\n",
        "\t\tg.add_edges_from(C_to_a_edges)\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t# Calculating min cost flow...\n",
        "\t\tf = nx.min_cost_flow(g)\n",
        "\t\t\n",
        "\t\t# assign\n",
        "\t\tM_new = np.ones(len(data), dtype=np.int) * -1\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tp = sorted(f[i].items(), key=lambda x: x[1])[-1][0]\n",
        "\t\t\tM_new[i] = p - len(data)\n",
        "\t\t\t\n",
        "\t\t# stop condition\n",
        "\t\tif np.all(M_new == M):\n",
        "\t\t\t# Stop\n",
        "\t\t\treturn (C, M, f)\n",
        "\t\t\t\n",
        "\t\tM = M_new\n",
        "\t\t\t\n",
        "\t\t# compute new centers\n",
        "\t\tfor i in range(len(C)):\n",
        "\t\t\tC[i, :] = np.mean(data[M==i, :], axis=0)\n",
        "\t\t\t\n",
        "\t\tif maxiter is not None and itercnt >= maxiter:\n",
        "\t\t\t# Max iterations reached\n",
        "\t\t\treturn (C, M, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQKajNw47W7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_dbscan_clustering(df_users, N_of_groups, list_of_attributes, id):\n",
        "  # #############################################################################\n",
        "  # Generate data\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  list_vectors = df_vectors[list_of_attributes].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0\n",
        "\n",
        "  # #############################################################################\n",
        "  # Compute DBSCAN\n",
        "  i = 0\n",
        "  while i<21:\n",
        "    db = DBSCAN(eps=2, min_samples=i).fit(X)\n",
        "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "    core_samples_mask[db.core_sample_indices_] = True\n",
        "    labels = db.labels_\n",
        "\n",
        "    # Number of clusters in labels, ignoring noise if present.\n",
        "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise_ = list(labels).count(-1)\n",
        "\n",
        "    print('Estimated number of clusters: %d' % n_clusters_)\n",
        "    print('Estimated number of noise points: %d' % n_noise_)\n",
        "    i = i+1\n",
        "  #print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
        "  #print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
        "  #print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
        "  #print(\"Adjusted Rand Index: %0.3f\"\n",
        "  #      % metrics.adjusted_rand_score(labels_true, labels))\n",
        "  #print(\"Adjusted Mutual Information: %0.3f\"\n",
        "  #      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
        "  print(\"Silhouette Coefficient: %0.3f\"\n",
        "        % metrics.silhouette_score(X, labels))\n",
        "  \n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[j, 'group_id'] = labels[j]\n",
        "  new_df_users.to_csv('groups_dbscan_' + id + '.csv')\n",
        "  destination = set_file_destination(local_list_of_attributes, 'dbscan', id)\n",
        "  upload_file('groups_dbscan_' + id + '.csv',destination)\n",
        "  \n",
        "  #SEARCH FOR CLUSTER DESCRIPTION AND SAME SIZE CLUSTERING\n",
        "\n",
        "  # #############################################################################\n",
        "  # Plot result\n",
        "\n",
        "  # Black removed and is used for noise instead.\n",
        "  unique_labels = set(labels)\n",
        "  colors = [plt.cm.Spectral(each)\n",
        "            for each in np.linspace(0, 1, len(unique_labels))]\n",
        "  for k, col in zip(unique_labels, colors):\n",
        "      if k == -1:\n",
        "          # Black used for noise.\n",
        "          col = [0, 0, 0, 1]\n",
        "\n",
        "      class_member_mask = (labels == k)\n",
        "\n",
        "      xy = X[class_member_mask & core_samples_mask]\n",
        "      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "              markeredgecolor='k', markersize=14)\n",
        "\n",
        "      xy = X[class_member_mask & ~core_samples_mask]\n",
        "      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "              markeredgecolor='k', markersize=6)\n",
        "\n",
        "  plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "  plt.show()\n",
        "  ####################################################################\n",
        "  return new_df_users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRs30-Z9heM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_kmeans_clustering(df_users, N_of_groups, list_of_attributes, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  #if(list_of_attributes == ['age', 'gender', 'ethnicity']):\n",
        "    #age, feminine, masculine, asian, black, hispanic, arabs, hawaiian, white\n",
        "    #init_centroids = np.array([\n",
        "    #                 [20, 1, -10, -10, -10, -10, -10, -10, 1],\n",
        "    #                 [60, -10, 1, -10, -10, -10, -10, -10, 1],\n",
        "    #                 [39, 0, 0, -10, -10, 1, 1, 1, -30],\n",
        "    #                 [60, 0, 0, 1, -10, -10, -10, -10, -30],\n",
        "    #                 [20, 0, 0, -10, 1, -10, -10, -10, -30],\n",
        "    #                 [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
        "    #                np.float64)\n",
        "\n",
        "  # FOR NOW WE EXCLUDE TEXTUAL ATTRIBUTES\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  list_vectors = df_vectors[list_of_attributes].values.tolist()\n",
        "  # ex. ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']\n",
        "  # ex. ['loc1', 'loc2', 'loc3']\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0\n",
        "  kmeans = KMeans(n_clusters=N_of_groups, random_state=0).fit(X) #init=init_centroids\n",
        "  labels = kmeans.labels_\n",
        "  # kmeans.predict([[0, 0, 20], [12, 3, 5]]))\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  temp_df = pd.DataFrame(data=centroids)\n",
        "  temp_df.columns = list_of_attributes\n",
        "  destination = set_file_destination(local_list_of_attributes, 'kmeans', id)\n",
        "  temp_df.to_csv(\"centroids_kmeans_\" + id + \".csv\", float_format=\"%.3f\")\n",
        "  upload_file(\"centroids_kmeans_\" + id + \".csv\",destination)\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = labels[i]\n",
        "  new_df_users.to_csv('groups_kmeans_' + id + '.csv')\n",
        "  upload_file('groups_kmeans_' + id + '.csv',destination)\n",
        "  return new_df_users\n",
        "\n",
        "\n",
        "def generate_dummies(df, text_attribute_list):\n",
        "  dummy_columns = []\n",
        "  for attr in text_attribute_list:\n",
        "    gender_dummies = pd.get_dummies(df[attr])\n",
        "    dummy_columns = dummy_columns + list(gender_dummies.columns)\n",
        "    df = pd.merge(df, gender_dummies, how=\"left\",left_index=True, right_index=True)\n",
        "    \n",
        "    # drop all the unnamed columns\n",
        "    df = drop_unnamed(df)\n",
        "  return df, dummy_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU_zGBsWzAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_spectral_clustering(df_users, N_of_groups, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['top_location']\n",
        "  df_vectors = one_hot_encoding(df_vectors, text_attribute_list)\n",
        "  \n",
        "  list_vectors = df_vectors[['review_count', 'fans', 'average_stars', 'top_location',\n",
        "                             'useful', 'funny', 'cool']].values.tolist()\n",
        "  # Convert list of list in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  mat = cosine_similarity(list_of_arrays)\n",
        "  group_array = SpectralClustering(n_clusters=N_of_groups, affinity='precomputed').fit_predict(mat)\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = group_array[i]\n",
        "  new_df_users.to_csv('groups_spectral_' + id + '.csv')\n",
        "  return new_df_users\n",
        "\n",
        "def one_hot_encoding(df, text_attribute_list):\n",
        "  for attr in text_attribute_list:\n",
        "    text = list(df[attr])\n",
        "    vectorizer = CountVectorizer().fit_transform(text)\n",
        "    embeddings = vectorizer.toarray()\n",
        "    for i, user in df.iterrows():\n",
        "      df.at[i, attr] = embeddings[i]\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awAE4AWWxEFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_percentile(df_users, attribute, N_of_groups, business_id):\n",
        "  values = df_users[attribute].tolist()\n",
        "\n",
        "  print(df_users)\n",
        "  print(len(df_users[df_users['fans']<=3].index))\n",
        "  # Create array of percents to define limit of group \n",
        "  scale = 100/N_of_groups\n",
        "  percents = []\n",
        "  current_percent = 0\n",
        "  while current_percent < 100:\n",
        "    current_percent = current_percent + scale\n",
        "    '''if current_percent > 100:\n",
        "      current_percent = 100\n",
        "      percents.append(current_percent)\n",
        "      break'''\n",
        "    percents.append(current_percent)\n",
        "  # ex. percents = [20, 40, 60, 80, 100]\n",
        "  print(percents)\n",
        "\n",
        "  # Create array with limit values (max) of each group\n",
        "  i = 0\n",
        "  limit_values = []\n",
        "  while i < len(percents):\n",
        "    limit_values.append(np.percentile(values, percents[i]))\n",
        "    i = i + 1\n",
        "  # ex. limit_values = [5.0, 13.0, 27.0, 73.0, 10022.0]\n",
        "  print(limit_values)\n",
        "  with open('descr_groups_' + attribute + '_' + business_id + '.txt', 'w') as output:\n",
        "    output.write(str(percents) + '\\n' + str(limit_values))\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  \n",
        "  for i, user in df_users.iterrows():\n",
        "    id_group = 0\n",
        "\n",
        "    if (user[attribute] >= 0) and (user[attribute] <= limit_values[id_group]):\n",
        "      new_df_users.loc[i, 'group_id'] = id_group\n",
        "    id_group = id_group + 1\n",
        "    while id_group < len(percents):\n",
        "      if (user[attribute] > limit_values[id_group - 1]) and (user[attribute] <= limit_values[id_group]):\n",
        "        new_df_users.loc[i, 'group_id'] = id_group\n",
        "      id_group = id_group + 1\n",
        "  \n",
        "  destination = set_file_destination([attribute], '', business_id)\n",
        "  new_df_users.to_csv('groups_' + attribute + '_' + business_id + '.csv')\n",
        "  upload_file('groups_' + attribute + '_' + business_id + '.csv',destination)\n",
        "  return new_df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbpLTiPSWuL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_disparate_impact_exposure(business_id, method,  df_ranking, reviews,\n",
        "                                    filename, list_of_attributes):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp = disparate_impact_exposure(df_groups[df_groups['group_id'] == i], df_ranking, reviews, business_id)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  exposures.to_csv('exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures\n",
        "\n",
        "\n",
        "def print_demographic_parity_exposure(business_id, method, df_ranking, filename, list_of_attributes):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp = demographic_parity_exposure(df_groups[df_groups['group_id'] == i], df_ranking)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  exposures.to_csv('exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures\n",
        "\n",
        "\n",
        "def disparate_impact_exposure(df_group, ranking, reviews, business_id):\n",
        "    if len(df_group.index) == 0:\n",
        "      return 0\n",
        "    return dmp_sommatory(df_group, ranking, business_id, reviews)/(len(df_group.index))\n",
        "\n",
        "\n",
        "def demographic_parity_exposure(df_group, ranking):\n",
        "    if len(df_group.index) == 0:\n",
        "      return 0\n",
        "    return demgr_sommatory(df_group, ranking)/(len(df_group.index))\n",
        "\n",
        "\n",
        "def dmp_sommatory(df_group, ranking, business_id, reviews):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "\n",
        "        #Integrate reviews info for clicks counting\n",
        "        df_temp = reviews[(reviews['business_id'] == business_id) & (reviews['user_id'] == user_id)]\n",
        "        useful = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['useful']\n",
        "        funny = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['funny']\n",
        "        cool = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['cool']\n",
        "        counts = useful + funny + cool + 1\n",
        "        base = 2  # con 10 i valori sono troppo bassi\n",
        "        counts = math.log(counts, base) \n",
        "\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        addend = exp(position) * counts\n",
        "        sum = sum + addend\n",
        "    return sum\n",
        "\n",
        "\n",
        "def demgr_sommatory(df_group, ranking):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        sum = sum + exp(position)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/np.log(1 + position)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPGOC05EljLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_plots(yelp_exposures, date_exposures, random_exposures, percents, title,\n",
        "              list_of_attributes, method, business_id):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  print(y1)\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='yelp')\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date')\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0),this_range)\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Iq3ciMMcZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO UPDATE CENTROIDS\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg'] \n",
        "method = 'kmeans'\n",
        "for id in id_list:\n",
        "  pd.options.display.float_format = '{:.3f}'.format\n",
        "  centroids = pd.read_csv('centroids_' + method + '_' + id + '.csv')\n",
        "  values = [float(x) for x in centroids.columns.values]\n",
        "  centroids.loc[-1] = values # adding a row\n",
        "  centroids.index = centroids.index + 1  # shifting index\n",
        "  centroids.sort_index(inplace=True)\n",
        "  centroids.columns = ['loc1', 'loc2', 'loc3']  \n",
        "  centroids.to_csv('centroids_' + method + '_' + id + '.csv', float_format='%.3f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22E0H3eY8Vb3",
        "colab_type": "code",
        "outputId": "5116c13a-fb95-4c8f-a24f-ef34ddfd0c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TO UPDATE EXPOSURE\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA']\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg'] #RICALCOLARE DA CAPO\n",
        "# id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "for id in id_list:\n",
        "  df_rel_ranking = pd.read_csv('dataset_rel_ranking_'+id+'.csv')\n",
        "  df_date_ranking = pd.read_csv('dataset_date_ranking_'+id+'.csv')\n",
        "  df_rand_ranking = pd.read_csv('dataset_rand_ranking_'+id+'.csv')\n",
        "\n",
        "  method = 'kmeans'\n",
        "  N_of_groups = 5\n",
        "  df_groups = pd.read_csv('groups_' + method + '_'+id+'.csv')\n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews, percents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[92.94037011651817, 0.06854009595613435, 1.0281014393420151, 5.68882796435915, 0.2741603838245374]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.162933\n",
            "1        1  0.220105\n",
            "2        2  0.240493\n",
            "3        3  0.200643\n",
            "4        4  0.185087\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.16376\n",
            "1        1  0.152814\n",
            "2        2  0.172153\n",
            "3        3  0.201048\n",
            "4        4  0.169292\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.16606\n",
            "1        1  0.156778\n",
            "2        2  0.165594\n",
            "3        3  0.164798\n",
            "4        4  0.165478\n",
            "\n",
            "\n",
            "[0.16293274380030887, 0.2201045822301589, 0.2404932137794746, 0.20064345885486787, 0.1850865033952374]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.070566\n",
            "1        1   1.11977\n",
            "2        2  0.714228\n",
            "3        3  0.329566\n",
            "4        4  0.366383\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0740012\n",
            "1        1   0.777435\n",
            "2        2   0.466203\n",
            "3        3    0.39857\n",
            "4        4   0.300483\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0673801\n",
            "1        1   0.797605\n",
            "2        2   0.473072\n",
            "3        3    0.28051\n",
            "4        4   0.303709\n",
            "\n",
            "\n",
            "[0.0705660245314008, 1.1197738832848634, 0.714228081159378, 0.3295657039859884, 0.3663834882008638]\n",
            "[93.1098696461825, 0.0931098696461825, 0.74487895716946, 0.186219739292365, 5.865921787709497]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.173895\n",
            "1        1  0.621335\n",
            "2        2   0.21503\n",
            "3        3  0.228349\n",
            "4        4  0.183252\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.173915\n",
            "1        1  0.144723\n",
            "2        2  0.182823\n",
            "3        3  0.171159\n",
            "4        4  0.196404\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.175771\n",
            "1        1  0.151612\n",
            "2        2  0.170644\n",
            "3        3  0.167176\n",
            "4        4  0.168518\n",
            "\n",
            "\n",
            "[0.17389513349465582, 0.6213349345596119, 0.2150297612553226, 0.2283487263806268, 0.18325220317761595]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0948298\n",
            "1        1    3.07822\n",
            "2        2   0.537432\n",
            "3        3   0.469229\n",
            "4        4   0.283128\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.10486\n",
            "1        1  0.716986\n",
            "2        2  0.406378\n",
            "3        3  0.326073\n",
            "4        4  0.334458\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0938564\n",
            "1        1   0.751116\n",
            "2        2   0.378691\n",
            "3        3   0.318491\n",
            "4        4   0.263216\n",
            "\n",
            "\n",
            "[0.09482980330121404, 3.0782152403097, 0.5374321645605649, 0.46922881898353463, 0.28312772973154576]\n",
            "[93.4054054054054, 0.10810810810810811, 0.9729729729729729, 5.297297297297297, 0.21621621621621623]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.176476\n",
            "1        1  0.200383\n",
            "2        2  0.208942\n",
            "3        3  0.228386\n",
            "4        4  0.460329\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.178995\n",
            "1        1  0.234594\n",
            "2        2  0.224002\n",
            "3        3   0.19136\n",
            "4        4  0.194405\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.180793\n",
            "1        1  0.166559\n",
            "2        2  0.185239\n",
            "3        3   0.16935\n",
            "4        4  0.165265\n",
            "\n",
            "\n",
            "[0.17647606219677034, 0.20038343021591398, 0.20894189258758333, 0.22838557398304843, 0.4603293537434925]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.114704\n",
            "1        1   1.01081\n",
            "2        2  0.727906\n",
            "3        3  0.548388\n",
            "4        4   2.27585\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.124164\n",
            "1        1   1.18339\n",
            "2        2  0.761529\n",
            "3        3  0.483666\n",
            "4        4  0.957387\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.111639\n",
            "1        1  0.840188\n",
            "2        2  0.633305\n",
            "3        3  0.414278\n",
            "4        4  0.814873\n",
            "\n",
            "\n",
            "[0.11470445783168728, 1.0108129969980315, 0.7279060122864212, 0.5483878282837408, 2.2758477067956013]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FcLo9mPIeHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO UPDATE PLOTS\n",
        "id = 'WbJ1LRQdOuYYlRLyTkuuxw'\n",
        "grouping = 'review_count'\n",
        "exp = 'demgr'\n",
        "y = pd.read_csv('exp_' + exp + '_' + grouping + '_yelp_' + id + '.csv')\n",
        "d = pd.read_csv('exp_' + exp + '_' + grouping + '_date_' + id + '.csv')\n",
        "r = pd.read_csv('exp_' + exp + '_' + grouping + '_random_' + id + '.csv')\n",
        "get_plots(y,d,r, [93.42, 0.17, 0.003, 5.81, 0.188], \"title\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40JdFbw0Qqmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FILTER RESTAURANTS\n",
        "is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "restaurants = business[is_restaurant]\n",
        "\n",
        "# Filter closed restaurants\n",
        "restaurants = restaurants[restaurants['is_open'] == 1]\n",
        "\n",
        "# Order restaurants by review_count\n",
        "restaurants = restaurants.sort_values('review_count')\n",
        "\n",
        "restaurants.to_csv('restaurants_input.csv')\n",
        "# NOW CHOOSE THE RESTAURANTS AND SAVE THEIR IDS INTO TXT FILE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUr0D9xWp5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_size(df_users, attribute, range):\n",
        "    # order df_users by review_count\n",
        "    df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "    group_list = [[]]\n",
        "    i = 0\n",
        "    group_index = 0\n",
        "    prec = -1\n",
        "    for index, user in df_users.iterrows():\n",
        "        if i == range:\n",
        "            i = 0\n",
        "            group_list.append([])\n",
        "            group_index = group_index + 1\n",
        "        temp = user[attribute]\n",
        "        if temp != prec:\n",
        "            prec = temp\n",
        "            i = i + 1\n",
        "        group_list[group_index].append(user)\n",
        "    print(\"Number of groups created: \" + str(group_index+1))\n",
        "    return group_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYumnpYHecF",
        "colab_type": "code",
        "outputId": "e54f518e-730b-47d2-c8dd-a165910cde70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "### UNIQUE VALUE OF state, city AND categories OF BUSINESS\n",
        "# To filter only RESTAURANTS\n",
        "# is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "# restaurants = business[is_restaurant]\n",
        "\n",
        "print('UNIQUE VALUE OF \\'state\\' ATTRIBUTE IN RESTAURANTS = ', business['state'].nunique())\n",
        "print('UNIQUE VALUE OF \\'city\\' ATTRIBUTE IN RESTAURANTS = ', business['city'].nunique())\n",
        "df_cat = business[['categories']]\n",
        "distinct_categories_list = []\n",
        "for index, row in df_cat.iterrows():\n",
        "  if row['categories'] != None:\n",
        "    lst = [item.strip() for item in row['categories'].split(',')]\n",
        "    distinct_categories_list = list(set(distinct_categories_list + lst))\n",
        "print('UNIQUE VALUE OF \\'categories\\' ATTRIBUTE distinct values ', len(distinct_categories_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIQUE VALUE OF 'state' ATTRIBUTE IN RESTAURANTS =  36\n",
            "UNIQUE VALUE OF 'city' ATTRIBUTE IN RESTAURANTS =  1204\n",
            "UNIQUE VALUE OF 'categories' ATTRIBUTE distinct values  1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05AeyBn2gGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRINT THE state OF EACH user FOR HIS RESTAURANT reviews\n",
        "def statistics(users, reviews, business):\n",
        "  res = pd.DataFrame()\n",
        "  cont = 0\n",
        "  prec_user_id = '0'\n",
        "\n",
        "  for i, user in users.iterrows():\n",
        "    if cont == 1000:\n",
        "      break\n",
        "    user_id = user['user_id']  # 4, 9, 16, 21!, 22!, 23, 24\n",
        "    # Get all reviews of one user\n",
        "    # 'https://www.yelp.com/user_details_reviews_self?userid=NQffx45eJaeqhFcMadKUQA&rec_pagestart=90'??? no\n",
        "    user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "    # Get all restaurants\n",
        "    business_ids = user_reviews[['business_id']]\n",
        "    result = business_ids.merge(business)\n",
        "    is_restaurant = result['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "    user_restaurant_reviews = result[is_restaurant]\n",
        "    user_restaurant_reviews['user_id'] = user_id\n",
        "    #user_restaurant_reviews.to_csv('user_restaurant_reviews' + str(cont) + '.csv')\n",
        "    res = res.append(user_restaurant_reviews.groupby(['user_id', 'state'])['user_id'].count().reset_index(name=\"review_count\"))\n",
        "    if (user_id != prec_user_id) and not user_restaurant_reviews.empty:\n",
        "      cont = cont + 1\n",
        "    # print(res)\n",
        "    prec_user_id = user_id\n",
        "\n",
        "  res.to_csv('result.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0M0ETqjMrqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_df_users(ranking):\n",
        "  df_ranking = pd.DataFrame(ranking, columns=['position', 'user_id', 'review_count', 'date']).sort_values('position')\n",
        "  return df_ranking"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6gUlnqBZ9eoa",
        "colab": {}
      },
      "source": [
        "# TODO: get review_count from web page\n",
        "def get_ranking(url):\n",
        "\n",
        "  i=1\n",
        "  reviews_text = []\n",
        "  reviews_date = []\n",
        "  reviews_userid = []\n",
        "  x=0\n",
        "\n",
        "  while 1:\n",
        "    if x == 0:\n",
        "      page_content = requests.get(url)\n",
        "    else:\n",
        "        page_content = requests.get(url + '?start=' + str(x))\n",
        "    x = x + 20\n",
        "    \n",
        "    tree = html.fromstring(page_content.content)\n",
        "    recommended_reviews_text = \"Recommended Reviews\"\n",
        "    reviews_list = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/*' % recommended_reviews_text)\n",
        "    if not reviews_list:\n",
        "      break\n",
        "    else:\n",
        "      # Index for ranking\n",
        "      j=1\n",
        "      for review in reviews_list:\n",
        "          reviews_text.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[p]/p/span' % (recommended_reviews_text, j))[0].text))\n",
        "          reviews_date.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[1]/div/div[2]/span' % (recommended_reviews_text, j))[0].text))\n",
        "          user_link = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div/div/div/div/div/div/a/@href' % (recommended_reviews_text, j))[0]\n",
        "          reviews_userid.append((i,user_link[user_link.find('=')+1:]))\n",
        "          i = i + 1\n",
        "          j = j + 1\n",
        "\n",
        "\n",
        "  print(reviews_text)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_text)))\n",
        "  print(reviews_date)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_date)))\n",
        "  print(reviews_userid)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_userid)))\n",
        "  return reviews_userid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHl6bORqNzeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_users_in_dataset(users):\n",
        "    data = []\n",
        "    with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            data.append(json_data)\n",
        "    df = pd.DataFrame(data)\n",
        "    df_ranking = pd.DataFrame(users, columns=['Position', 'user_id'])\n",
        "    df_merged = df_ranking.merge(df, on='user_id')\n",
        "    # temp = df[df['user_id'].isin(users)]\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def exists_in_dataset(userid):\n",
        "  found = False\n",
        "  with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            if json_data['user_id'] == userid:\n",
        "              found = True\n",
        "  if found:\n",
        "    print(\"Trovato\")\n",
        "  else:\n",
        "    print(\"Non trovato\")\n",
        "  return found\n",
        "\n",
        "\n",
        "def find_users_in_chopped_dataset(users):\n",
        "    found = False\n",
        "    data = []\n",
        "    i = 1\n",
        "    j = 100000\n",
        "    df_users = pd.DataFrame()\n",
        "    while not found:\n",
        "        path = \"user\" + str(i) + \"-\" + str(j) + \".json\"\n",
        "        try:\n",
        "            with open(path, 'r') as file:\n",
        "                for line in file:\n",
        "                    json_data = json.loads(line)\n",
        "                    data.append(json_data)\n",
        "            df = pd.DataFrame(data)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Cannot find all users into the dataset\")\n",
        "            return 0\n",
        "        else:\n",
        "            temp = df[df['user_id'].isin(users)]  # df of users of the ranking\n",
        "            df_users = df_users.append(temp)\n",
        "            if df_users.shape[0] == len(users):\n",
        "                found = True\n",
        "            else:\n",
        "                i = i + 100000\n",
        "                j = j + 100000\n",
        "    return df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}