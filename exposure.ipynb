{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exposure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/exposure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPpCkgv0271l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import datetime\n",
        "import math\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#!pip install balanced_kmeans\n",
        "#from balanced_kmeans import kmeans\n",
        "#from balanced_kmeans import kmeans_equal\n",
        "\n",
        "import networkx as nx\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkoXGrGIEQB3",
        "colab_type": "code",
        "outputId": "52c07dbf-9541-45a7-9f3d-2deb9748e833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COSfvffRFRRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "root_path = 'gdrive/My Drive/Tesi/Fairness/data/fairness_data/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7PR_gGjwNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONSTANTS\n",
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n",
        "FOLDER = {'WbJ1LRQdOuYYlRLyTkuuxw':\n",
        "            {'FID_RANKING': '1U0Ml5puObfOk3qHCKhO9bQ06WyhJoffW',\n",
        "             'FID_G1': '1cauvqKXzF5VhiveP84aL3U0oDrXP32XV',\n",
        "             'FID_G2': '1vC1fc2owpCis_PTl1ABKT6BfG6-2yoiJ',\n",
        "             'FID_G3': '1_iKbmJkBwWEEIIh73UvR431Ss6xnGb2f',\n",
        "             'FID_G4': '1m_feamMvpx5B5mTe_lH6qlhoZFEpwlSs',\n",
        "             'FID_G5': '1qJgFbR4FUX5t7O9h5cHgp6aE6lpfbdOs',\n",
        "             'FID_G6': '1BSQrghVusO_eNbe10fFE4-bVfRK8ly0y',\n",
        "             'FID_G7': '1fJD4WIErH-u5plesyTePCAQ3IyfjoOKf',\n",
        "             'FID_G8': '1fD_fdl-1KAzXCjPOsEvFOZS3u354XhLL',\n",
        "             'FID_G9': '1ZdiaS2cZP9Ks3rheSSVaoLpw-qMYF9EA',\n",
        "             'FID_G10': '',\n",
        "             'FID_G11': '1ce7VQPdbQ9ieimmpz66RaHgbzEFa4TsT',\n",
        "             'FID_G12': '1tf-KwOYXdMGS3uYdoe8Kc4LV-rdxHY_3'\n",
        "             },\n",
        "          'T2tEMLpTeSMxLKpxwFdS3g':\n",
        "            {'FID_RANKING': '136FZ0Y90Zx-4UYDyW50cX8zUa_DhN3XS',\n",
        "             'FID_G1': '1eiju1aTzsRi4QQZ8XrSouqcLtFSN-tbj',\n",
        "             'FID_G2': '1VuU8MAcXgN_E85q9SH_P1HsHVkeJakwH',\n",
        "             'FID_G3': '1L240ycUElvmF0R4l8PbnuoBbqQY6q1og',\n",
        "             'FID_G4': '1S4oTf-ZSD8q9aTOjrDEA2KeNJ99uh-2n',\n",
        "             'FID_G5': '1u9ZFeCwmol1uXRH-l8djJ2hst0B01VI3',\n",
        "             'FID_G6': '19TgwV3uHOtDDJ0a8krpvLUeooe9rhQFJ',\n",
        "             'FID_G7': '11YpFHw34cDglPD87W8yWvRM1VAOWXPtF',\n",
        "             'FID_G8': '1tR-GotUUNgiilJLBxvr7PkUpbPG5F_J7',\n",
        "             },\n",
        "          'ALwAlxItASeEs2vYAeLXHA':\n",
        "            {'FID_RANKING': '12zXi3XyQaNgukGHW_805cyrkHhSdF7Df',\n",
        "             'FID_G1': '1NB-isOm1cDArAlwIMDMou1Q_XAap09KA',\n",
        "             'FID_G2': '1ELMUBbKGryblHDnbh7ghiIgMdZ48pBG1',\n",
        "             'FID_G3': '1QZ90pwDjBoaU41P8wP8v_I9tsuhObEUP',\n",
        "             'FID_G4': '1MXivbV9VGB0vBGGuLhYu0zJlpv6O-2Xb',\n",
        "             'FID_G5': '1bTetFx3Jhy5QbRKYGV09muMy7D1NKmJT',\n",
        "             'FID_G6': '1F_7bdMS5MW4sIpyUbDz62ZTYJtHoouGG',\n",
        "             'FID_G7': '1uRFceNIlYk5vQXEGd2H6yanrlwrSIM9V',\n",
        "             },\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw':\n",
        "            {'FID_RANKING': '1ZyNQavpG0akr3ca3PJjjl_D89IA5wlcc',\n",
        "             'FID_G1': '1kQkAi_9V3mSld1LlP50j6uw0bwqnkLrR',\n",
        "             'FID_G2': '1hGR4eZiP3Q9Etn7RdlY58IsNsTMMTydz',\n",
        "             'FID_G3': '1YcAPHxioFpiVXWndO5tK17US1h36cWgK',\n",
        "             'FID_G4': '1NxzzH0l18fGaO4bZ35D9NMq2CeW5BRVP',\n",
        "             'FID_G5': '1HC1wZGPtbb8UvRlfbGoHl_ijl3G67Ei4',\n",
        "             'FID_G6': '1J_15pO-CFbHR_0YsHK5kDnKp49kDL0kZ',\n",
        "             'FID_G7': '1RTwCqhYzB3djzpe3-uwbQ3Ur5P8YoECa',\n",
        "             },\n",
        "          'Sovgwq-E-n6wLqNh3X_rXg':\n",
        "            {'FID_RANKING': '1EZqvt9x5PN07BgUad1RtNIUXTVqujA0g',\n",
        "             'FID_G1': '1pkUEBYeZ66GfIvXoTkzZ28gGFlZNWmEq',\n",
        "             'FID_G2': '1lJzHAfBlvL9_EgNizH4vnyPTn-j-Vbrx',\n",
        "             'FID_G3': '1NRYqjUsQJTwgB0JXtcCx7lQErirMH1TK',\n",
        "             'FID_G4': '1nLtZbS0NGXKAs6aETNs7EWasNs8omaw6',\n",
        "             'FID_G5': '14w-jVhOuod3s_EYSipvS92SPx_3zT_FD',\n",
        "             'FID_G6': '1tbUyiamFmyAoeA3W6j51x97h3MjUSVsd',\n",
        "             'FID_G7': '1TNAmibHwJHkvNajHhRffN1gEY0-QnBZH',\n",
        "             },\n",
        "          'j5nPiTwWEFr-VsePew7Sjg':\n",
        "            {'FID_RANKING': '18bQVXYZ03vIpfEFLPh8I2i7cPTlokGxv',\n",
        "             'FID_G1': '1U8STZ7irZLPcUpP1ALR6QYIFnoXB2tbE',\n",
        "             'FID_G2': '1xScKc0_DlnQZucHLeb26hLqpgjqMA3RH',\n",
        "             'FID_G3': '1JFYl2eKtDyWbsyVdBpTXsvPn2h6MO3Cr',\n",
        "             'FID_G4': '1aFYpJmymdnHxquuqbbMwXzN5wG-3Zf2O',\n",
        "             'FID_G5': '1NfHlsBTk87jscmZfagtnd--kcdpy8t_8',\n",
        "             'FID_G6': '1b88afNqrZSRptYA0pL3lCEV49mFaEP6Q',\n",
        "             'FID_G7': '15rDbY-MTUBVjHxOedbenZIjTxQ_wEAZW',\n",
        "             },\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg':\n",
        "            {'FID_RANKING': '1nl6A997UnuR5ceYRZS1242JJCvK-Sf_u',\n",
        "             'FID_G1': '1hrssLlhPmnS48yVxXpwBBIuhL1YdDNhj',\n",
        "             'FID_G2': '1shLPc3aUsVaxeZhZ_SvDS3agPFlMw1dn',\n",
        "             'FID_G3': '13eUtYFVp-KKjxV6-mhCsxQGHkKFOgB82',\n",
        "             'FID_G4': '1nQVPHZT4sHpBXD32qX_x41VywkmyppFj',\n",
        "             'FID_G5': '1gJpfGhb721Q64gynq4jSGMis5pDA7qAx',\n",
        "             'FID_G6': '1WbZvolaSwambWOKHx4v4eFIV-KUzygGF',\n",
        "             'FID_G7': '1IWyQpgMjLny2JTbw0nUuHkNRPV5CCA_A',\n",
        "             },\n",
        "          'e4NQLZynhSmvwl38hC4m-A':\n",
        "            {'FID_RANKING': '1K566Y5Q2N6Lw7S6yDicKC_R-zFVRqnFr',\n",
        "             'FID_G1': '1RNxub2faGfc4NAFvau5SsMADAzBlQd2j',\n",
        "             'FID_G2': '16tjv4k5CJSOxwgvC0wDJU67MEjRaw_0o',\n",
        "             'FID_G3': '1p-9HrVncD_FZOxO0YzHiY7KTe47ugSTQ',\n",
        "             'FID_G4': '1qGpECepAyJjqyfPx8Q94odPOCHbOixtZ',\n",
        "             'FID_G5': '15AGLE1YT0v8tQ3r6i6k7j5XaNORToUct',\n",
        "             'FID_G6': '1nNbAPulqJCzO6urHNsqc4qzSM7Yih8uk',\n",
        "             'FID_G7': '1o9Ew39-Q9VO3wjEyv4qGJ9YlfRmkzTed',\n",
        "             },\n",
        "          'S-oLPRdhlyL5HAknBKTUcQ':\n",
        "            {'FID_RANKING': '1FJqfJgaJinXS3oIvdA53FMuO9O7bCDVn',\n",
        "             'FID_G1': '1CcLHVEA41AgSzJKl37YlcJKf-o5NaCUo',\n",
        "             'FID_G2': '1j-lp1gcu6YGTwcOV4aYiY7dh2G9_TL6J',\n",
        "             'FID_G3': '1I-Qn9oz4VopXbpj5i8_iqLvcSp8jpb2_',\n",
        "             'FID_G4': '1gsS9uFbRyI18ifF5yLp20WHhP66gdr57',\n",
        "             'FID_G5': '1LJIVm9tCZMcgzBkGilYem0nqKoUHpdpX',\n",
        "             'FID_G6': '1syB0wF48fiSo0_uW9dedS-ZlFfXUv1B8',\n",
        "             'FID_G7': '1cBPjBprJ19RxI6-Bcpb-Fifv0s1bQSV2',\n",
        "             }\n",
        "        }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bw0Twhc3qgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upload_file(filename, folder_id):\n",
        "  drive = authenticate()\n",
        "  fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "  drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Check if file already exists in Google Drive (prevents duplicates)\n",
        "  for file in fileList:\n",
        "      if file['title'] == filename:  # The file already exists, then overwrite it\n",
        "          fileID = file['id']\n",
        "          drive_file = drive.CreateFile({'id': fileID, 'title': filename, 'parents': [{'id': folder_id}]})\n",
        "\n",
        "  # Create a local copy of user picture\n",
        "  # Already created!\n",
        "\n",
        "  # Upload user picture on Google Drive\n",
        "  drive_file.SetContentFile(filename)  # path of local file content\n",
        "  drive_file.Upload()  # Upload the file.\n",
        "  \n",
        "  # Delete local user pictures\n",
        "  #if os.path.exists(filename):\n",
        "  #    os.remove(filename)\n",
        "  #else:\n",
        "  #    print(\"The file does not exist\")\n",
        "\n",
        "def set_file_destination(lst, method, id):\n",
        "  if method == 'ranking':\n",
        "    return FOLDER[id]['FID_RANKING']\n",
        "  if lst == ['review_count']:\n",
        "    return FOLDER[id]['FID_G1']\n",
        "  if lst == ['fans']:\n",
        "      return FOLDER[id]['FID_G6']\n",
        "  if method == 'kmeans':\n",
        "    if lst == ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']:\n",
        "      return FOLDER[id]['FID_G2']\n",
        "    if lst == ['loc1', 'loc2', 'loc3']:\n",
        "      return FOLDER[id]['FID_G3']\n",
        "    if lst == ['age', 'gender', 'ethnicity']: \n",
        "      return FOLDER[id]['FID_G4']\n",
        "    if lst == ['review_sentiment']:\n",
        "      return FOLDER[id]['FID_G5']\n",
        "    if lst == ['useful','funny', 'cool']:\n",
        "      return FOLDER[id]['FID_G7']\n",
        "  if method == 'balanced_kmeans':\n",
        "    if lst == ['age', 'gender', 'ethnicity']:\n",
        "      return FOLDER[id]['FID_G8']\n",
        "    if lst == ['loc1', 'loc2', 'loc3']:\n",
        "      return FOLDER[id]['FID_G9']\n",
        "  if method == 'custom':\n",
        "    return FOLDER[id]['FID_G12']\n",
        "\n",
        "\n",
        "\n",
        "# READ JSON\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "  cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "  return df[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyIm6NR55FGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "drive = authenticate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrfK_M25rAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD user.json FROM DRIVE\n",
        "users_dataset_id = '1JokoV68YD5Iq2l4Y_IV2RJzBpD_mSCyq'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': users_dataset_id})\n",
        "download.GetContentFile('user.json')\n",
        "users = read_json('user.json')\n",
        "users = pd.DataFrame(users)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMGOW-SHRrP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD business.json FROM DRIVE\n",
        "business_dataset_id = '1Qoy132gb205xAIFjkBbZ2CyYDeaz9yqU'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': business_dataset_id})\n",
        "download.GetContentFile('business.json')\n",
        "business = read_json('business.json')\n",
        "business = pd.DataFrame(business)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tkQRFfw16x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD review.json FROM DRIVE\n",
        "review_dataset_id = '1mW1WbpMFjN0qQpLnM-R_TzNpAkFsBLHQ'  # FILE ID, got on google drive with condivision link\n",
        "download = drive.CreateFile({'id': review_dataset_id})\n",
        "download.GetContentFile('review.json')\n",
        "reviews = read_json('review.json')\n",
        "reviews = pd.DataFrame(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYF2ESayjKVg",
        "colab_type": "code",
        "outputId": "273f39b4-bd52-4383-ca2f-e1f27636cca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# DOWNLOAD DEMOGRAPHICS\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A', 'S-oLPRdhlyL5HAknBKTUcQ']\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "file_id_list = {'WbJ1LRQdOuYYlRLyTkuuxw':['1uuA0QH-DJklvtiekxS-7yaXfKRi9ux06'],\n",
        "#                                          '13amOFvuku27snF8mea8Yp7rH36Mve42N',\n",
        "#                                          '1VskD_0Ijwe3_fzVbgYqK9Bk-YOCK046t',\n",
        "#                                          '1uuA0QH-DJklvtiekxS-7yaXfKRi9ux06'],\n",
        "                'T2tEMLpTeSMxLKpxwFdS3g':['102OYWJOKF54GMnFh4_AKc5e_Mz7xiXZt'],\n",
        "                                          #'11fM_aHYkCLQ2z3g7Z76aZZ3cAsIrpezN',\n",
        "                                          #'102OYWJOKF54GMnFh4_AKc5e_Mz7xiXZt',\n",
        "                                          #'1ObvjBbxRNIp1vOQipUGZxZEi_eQH7kJQ'],\n",
        "                'ALwAlxItASeEs2vYAeLXHA':['1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk'],\n",
        "                                          #'1crTNpDR2VjBdGSgavIvN4QNhwTUR3-Bk',\n",
        "                                          #'1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk',\n",
        "                                          #'1tgJnMW0ZImx1vbDzWpvFBRX0FJu1Kdbt'],\n",
        "                'OVTZNSkSfbl3gVB9XQIJfw':['12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF'],\n",
        "                                          #'1Vmkzpdi_0m9CGcyp5ggeNsUsueBzqufH',\n",
        "                                          #'12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF',\n",
        "                                          #'1qecYhCBytq1Z6lm_ueCWIuithvRG0wIq'],\n",
        "                'Sovgwq-E-n6wLqNh3X_rXg':['1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT'],\n",
        "                                          #'1VVePrvWR7e5XkRmhp5Q5qOPXPskVmnnt',\n",
        "                                          #'1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT',\n",
        "                                          #'1R5GFYo2c2YC_AOB9G0pdOfROTyglePDI'],\n",
        "                'j5nPiTwWEFr-VsePew7Sjg':['16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                                          #'1txkAzGKMRjXku18rv9222q-FqVo42A9o',\n",
        "                                          #'1e8iSM3SpyB2Y_h6rzRSVVgphjHEIJnZU',\n",
        "                                          #'16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                'aiX_WP7NKPTdF9CfI-M-wg':['1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                                          #'1VrVgBoJp5cRVMH4I-5uKvdF3b-PSfbmL',\n",
        "                                          #'1f7T7ksdCCGxhVF_RJU9zu38-r8aTTJcD',\n",
        "                                          #'1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                'e4NQLZynhSmvwl38hC4m-A':['1-myKdlTuCpUyPuK8OwHhglpQQGe7Bh-y'],\n",
        "                                          #'1yYG8ftjE1i9prP61ej5EdGfJDlweiK3C',\n",
        "                                          #'1KSdN24eXC-Bnk2_mTQR6oNaGll1KP5hO',\n",
        "                                          #'1-myKdlTuCpUyPuK8OwHhglpQQGe7Bh-y'],\n",
        "                'S-oLPRdhlyL5HAknBKTUcQ':['1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "                                          #'1qcX9pGfmogOokbBOPnZY-NbroolffAlg',\n",
        "                                          #'1YVdSND3VNit4ieORXC_mVzcv4mphGUnY',\n",
        "                                          #'1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "for business_id in id_list:\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for file_id in file_id_list[business_id]:\n",
        "    download = drive.CreateFile({'id': file_id})\n",
        "    download.GetContentFile('demographics_' + business_id + '.csv')\n",
        "    df_temp = pd.read_csv('demographics_' + business_id + '.csv')\n",
        "    df = df.append(df_temp)\n",
        "    \n",
        "  print(len(df.index))\n",
        "  df = df.sort_values(['id', 'age'], ascending=[True, False])\n",
        "  df = df.drop_duplicates('id',keep='first').reset_index(drop=True)\n",
        "  print('senza duplicati: ', len(df.index))\n",
        "  df = df.rename(columns={'id':'user_id'})\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('demographics_' + business_id + '.csv')\n",
        "  upload_file('demographics_' + business_id + '.csv',FOLDER[business_id]['FID_G4'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1564\n",
            "senza duplicati:  1209\n",
            "1034\n",
            "senza duplicati:  832\n",
            "952\n",
            "senza duplicati:  779\n",
            "1070\n",
            "senza duplicati:  862\n",
            "972\n",
            "senza duplicati:  769\n",
            "1064\n",
            "senza duplicati:  885\n",
            "1514\n",
            "senza duplicati:  1161\n",
            "953\n",
            "senza duplicati:  740\n",
            "965\n",
            "senza duplicati:  772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki8CKUSFkv4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD SENTIMENT ANALYSIS\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "#          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A']\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "file_id_list = {'WbJ1LRQdOuYYlRLyTkuuxw':['1lvZnVuA4QN5KmnJj6IJ8BgxWdcyQixEd',\n",
        "                                          '1w4sWZYFmWLMywS43TuleneQPKQwauPQK',\n",
        "                                          '1sIHc1BZcBWT3Iqd94-PaA4BF5elxcEYi'],\n",
        "                'T2tEMLpTeSMxLKpxwFdS3g':['11fM_aHYkCLQ2z3g7Z76aZZ3cAsIrpezN',\n",
        "                                          '1TPexYC2YHq_8ywZSuRRIJkrUCstwoxSp',\n",
        "                                          '1ObvjBbxRNIp1vOQipUGZxZEi_eQH7kJQ'],\n",
        "                'ALwAlxItASeEs2vYAeLXHA':['1crTNpDR2VjBdGSgavIvN4QNhwTUR3-Bk',\n",
        "                                          '1sJOKR7-__9dKJG0ewVQyfmV6i_dLpcRk',\n",
        "                                          '1tgJnMW0ZImx1vbDzWpvFBRX0FJu1Kdbt'],\n",
        "                'OVTZNSkSfbl3gVB9XQIJfw':['1Vmkzpdi_0m9CGcyp5ggeNsUsueBzqufH',\n",
        "                                          '12aiPnkvuUtyc8B2UmJEu2BpeXVYbPumF',\n",
        "                                          '1qecYhCBytq1Z6lm_ueCWIuithvRG0wIq'],\n",
        "                'Sovgwq-E-n6wLqNh3X_rXg':['1VVePrvWR7e5XkRmhp5Q5qOPXPskVmnnt',\n",
        "                                          '1fSqJYcczjUsbryfI7ekeqANaz-xD4PFT',\n",
        "                                          '1R5GFYo2c2YC_AOB9G0pdOfROTyglePDI'],\n",
        "                'j5nPiTwWEFr-VsePew7Sjg':['1txkAzGKMRjXku18rv9222q-FqVo42A9o',\n",
        "                                          '1e8iSM3SpyB2Y_h6rzRSVVgphjHEIJnZU',\n",
        "                                          '16H267f71Y_1l8O5-n5JlF5v8GNpnt_kc'],\n",
        "                'aiX_WP7NKPTdF9CfI-M-wg':['1VrVgBoJp5cRVMH4I-5uKvdF3b-PSfbmL',\n",
        "                                          '1f7T7ksdCCGxhVF_RJU9zu38-r8aTTJcD',\n",
        "                                          '1ApgXp06OyhQSgFg4pyxaH_vF6djUFmNc'],\n",
        "                'e4NQLZynhSmvwl38hC4m-A':['1yYG8ftjE1i9prP61ej5EdGfJDlweiK3C',\n",
        "                                          '1KSdN24eXC-Bnk2_mTQR6oNaGll1KP5hO',\n",
        "                                          '1PA2eYsYj2bj4Mo7ofbnvbQgYjmd2SwRG'],\n",
        "                'S-oLPRdhlyL5HAknBKTUcQ':['1qcX9pGfmogOokbBOPnZY-NbroolffAlg',\n",
        "                                          '1YVdSND3VNit4ieORXC_mVzcv4mphGUnY',\n",
        "                                          '1XrZViuuarqibJmaURcb0SHU5nd_3zGpo']}\n",
        "for business_id in id_list:\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for file_id in file_id_list[business_id]:\n",
        "    download = drive.CreateFile({'id': file_id})\n",
        "    download.GetContentFile('sentiment_' + business_id + '.csv')\n",
        "    df_temp = pd.read_csv('sentiment_' + business_id + '.csv')\n",
        "    df = df.append(df_temp)\n",
        "    \n",
        "  print(len(df.index))\n",
        "  df = df.drop_duplicates('id',keep='first').reset_index(drop=True)\n",
        "  print(len(df.index))\n",
        "  df = df.rename(columns={'id':'user_id'})\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('sentiment_' + business_id + '.csv')\n",
        "  upload_file('sentiment_' + business_id + '.csv',FOLDER[business_id]['FID_G5'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbM06dpOYYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXEC IN LOCAL THE DOWNLOAD OF REVIEW RANKINGS OF EACH RESTAURANT\n",
        "# UPLOAD CSV IN COLAB\n",
        "############################# EXECUTE IN LOCAL (if not blocked by yelp) #############################################\n",
        "def get_ranking_from_call(url_business, lang, sort, query):\n",
        "    headers = [{\"name\": \"Accept\", \"value\": \"*/*\"}, {\"name\": \"Accept-Encoding\", \"value\": \"gzip, deflate, br\"},\n",
        "               {\"name\": \"Accept-Language\", \"value\": \"it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3\"},\n",
        "               {\"name\": \"Connection\", \"value\": \"keep-alive\"},\n",
        "               {\"name\": \"Content-Type\", \"value\": \"application/x-www-form-urlencoded; charset=utf-8\"}, {\"name\": \"Cookie\",\n",
        "                                                                                                       \"value\": \"qntcst=D; hl=en_US; wdi=1|3C26116D69138F61|0x1.78d019f71a444p+30|a7756ff94751d3a9; _ga=GA1.2.3C26116D69138F61; location=%7B%22city%22%3A+%22New+York%22%2C+%22state%22%3A+%22NY%22%2C+%22country%22%3A+%22US%22%2C+%22latitude%22%3A+40.713%2C+%22longitude%22%3A+-74.0072%2C+%22max_latitude%22%3A+40.8523%2C+%22min_latitude%22%3A+40.5597%2C+%22max_longitude%22%3A+-73.7938%2C+%22min_longitude%22%3A+-74.1948%2C+%22zip%22%3A+%22%22%2C+%22address1%22%3A+%22%22%2C+%22address2%22%3A+%22%22%2C+%22address3%22%3A+null%2C+%22neighborhood%22%3A+null%2C+%22borough%22%3A+null%2C+%22provenance%22%3A+%22YELP_GEOCODING_ENGINE%22%2C+%22display%22%3A+%22New+York%2C+NY%22%2C+%22unformatted%22%3A+%22New+York%2C+NY%2C+US%22%2C+%22accuracy%22%3A+4.0%2C+%22language%22%3A+null%7D; xcj=1|Ptt9P03gfc75x_PBT9zmqCkUuSuyB7PR-wWUBvABNi4; __qca=P0-60561249-1581956668708; G_ENABLED_IDPS=google; __cfduid=db8764ff59d8028a6c2e1b214867927d81583160194; _gid=GA1.2.2014867238.1583835527; bse=05dcd9d5de304ef0b1d9a76fa768b10f; sc=8a1ca0dbc2; pid=505721aa4569e7bb\"},\n",
        "               {\"name\": \"Host\", \"value\": \"www.yelp.com\"},\n",
        "               {\"name\": \"Referer\", \"value\": \"https://www.yelp.com/biz/noche-de-margaritas-new-york\"},\n",
        "               {\"name\": \"TE\", \"value\": \"Trailers\"}, {\"name\": \"User-Agent\",\n",
        "                                                     \"value\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0\"},\n",
        "               {\"name\": \"X-Requested-By-React\", \"value\": \"true\"},\n",
        "               {\"name\": \"X-Requested-With\", \"value\": \"XMLHttpRequest\"}]\n",
        "    headers_ok = {}\n",
        "    for header in headers:\n",
        "        temp = {\n",
        "            header['name']: header['value']\n",
        "        }\n",
        "        headers_ok.update(temp)\n",
        "\n",
        "    x = 0\n",
        "    reviews_list = []\n",
        "    position = 1\n",
        "    url = url_business + \"/review_feed?rl=\" + lang + \"&sort_by=\" + sort + \"&q=\" + query\n",
        "\n",
        "    while 1:\n",
        "        if x == 0:\n",
        "            page_load = requests.get(url + '&start=', headers=headers_ok)\n",
        "        else:\n",
        "            page_load = requests.get(url + '&start=' + str(x), headers=headers_ok)\n",
        "        print(page_load)\n",
        "        x = x + 20\n",
        "        reviews = page_load.json()['reviews']\n",
        "        # print(json.dumps(reviews, indent=4, sort_keys=True))\n",
        "        if not reviews:\n",
        "            break\n",
        "        for review in reviews:\n",
        "            reviews_list.append((position, review['userId'], review['user'],#['reviewCount'],\n",
        "                                 datetime.datetime.strptime(review['localizedDate'], '%m/%d/%Y')))\n",
        "            position = position + 1\n",
        "    df_reviews = pd.DataFrame(reviews_list, columns=[\"position\", \"user_id\", \"user\", \"date\"])\n",
        "    return df_reviews\n",
        "\n",
        "\n",
        "def retrieve_rankings(business_id):\n",
        "    df_rel_ranking = get_ranking_from_call(\"https://www.yelp.com/biz/\" + business_id, \"en\", \"relevance_desc\", \"\")\n",
        "    df_date_ranking = df_rel_ranking.sort_values(by=['date']).reset_index(drop=True)\n",
        "    df_date_ranking['position'] = df_date_ranking.index + 1\n",
        "    df_rand_ranking = df_rel_ranking.sample(frac=1).reset_index(drop=True)\n",
        "    df_rand_ranking['position'] = df_rand_ranking.index + 1\n",
        "\n",
        "    df_rel_ranking = drop_unnamed(df_rel_ranking)\n",
        "    df_rand_ranking = drop_unnamed(df_rand_ranking)\n",
        "    df_date_ranking = drop_unnamed(df_date_ranking)\n",
        "\n",
        "    df_rel_ranking.to_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "    df_date_ranking.to_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "    df_rand_ranking.to_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "\n",
        "    destination = set_file_destination('', 'ranking', business_id)\n",
        "    upload_file(\"rel_ranking_\" + business_id + \".csv\", destination)\n",
        "    upload_file(\"date_ranking_\" + business_id + \".csv\", destination)\n",
        "    upload_file(\"rand_ranking_\" + business_id + \".csv\", destination)\n",
        "\n",
        "\n",
        "business_ids = ['e4NQLZynhSmvwl38hC4m-A', 'S-oLPRdhlyL5HAknBKTUcQ']\n",
        "\n",
        "for id in business_ids:\n",
        "    retrieve_rankings(id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdV4KHSkePdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ALTERNATIVE METHOD TO SHUFFLE THE RANKING FOR RANDOM\n",
        "#see https://gist.github.com/cadrev/6b91985a1660f26c2742\n",
        "business_id = 'WbJ1LRQdOuYYlRLyTkuuxw'\n",
        "df_rel_ranking = pd.read_csv('rel_ranking_' + business_id + '.csv')\n",
        "df_date_ranking = df_rel_ranking.sort_values(by=['Date']).reset_index(drop=True)\n",
        "df_date_ranking['Position'] = df_date_ranking.index + 1\n",
        "df_rand_ranking = df_rel_ranking\n",
        "\n",
        "#random.shuffle(df_rand_ranking) DOESN'T WORK KEY ERROR\n",
        "df_rand_ranking = df_rand_ranking.reindex(np.random.permutation(df_rand_ranking.index))\n",
        "\n",
        "df_rand_ranking = df_rand_ranking.reset_index(drop=True)\n",
        "\n",
        "# drop all the unnamed columns\n",
        "df_rel_ranking = drop_unnamed(df_rel_ranking)\n",
        "df_rand_ranking = drop_unnamed(df_rand_ranking)\n",
        "df_date_ranking = drop_unnamed(df_date_ranking)\n",
        "\n",
        "df_rand_ranking['Position'] = df_rand_ranking.index + 1\n",
        "\n",
        "print(df_rand_ranking)\n",
        "\n",
        "df_rel_ranking.to_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "df_date_ranking.to_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "df_rand_ranking.to_csv(\"rand_ranking_\" + business_id + \".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070u7qOzHsiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# JOIN USER FROM YELP WITH USER FROM DATASET AND PRINT LOST USERS\n",
        "def filter_user_from_dataset(df, users):\n",
        "  df = df.rename(columns={'Position':'position','User_Id': 'user_id',\n",
        "                          'Date':'date'})\n",
        "  df = df[['position','user_id','date']]\n",
        "  df_merged = df.merge(users, on='user_id')\n",
        "  total_review = df['position'].max()\n",
        "  print('Utenti persi: totali ' + str(total_review) + ' - utenti nel dataset ' +\n",
        "        str(len(df_merged.index)) + ' = ' + str(total_review - len(df_merged.index)))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "\n",
        "  cols = df_merged.columns.tolist()\n",
        "\n",
        "  df_merged = df_merged[['position', 'user_id', 'date', 'fans', 'average_stars', 'review_count']]\n",
        "\n",
        "  return df_merged\n",
        "\n",
        "\n",
        "def compute_groups_percents(df_groups, N_of_groups):\n",
        "  total = len(df_groups.index)\n",
        "  percents = []\n",
        "  i = 0\n",
        "  while i < N_of_groups:\n",
        "    current_length = len(df_groups[df_groups['group_id'] == i].index)\n",
        "    percents.append((current_length/total)*100)\n",
        "    i = i + 1\n",
        "  print(percents)\n",
        "  return percents\n",
        "\n",
        "\n",
        "# ADD REVIEW INFO IN RANKING DATAFRAME\n",
        "def integrate_review_info(df, reviews, business_id):\n",
        "  business_reviews = reviews[reviews['business_id'] == business_id]\n",
        "  df_merged = business_reviews.merge(df, on='user_id')\n",
        "  del df_merged['date_y']  # it's date from yelp website (not updated in dataset)\n",
        "  df_merged = df_merged.rename(columns={'date_x':'date'})\n",
        "  # drop duplicates reviews from same user\n",
        "  df_merged = df_merged.sort_values('date').drop_duplicates('user_id',keep='last')\n",
        "  df_merged = df_merged.sort_values(by=['position']).reset_index(drop=True)\n",
        "  print('Review perse: ', len(df.index) - len(df_merged.index))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "\n",
        "  df_merged = df_merged[['position', 'user_id', 'review_id', 'date', 'text', 'fans', 'average_stars', 'review_count', 'business_id']]\n",
        "  \n",
        "  return df_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezxCXIZBkcAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADD USER DATA TO GROUP USERS\n",
        "def create_vectors_only_demographics(df_users, business_id, list_of_attributes, method):\n",
        "  vectors = df_users[['user_id']]\n",
        "\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "def create_vectors(df_users, reviews, business, business_id, list_of_attributes, method):\n",
        "  vectors = df_users[['user_id', 'review_count', 'fans' , 'average_stars']]\n",
        "  vectors[\"useful\"] = np.NaN\n",
        "  vectors[\"funny\"] = np.NaN\n",
        "  vectors[\"cool\"] = np.NaN\n",
        "  vectors[\"loc1\"] = np.NaN\n",
        "  vectors[\"loc2\"] = np.NaN\n",
        "  vectors[\"loc3\"] = np.NaN\n",
        "  print(vectors.columns)\n",
        "  for index, user in vectors.iterrows():\n",
        "    print(index)\n",
        "    # TOP THREE LOCATION\n",
        "    location_list = get_top_three_loc(user, reviews, business)\n",
        "    i = 0\n",
        "    while i < len(location_list):\n",
        "      vectors.loc[index, 'loc'+str(i+1)] = location_list[i]\n",
        "      i = i + 1\n",
        "\n",
        "    # USEFUL FUNNY COOL\n",
        "    useful, funny, cool = get_details_user(user, reviews)\n",
        "    vectors.loc[index, 'useful'] = useful\n",
        "    vectors.loc[index, 'funny'] = funny\n",
        "    vectors.loc[index, 'cool'] = cool\n",
        "\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "\n",
        "def get_demographics(vectors, id):\n",
        "  df_demographics = pd.read_csv('demographics_' + id + '.csv')\n",
        "  new_vectors = pd.merge(vectors, df_demographics, on='user_id', how='left')\n",
        "  new_vectors = drop_unnamed(new_vectors)\n",
        "  return new_vectors\n",
        "\n",
        "\n",
        "def get_details_user(user, reviews):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  return user_reviews['useful'].sum(), user_reviews['funny'].sum(), user_reviews['cool'].sum()\n",
        "\n",
        "\n",
        "def get_top_three_loc(user, reviews, business):\n",
        "  user_id = user['user_id']\n",
        "  user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "  result = user_reviews.merge(business, on='business_id')\n",
        "  top_loc = result['state'].value_counts().index.tolist()[:3]\n",
        "  top_zip = []\n",
        "  for location in top_loc:\n",
        "    # exclude postal_code of Canada, that is strings\n",
        "    temp_list = result[result['state']==location]['postal_code'].value_counts().index.tolist()\n",
        "    if temp_list != []:\n",
        "      temp_list = [elem for elem in temp_list if str(elem).isdigit()]\n",
        "      if temp_list != []:\n",
        "        top_zip.append(int(str(temp_list[0])[:3]))\n",
        "  return top_zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHR1vvEz-yk4",
        "colab_type": "code",
        "outputId": "65a0e83b-f2a1-4995-beef-3c4dd101a9fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "def print_statistics_of_demographics(id):\n",
        "  df = pd.read_csv('demographics_' + id + '.csv')\n",
        "  all_users = len(df.index)\n",
        "  print(all_users)\n",
        "\n",
        "  print('-------------' + id + '--------------')\n",
        "\n",
        "  u_0_20 = len(df[(df['age']>=0) & (df['age']<=20)])\n",
        "  print('Utenti in fascia età 0-20: %d (%.2f%%)' % (u_0_20, (u_0_20/all_users)*100))\n",
        "\n",
        "  u_20_40 = len(df[(df['age']>20) & (df['age']<=40)])\n",
        "  print('Utenti in fascia età 20-40: %d (%.2f%%)' % (u_20_40, (u_20_40/all_users)*100))\n",
        "\n",
        "  u_40_60 = len(df[(df['age']>40) & (df['age']<=60)])\n",
        "  print('Utenti in fascia età 40-60: %d (%.2f%%)' % (u_40_60, (u_40_60/all_users)*100))\n",
        "\n",
        "  u_60_90 = len(df[(df['age']>60) & (df['age']<=90)])\n",
        "  print('Utenti in fascia età 60-90: %d (%.2f%%)' % (u_60_90, (u_60_90/all_users)*100))\n",
        "\n",
        "  u_over_90 = len(df[(df['age']>90)])\n",
        "  print('Utenti in fascia età >90: %d (%.2f%%)' % (u_over_90, (u_over_90/all_users)*100))\n",
        "\n",
        "  #temp = df[(df['age']>=0) & (df['age']<=20)][['user_id', 'age']]\n",
        "  #temp = drop_unnamed(temp)\n",
        "  #print(temp)\n",
        "\n",
        "  print('--------------------')\n",
        "\n",
        "  u_fem = len(df[df['gender']=='feminine'])\n",
        "  print('Utenti femmine: %d (%.2f%%)' % (u_fem, (u_fem/all_users)*100))\n",
        "\n",
        "  u_mas = len(df[df['gender']=='masculine'])\n",
        "  print('Utenti maschi: %d (%.2f%%)' % (u_mas, (u_mas/all_users)*100))\n",
        "\n",
        "  print('--------------------')\n",
        "\n",
        "  u_white = len(df[df['ethnicity']=='white'].reset_index(drop=True))\n",
        "  print('Utenti bianchi: %d (%.2f%%)' % (u_white, (u_white/all_users)*100))\n",
        "  bianchi = df[df['ethnicity']=='white'].reset_index(drop=True)\n",
        "  b_under_39 = len(bianchi[(bianchi['age']<39)])\n",
        "  print('Bianchi in fascia età <39: %d (%.2f%%)' % (b_under_39, (b_under_39/all_users)*100))\n",
        "  b_over_39 = len(bianchi[(bianchi['age']>=39)])\n",
        "  print('Bianchi in fascia età >39: %d (%.2f%%)' % (b_over_39, (b_over_39/all_users)*100))\n",
        "  b_fem = len(bianchi[bianchi['gender']=='feminine'])\n",
        "  print('Bianchi femmine: %d (%.2f%%)' % (b_fem, (b_fem/all_users)*100))\n",
        "  b_mas = len(bianchi[bianchi['gender']=='masculine'])\n",
        "  print('Bianchi maschi: %d (%.2f%%)' % (b_mas, (b_mas/all_users)*100))\n",
        "\n",
        "\n",
        "  u_black = len(df[df['ethnicity']=='black or african american'])\n",
        "  print('Utenti neri: %d (%.2f%%)' % (u_black, (u_black/all_users)*100))\n",
        "  neri = df[df['ethnicity']=='black or african american']\n",
        "  n_under_39 = len(neri[(neri['age']<39)])\n",
        "  print('Neri in fascia età <39: %d (%.2f%%)' % (n_under_39, (n_under_39/all_users)*100))\n",
        "  n_over_39 = len(neri[(neri['age']>=39)])\n",
        "  print('Neri in fascia età >39: %d (%.2f%%)' % (n_over_39, (n_over_39/all_users)*100))\n",
        "  n_fem = len(neri[neri['gender']=='feminine'])\n",
        "  print('Neri femmine: %d (%.2f%%)' % (n_fem, (n_fem/all_users)*100))\n",
        "  n_mas = len(neri[neri['gender']=='masculine'])\n",
        "  print('Neri maschi: %d (%.2f%%)' % (n_mas, (n_mas/all_users)*100))\n",
        "\n",
        "  u_altro = len(df[(df['ethnicity']=='hispanic, latino, or spanish origin') | (df['ethnicity']=='asian') | (df['ethnicity']=='middle eastern or north african') | (df['ethnicity']=='native hawaiian or pacific islander')])\n",
        "  print('Utenti altre etnie: %d (%.2f%%)' % (u_altro, (u_altro/all_users)*100))\n",
        "  altri = df[(df['ethnicity']=='hispanic, latino, or spanish origin') | (df['ethnicity']=='asian') | (df['ethnicity']=='middle eastern or north african') | (df['ethnicity']=='native hawaiian or pacific islander')]\n",
        "  a_under_39 = len(altri[(altri['age']<39)])\n",
        "  print('Altri in fascia età <39: %d (%.2f%%)' % (a_under_39, (a_under_39/all_users)*100))\n",
        "  a_over_39 = len(altri[(altri['age']>=39)])\n",
        "  print('Altri in fascia età >39: %d (%.2f%%)' % (a_over_39, (a_over_39/all_users)*100))\n",
        "  a_fem = len(altri[altri['gender']=='feminine'])\n",
        "  print('Altri femmine: %d (%.2f%%)' % (a_fem, (a_fem/all_users)*100))\n",
        "  a_mas = len(altri[altri['gender']=='masculine'])\n",
        "  print('Altri maschi: %d (%.2f%%)' % (a_mas, (a_mas/all_users)*100))\n",
        "\n",
        "  '''u_latino = len(df[df['ethnicity']=='hispanic, latino, or spanish origin'])\n",
        "  print('Utenti latini: %d (%.2f%%)' % (u_latino, (u_latino/all_users)*100))\n",
        "\n",
        "  u_asian = len(df[df['ethnicity']=='asian'])\n",
        "  print('Utenti asiatici: %d (%.2f%%)' % (u_asian, (u_asian/all_users)*100))\n",
        "\n",
        "  u_arabs = len(df[df['ethnicity']=='middle eastern or north african'])\n",
        "  print('Utenti arabi: %d (%.2f%%)' % (u_arabs, (u_arabs/all_users)*100))\n",
        "\n",
        "  u_hawa = len(df[df['ethnicity']=='native hawaiian or pacific islander'])\n",
        "  print('Utenti hawaiiani: %d (%.2f%%)' % (u_hawa, (u_hawa/all_users)*100))'''\n",
        "\n",
        "\n",
        "\n",
        "  print('---------ALTRE MISURE-----------')\n",
        "\n",
        "  u_0_35 = len(df[(df['age']>=0) & (df['age']<35)])\n",
        "  print('Utenti in fascia età 0-35: %d (%.2f%%)' % (u_0_35, (u_0_35/all_users)*100))\n",
        "  \n",
        "  u_35_40 = len(df[(df['age']>=35) & (df['age']<=40)])\n",
        "  print('Utenti in fascia età 35-40: %d (%.2f%%)' % (u_35_40, (u_35_40/all_users)*100))\n",
        "\n",
        "  u_over_40 = len(df[(df['age']>40)])\n",
        "  print('Utenti in fascia età >40: %d (%.2f%%)' % (u_over_40, (u_over_40/all_users)*100))\n",
        "\n",
        "  print('---------ALTRE MISURE 2-----------')\n",
        "\n",
        "  u_under_39 = len(df[(df['age']<39)])\n",
        "  print('Utenti in fascia età <39: %d (%.2f%%)' % (u_under_39, (u_under_39/all_users)*100))\n",
        "  \n",
        "  u_over_39 = len(df[(df['age']>=39)])\n",
        "  print('Utenti in fascia età >39: %d (%.2f%%)' % (u_over_39, (u_over_39/all_users)*100))\n",
        "\n",
        "\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "for id in id_list:\n",
        "  print_statistics_of_demographics(id)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1209\n",
            "-------------WbJ1LRQdOuYYlRLyTkuuxw--------------\n",
            "Utenti in fascia età 0-20: 25 (2.07%)\n",
            "Utenti in fascia età 20-40: 865 (71.55%)\n",
            "Utenti in fascia età 40-60: 293 (24.23%)\n",
            "Utenti in fascia età 60-90: 24 (1.99%)\n",
            "Utenti in fascia età >90: 2 (0.17%)\n",
            "--------------------\n",
            "Utenti femmine: 775 (64.10%)\n",
            "Utenti maschi: 434 (35.90%)\n",
            "--------------------\n",
            "Utenti bianchi: 693 (57.32%)\n",
            "Bianchi in fascia età <39: 222 (18.36%)\n",
            "Bianchi in fascia età >39: 471 (38.96%)\n",
            "Bianchi femmine: 422 (34.90%)\n",
            "Bianchi maschi: 271 (22.42%)\n",
            "Utenti neri: 245 (20.26%)\n",
            "Neri in fascia età <39: 111 (9.18%)\n",
            "Neri in fascia età >39: 134 (11.08%)\n",
            "Neri femmine: 178 (14.72%)\n",
            "Neri maschi: 67 (5.54%)\n",
            "Utenti altre etnie: 270 (22.33%)\n",
            "Altri in fascia età <39: 127 (10.50%)\n",
            "Altri in fascia età >39: 143 (11.83%)\n",
            "Altri femmine: 174 (14.39%)\n",
            "Altri maschi: 96 (7.94%)\n",
            "---------ALTRE MISURE-----------\n",
            "Utenti in fascia età 0-35: 306 (25.31%)\n",
            "Utenti in fascia età 35-40: 584 (48.30%)\n",
            "Utenti in fascia età >40: 319 (26.39%)\n",
            "---------ALTRE MISURE 2-----------\n",
            "Utenti in fascia età <39: 460 (38.05%)\n",
            "Utenti in fascia età >39: 749 (61.95%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW2AKZ8lHgoB",
        "colab_type": "code",
        "outputId": "82778c6a-7315-44aa-8446-bce8682f62f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# PIPELINE\n",
        "N_of_groups = 7\n",
        "#id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg','j5nPiTwWEFr-VsePew7Sjg',\n",
        "#          'aiX_WP7NKPTdF9CfI-M-wg', 'e4NQLZynhSmvwl38hC4m-A', 'S-oLPRdhlyL5HAknBKTUcQ']\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "\n",
        "#list_of_attributes = ['review_count']\n",
        "#list_of_attributes = ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']\n",
        "#list_of_attributes = ['loc1', 'loc2', 'loc3']\n",
        "list_of_attributes = ['age', 'gender', 'ethnicity']\n",
        "#list_of_attributes = ['fans']\n",
        "#list_of_attributes = ['useful', 'funny', 'cool']\n",
        "#list_of_attributes = ['gender', 'ethnicity']\n",
        "method = 'custom'\n",
        "\n",
        "for id in id_list:\n",
        "  authenticate()\n",
        "  #df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1_yelp(id, users, reviews)\n",
        "\n",
        "  df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1_dataset(id, users, reviews)\n",
        "  #df_rel_ranking = pd.read_csv(\"dataset_rel_ranking_\" + id + \".csv\")\n",
        "  #df_date_ranking = pd.read_csv(\"dataset_date_ranking_\" + id + \".csv\")\n",
        "  #df_rand_ranking = pd.read_csv(\"dataset_rand_ranking_\" + id + \".csv\")\n",
        "  group_list, percents = pipeline2(df_rel_ranking, reviews, business, id,\n",
        "                                   N_of_groups, list_of_attributes, method)\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews,\n",
        "            percents, list_of_attributes)\n",
        "  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Utenti persi: totali 2089 - utenti nel dataset 1715 = 374\n",
            "Utenti persi: totali 2089 - utenti nel dataset 1715 = 374\n",
            "Utenti persi: totali 2089 - utenti nel dataset 1715 = 374\n",
            "Review perse:  258\n",
            "Review perse:  258\n",
            "Review perse:  258\n",
            "\n",
            "++++++++++++++++ RANKING ++++++++++++++++++\n",
            "\n",
            "Ranking by Yelp filter:\n",
            "      position                 user_id               review_id  \\\n",
            "0            1  mCrhj_CG3_pOmDFcfKotRQ  xDSSvsU44pqQeEbinlX3Hw   \n",
            "1            2  VlcasgkqiTuPi-nVT7rEtw  pWuMStfD6x0H1z0ag8UFLA   \n",
            "2            3  xDl9ZF3SckkZde_48W6WeA  dqE-KMgIsYywo0CEGTzOPg   \n",
            "3            4  TKwzreBMGFbu9zN6taeVLA  qb17dS1Taj_O6_3S2JIqqw   \n",
            "4            5  GWOf1oB2mTalRY_A-EjSiQ  4hs0ARCPqpBZWNQQGJZDIg   \n",
            "...        ...                     ...                     ...   \n",
            "1452      1453  03_OcS8SbBcO4RTx_jYAUw  ITdCpwUQBirWyFYP-rbepA   \n",
            "1453      1454  vpCdKYNXUjs5o-tVSmwGAg  Uky0g-WO7e1dmkYgt0V_gA   \n",
            "1454      1455  vVBNWuQO9m5Utm69PGoRCw  YsL15iO3svxNiI3y9XJ_Rw   \n",
            "1455      1456  h7zHYM8LbPWmXx0ZEqOOAw  w3CG6iiaHWCQ8bscVjcdng   \n",
            "1456      1457  4wbMeUS9tp2QGqyNhj3YTg  z9l1bGaq_vZICHe1_qFTXA   \n",
            "\n",
            "                     date                                               text  \\\n",
            "0     2018-09-30 04:34:20  We decided to celebrate our 20+ wedding annive...   \n",
            "1     2018-08-12 16:31:00  I wasn't really impressed. I heard so many gre...   \n",
            "2     2018-02-04 18:45:36  Review Update: I really hate to leave this, bu...   \n",
            "3     2018-10-13 23:22:29  I had lunch at Tupelo Honey on a Saturday with...   \n",
            "4     2018-10-11 03:09:12  (3.5 stars) Pretty solid food with an extensiv...   \n",
            "...                   ...                                                ...   \n",
            "1452  2016-03-26 23:01:15  This was a disappointing first visit to Tupelo...   \n",
            "1453  2014-01-06 17:36:13  My husband and I took my siblings on the first...   \n",
            "1454  2013-12-27 20:55:11  We were thrilled to hear THC was openin in Cha...   \n",
            "1455  2015-11-23 22:42:39  So disappointed in our last visit here. My BF ...   \n",
            "1456  2014-09-01 15:12:46  My wife has been to the TH's in Asheville seve...   \n",
            "\n",
            "      fans  average_stars  review_count             business_id  \n",
            "0        2           3.73            82  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1       13           4.41           279  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "2       95           3.65           950  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "3       12           3.75           338  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "4       48           3.69           745  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "...    ...            ...           ...                     ...  \n",
            "1452     0           3.50             4  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1453     0           2.33             3  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1454     0           2.57             7  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1455     0           3.39            17  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1456     2           2.57            41  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "\n",
            "[1457 rows x 9 columns]\n",
            "\n",
            "\n",
            "Ranking by Date:\n",
            "      position                 user_id               review_id  \\\n",
            "0            1  PVyZXgOkVtnU6966FDFhuw  pyjzGmpzYMF_35rFasROmA   \n",
            "1            2  aK99XES2p7yNyWKpJjpxkQ  th1ZJDdp58YN_iGuQOK39Q   \n",
            "2            3  f6k8dOPuuFHJaj1icCV1ZA  L-Peuz58xOpUymG_kUFuMQ   \n",
            "3            4  hVXj8lnaTIMLTkD_yjSj6A  8TIrtWHkpttg_b7Mcz1vCQ   \n",
            "4            5  wCDC5NVQLKdTa-OwzLG9vg  g6s-w7urYtEBRgoeIZcFkg   \n",
            "...        ...                     ...                     ...   \n",
            "1452      1453  VlcasgkqiTuPi-nVT7rEtw  pWuMStfD6x0H1z0ag8UFLA   \n",
            "1453      1454  XGHvw9aF-8a3rn-2Lji8nw  O2nGTPz1kQhrr70i6hGnxQ   \n",
            "1454      1455  vH0RHYdDnzIXSUWmZbBo7A  s9eWSfzZsnhuK-5AWeZgRQ   \n",
            "1455      1456  BkIOqr6F6tNzZjcbx0HAUw  I7itarovek8dj4Cj1Q6hQg   \n",
            "1456      1457  mCrhj_CG3_pOmDFcfKotRQ  xDSSvsU44pqQeEbinlX3Hw   \n",
            "\n",
            "                     date                                               text  \\\n",
            "0     2013-12-09 03:17:14  Welcome to Charlotte, Tupelo Honey Cafe! Yelpe...   \n",
            "1     2013-12-09 15:29:25  The wings were amazing. The mac and cheese was...   \n",
            "2     2013-12-10 22:16:37  As expected, Tupelo Honey Charlotte did not di...   \n",
            "3     2013-12-13 03:53:55  Was pretty darn good.  Service was great.   Ja...   \n",
            "4     2013-12-14 16:01:55  We visited Tupelo Honey after only a week of b...   \n",
            "...                   ...                                                ...   \n",
            "1452  2018-08-12 16:31:00  I wasn't really impressed. I heard so many gre...   \n",
            "1453  2014-01-06 15:53:06  I could NOT wait to try out the new Tupelo Hon...   \n",
            "1454  2018-10-05 13:21:00  Really excited about this place based on revie...   \n",
            "1455  2016-01-15 20:21:07  Came here for my birthday today. My server, Al...   \n",
            "1456  2018-09-30 04:34:20  We decided to celebrate our 20+ wedding annive...   \n",
            "\n",
            "      fans  average_stars  review_count             business_id  \n",
            "0       61           3.82           572  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1        0           3.31            13  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "2        0           3.96            25  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "3        3           3.76           103  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "4        1           3.33            12  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "...    ...            ...           ...                     ...  \n",
            "1452    13           4.41           279  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1453     7           4.05           157  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1454     2           4.03           101  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1455     1           4.06            32  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1456     2           3.73            82  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "\n",
            "[1457 rows x 9 columns]\n",
            "\n",
            "\n",
            "Ranking Random:\n",
            "      position                 user_id               review_id  \\\n",
            "0            1  wNQAG02kjqL-pLJa9u_l9Q  fZyXrpSsIeRHhmnHCUO6nw   \n",
            "1            2  QdsSGFxPPlbFwEbiMoGl3g  OfX6iMTpHrUDMJavU_YMEQ   \n",
            "2            3  Iam8PKHiwUyDx0DeJ8O7hQ  YefC6_K8O0k5rF5oVnKxsw   \n",
            "3            4  NtR8iZusSDafM6EafTizOA  jKtciwhmZAO_hk9nd4Y18w   \n",
            "4            5  6MQECuBxnrT7FXrzFUV6nA  FafBD1048iWdknKk5dOjbQ   \n",
            "...        ...                     ...                     ...   \n",
            "1452      1453  7M-rWYRGPfCDnb7tjl7QGQ  lH7OMiDlDxYDRPr4AESZ7g   \n",
            "1453      1454  6i_z81v-HlDXwfrgjUcWkg  BEo42SS9K5_hL4FWoA-zOg   \n",
            "1454      1455  10GFv4rc2yEj3BuVSk8xTg  xZBlQmY9J2RUXTjeJyE9yw   \n",
            "1455      1456  BTZCzJjFmhDGhLYr2OXqeg  khbm8ubHN_aSdqf_iEfLaQ   \n",
            "1456      1457  FeaH3E-XDRHfrzV4Ea0LwQ  biAkXDln42FQjcL0WLGf8g   \n",
            "\n",
            "                     date                                               text  \\\n",
            "0     2015-07-20 18:51:27  Tried the Tupelo Honey in Charlotte today for ...   \n",
            "1     2017-04-11 13:55:23  I typically come here for brunch and really th...   \n",
            "2     2016-03-27 17:46:58  We had shrimp and grits, apple cider glazed sa...   \n",
            "3     2018-07-09 14:18:21  Food was awesome, service was awesome and the ...   \n",
            "4     2014-04-17 17:41:56  As a vegan, it's sometimes difficult to find a...   \n",
            "...                   ...                                                ...   \n",
            "1452  2013-12-29 06:01:18  Hah!  Like Giannabrie, I too kinda drink the T...   \n",
            "1453  2016-12-31 16:29:45  I have been here plenty times and loved my foo...   \n",
            "1454  2018-02-16 18:01:37  I remember when this place was The Pewter Rose...   \n",
            "1455  2018-05-30 02:41:39  The staff here are super nice. I came here for...   \n",
            "1456  2017-06-24 03:21:14  What a great place! My favorite will always be...   \n",
            "\n",
            "      fans  average_stars  review_count             business_id  \n",
            "0        0           3.20             5  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1        0           4.50            22  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "2        0           4.00             3  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "3        0           2.33             3  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "4        0           3.95            22  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "...    ...            ...           ...                     ...  \n",
            "1452    21           4.17           259  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1453     1           2.38            30  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1454     0           4.25             4  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1455     0           4.00             2  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "1456     0           3.70            22  WbJ1LRQdOuYYlRLyTkuuxw  \n",
            "\n",
            "[1457 rows x 9 columns]\n",
            "\n",
            "\n",
            "\n",
            "++++++++++++++++ GROUPS CREATION ++++++++++++++++++\n",
            "\n",
            "df_vectors len = 1457\n",
            "1209 = 873\n",
            "883 = 584\n",
            "                    user_id  review_count  fans  average_stars  useful  funny  \\\n",
            "0    hHQNNj8N_zuqMfO41SzmPQ           129     6           4.40     4.0    5.0   \n",
            "1    MWjOBJEQop__owYvB3quLg            86     3           4.35     0.0    0.0   \n",
            "2    Ojhtim0S96aveFKPkTLeDQ           120     3           3.79     0.0    0.0   \n",
            "3    9cHXK314MC9Y6nwI6-ehIw            58     3           3.97    19.0    5.0   \n",
            "4    mLSGxzKslN_wtjWeHQ6BcA           136     8           4.39     5.0    0.0   \n",
            "..                      ...           ...   ...            ...     ...    ...   \n",
            "309  fDKwVe0dHSKqaRPVULvZSg             1     0           5.00     0.0    0.0   \n",
            "310  SHO8XXRSeTE6_gLtyOc_JQ            11     1           3.38    20.0    5.0   \n",
            "311  wCDC5NVQLKdTa-OwzLG9vg            12     1           3.33    11.0    2.0   \n",
            "312  zx2mMtOoJTULgj2gTMiJZA            22     2           3.37    38.0    3.0   \n",
            "313  bAxtOfFyhgvyHwK3gkBUrg           207     7           4.06   107.0   33.0   \n",
            "\n",
            "     cool   loc1   loc2   loc3   age    gender ethnicity  feminine  masculine  \\\n",
            "0     7.0  282.0    NaN    NaN  39.0  feminine     white         1          0   \n",
            "1     2.0  282.0    NaN    NaN  39.0  feminine     white         1          0   \n",
            "2     0.0  282.0    NaN    NaN  55.0  feminine     white         1          0   \n",
            "3     8.0  282.0    NaN    NaN  39.0  feminine     white         1          0   \n",
            "4     4.0  282.0    NaN    NaN  36.0  feminine     white         1          0   \n",
            "..    ...    ...    ...    ...   ...       ...       ...       ...        ...   \n",
            "309   0.0  282.0    NaN    NaN  49.0  feminine     white         1          0   \n",
            "310   8.0  282.0    NaN    NaN  56.0  feminine     white         1          0   \n",
            "311   3.0  282.0    NaN    NaN  55.0  feminine     white         1          0   \n",
            "312   4.0  281.0  297.0    NaN  37.0  feminine     white         1          0   \n",
            "313  42.0  282.0  852.0  891.0  37.0  feminine     white         1          0   \n",
            "\n",
            "     asian  black or african american  hispanic, latino, or spanish origin  \\\n",
            "0        0                          0                                    0   \n",
            "1        0                          0                                    0   \n",
            "2        0                          0                                    0   \n",
            "3        0                          0                                    0   \n",
            "4        0                          0                                    0   \n",
            "..     ...                        ...                                  ...   \n",
            "309      0                          0                                    0   \n",
            "310      0                          0                                    0   \n",
            "311      0                          0                                    0   \n",
            "312      0                          0                                    0   \n",
            "313      0                          0                                    0   \n",
            "\n",
            "     middle eastern or north african  native hawaiian or pacific islander  \\\n",
            "0                                  0                                    0   \n",
            "1                                  0                                    0   \n",
            "2                                  0                                    0   \n",
            "3                                  0                                    0   \n",
            "4                                  0                                    0   \n",
            "..                               ...                                  ...   \n",
            "309                                0                                    0   \n",
            "310                                0                                    0   \n",
            "311                                0                                    0   \n",
            "312                                0                                    0   \n",
            "313                                0                                    0   \n",
            "\n",
            "     white  \n",
            "0        1  \n",
            "1        1  \n",
            "2        1  \n",
            "3        1  \n",
            "4        1  \n",
            "..     ...  \n",
            "309      1  \n",
            "310      1  \n",
            "311      1  \n",
            "312      1  \n",
            "313      1  \n",
            "\n",
            "[314 rows x 21 columns]\n",
            "                    user_id  review_count  fans  average_stars  useful  funny  \\\n",
            "0    LVwhg149FN8tDAVWTFe-Eg           352     7           4.42     5.0    2.0   \n",
            "1    tC4CCS-pXhm6MPlbrgJjSg            55     2           3.70    22.0    9.0   \n",
            "2    koMhxEdPlnNtDrPbTQoM0g           175    16           3.92   123.0   44.0   \n",
            "3    2uECxJWkFhekoPkxVQgTsw           100     1           4.06    32.0   10.0   \n",
            "4    z6v7O3QGZmD17zA6o1Cipg           102     2           4.32    25.0   22.0   \n",
            "..                      ...           ...   ...            ...     ...    ...   \n",
            "195  iImnb-W4UaMsLsXd9Eq3EA            22     0           3.24     9.0    6.0   \n",
            "196  VDFXrJXhrNRBVUH68-yW4A            52     1           4.17     2.0    0.0   \n",
            "197  eE1L8u2_iOV50yhMt9xyhw             1     0           5.00     0.0    0.0   \n",
            "198  y2T3FPC5BQgZ_C9IvhoTtg             6     0           4.17     8.0    0.0   \n",
            "199  Eeq2B13a-cavWckKaQJadg             3     0           1.00     1.0    0.0   \n",
            "\n",
            "     cool   loc1   loc2   loc3   age     gender ethnicity  feminine  \\\n",
            "0     2.0  282.0    NaN    NaN  70.0  masculine     white         0   \n",
            "1    10.0  281.0  297.0    NaN  39.0  masculine     white         0   \n",
            "2    44.0  282.0  852.0  891.0  39.0  masculine     white         0   \n",
            "3     9.0  282.0  297.0    NaN  37.0  masculine     white         0   \n",
            "4     8.0  282.0    NaN    NaN  39.0  masculine     white         0   \n",
            "..    ...    ...    ...    ...   ...        ...       ...       ...   \n",
            "195   4.0  282.0    NaN    NaN  36.0  masculine     white         0   \n",
            "196   1.0  282.0  891.0    NaN  58.0  masculine     white         0   \n",
            "197   0.0  282.0    NaN    NaN  27.0  masculine     white         0   \n",
            "198   0.0  282.0    NaN    NaN  39.0  masculine     white         0   \n",
            "199   0.0  282.0    NaN    NaN  39.0  masculine     white         0   \n",
            "\n",
            "     masculine  asian  black or african american  \\\n",
            "0            1      0                          0   \n",
            "1            1      0                          0   \n",
            "2            1      0                          0   \n",
            "3            1      0                          0   \n",
            "4            1      0                          0   \n",
            "..         ...    ...                        ...   \n",
            "195          1      0                          0   \n",
            "196          1      0                          0   \n",
            "197          1      0                          0   \n",
            "198          1      0                          0   \n",
            "199          1      0                          0   \n",
            "\n",
            "     hispanic, latino, or spanish origin  middle eastern or north african  \\\n",
            "0                                      0                                0   \n",
            "1                                      0                                0   \n",
            "2                                      0                                0   \n",
            "3                                      0                                0   \n",
            "4                                      0                                0   \n",
            "..                                   ...                              ...   \n",
            "195                                    0                                0   \n",
            "196                                    0                                0   \n",
            "197                                    0                                0   \n",
            "198                                    0                                0   \n",
            "199                                    0                                0   \n",
            "\n",
            "     native hawaiian or pacific islander  white  \n",
            "0                                      0      1  \n",
            "1                                      0      1  \n",
            "2                                      0      1  \n",
            "3                                      0      1  \n",
            "4                                      0      1  \n",
            "..                                   ...    ...  \n",
            "195                                    0      1  \n",
            "196                                    0      1  \n",
            "197                                    0      1  \n",
            "198                                    0      1  \n",
            "199                                    0      1  \n",
            "\n",
            "[200 rows x 21 columns]\n",
            "                    user_id  review_count  fans  average_stars  useful  funny  \\\n",
            "0    WCZFsrIfQaN8Fg2S1ersWw           228    11           3.31    32.0    9.0   \n",
            "1    5_vvuAY9sOVbJtBon8ce2A            87     7           4.06    68.0   26.0   \n",
            "2    oIxsWOWytMmV4bf_ffo01w           850    90           3.78  1917.0  411.0   \n",
            "3    sdLns7062kz3Ur_b8wgeYw           449    62           3.78   888.0  265.0   \n",
            "4    xEupSNbB6H8fomOjK65fgA           173    24           3.75   179.0   50.0   \n",
            "..                      ...           ...   ...            ...     ...    ...   \n",
            "118  Ejldt4p3QUVYJFz-0dbFuQ            51     0           2.74    89.0   58.0   \n",
            "119  YkgVUj-6eAplQRtSFmxahQ             1     0           4.00     0.0    0.0   \n",
            "120  -8mGZ-pJi-NcjZckuz1M7A            25     0           3.46    11.0    1.0   \n",
            "121  buEcsSARMfFhjWjdcMUE1Q            20     0           3.45    20.0    5.0   \n",
            "122  29vtBNW7413lNjUU3tGbew             9     0           4.44     1.0    0.0   \n",
            "\n",
            "      cool   loc1   loc2   loc3   age    gender                  ethnicity  \\\n",
            "0     18.0  282.0  891.0    NaN  39.0  feminine  black or african american   \n",
            "1     31.0  280.0    NaN    NaN  39.0  feminine  black or african american   \n",
            "2    894.0  282.0  297.0  152.0  45.0  feminine  black or african american   \n",
            "3    330.0  282.0  891.0  850.0  23.0  feminine  black or african american   \n",
            "4     89.0  282.0    NaN    NaN  36.0  feminine  black or african american   \n",
            "..     ...    ...    ...    ...   ...       ...                        ...   \n",
            "118   52.0  850.0  282.0    NaN  39.0  feminine  black or african american   \n",
            "119    0.0  282.0    NaN    NaN  39.0  feminine  black or african american   \n",
            "120    1.0  282.0    NaN    NaN  50.0  feminine  black or african american   \n",
            "121    4.0  282.0    NaN    NaN  37.0  feminine  black or african american   \n",
            "122    0.0  282.0    NaN    NaN  23.0  feminine  black or african american   \n",
            "\n",
            "     feminine  masculine  asian  black or african american  \\\n",
            "0           1          0      0                          1   \n",
            "1           1          0      0                          1   \n",
            "2           1          0      0                          1   \n",
            "3           1          0      0                          1   \n",
            "4           1          0      0                          1   \n",
            "..        ...        ...    ...                        ...   \n",
            "118         1          0      0                          1   \n",
            "119         1          0      0                          1   \n",
            "120         1          0      0                          1   \n",
            "121         1          0      0                          1   \n",
            "122         1          0      0                          1   \n",
            "\n",
            "     hispanic, latino, or spanish origin  middle eastern or north african  \\\n",
            "0                                      0                                0   \n",
            "1                                      0                                0   \n",
            "2                                      0                                0   \n",
            "3                                      0                                0   \n",
            "4                                      0                                0   \n",
            "..                                   ...                              ...   \n",
            "118                                    0                                0   \n",
            "119                                    0                                0   \n",
            "120                                    0                                0   \n",
            "121                                    0                                0   \n",
            "122                                    0                                0   \n",
            "\n",
            "     native hawaiian or pacific islander  white  \n",
            "0                                      0      0  \n",
            "1                                      0      0  \n",
            "2                                      0      0  \n",
            "3                                      0      0  \n",
            "4                                      0      0  \n",
            "..                                   ...    ...  \n",
            "118                                    0      0  \n",
            "119                                    0      0  \n",
            "120                                    0      0  \n",
            "121                                    0      0  \n",
            "122                                    0      0  \n",
            "\n",
            "[123 rows x 21 columns]\n",
            "                   user_id  review_count  fans  average_stars  useful  funny  \\\n",
            "0   DgJ3pIZCC63w_n7eBi718g             9     0           4.67     2.0    0.0   \n",
            "1   92UYzB-HR4Ei8mOmDzLPhA            82     5           3.67     3.0    3.0   \n",
            "2   UL1cN7493ro-VmsJC9jkqw            27     1           3.00     5.0    0.0   \n",
            "3   01X1xhJ1MOjE1w6IoEHn1Q            10     0           4.50     2.0    0.0   \n",
            "4   b5UhysODLR0B7sFOZA-tmw             6     4           4.00     1.0    0.0   \n",
            "5   SLL5OfkNIrUUPj9TuTASNA           112    29           3.58    24.0    7.0   \n",
            "6   yCaDISH0R8e5U376zDWTpQ           139    12           4.44   204.0   99.0   \n",
            "7   qosOd9d8uXEdhnvqyq3TCg             9     0           3.22     1.0    1.0   \n",
            "8   XmzBtmAGVycbe4NYif7CCA            84     3           4.62    20.0    7.0   \n",
            "9   8OkMDuKorEyGtOrqYPG2Sg             1     0           5.00     0.0    0.0   \n",
            "10  s4udX-WepzcGL4VtUBnXjQ            27     0           4.17     0.0    0.0   \n",
            "11  zTOI1--GfJJbhzf9MTDInA            25     2           3.24    25.0    5.0   \n",
            "12  PyRy1msqmdFRYVmTUqHGtg             4     0           4.25     0.0    0.0   \n",
            "13  dMx-PR1ujNveu6kdQU5Q7w            77     2           4.24    62.0   16.0   \n",
            "14  z2r6eXACH5z7qNauvVGt-g             7     0           4.38     0.0    0.0   \n",
            "15  _N7RCMRosGF6IvIljtF_9w             4     0           5.00     0.0    0.0   \n",
            "16  yEIgJOiaaav0K9VWLpQIWg             1     0           5.00     0.0    0.0   \n",
            "17  XbEQZZ2y7ES9gAg5ctgY4A           133     6           3.19     0.0    0.0   \n",
            "18  LXQRqJ_IVaJ0G4GkrFyEkQ             2     0           3.50     0.0    0.0   \n",
            "19  EUkB_ST79MoK6OZAsfppLA             2     0           3.50     0.0    0.0   \n",
            "20  sVn_rNopTpPik0iL_Xm7ag           188    13           3.30    91.0   24.0   \n",
            "21  Z_Fa_qcdvspaTReamuOQBA            96     0           3.81     1.0    0.0   \n",
            "22  VX7mr93MlWe_N5gPkVJD-g            81     1           3.72    28.0    2.0   \n",
            "23  E15Ebt_zDOGwJseeQRPTAA            22     0           4.36     1.0    1.0   \n",
            "24  d3DzgSwJqsSnFMO3yWO0hA             9     0           2.67     7.0    2.0   \n",
            "25  bdJqgpcvSQSzxnpBVz4VCg           507    32           3.19    36.0   70.0   \n",
            "26  29jEIlKu1oiPnaZUx_Z7-w             6     0           4.50     0.0    0.0   \n",
            "27  lGaca-VMDqVe9RYnUGS2Tw            11     1           3.00     1.0    2.0   \n",
            "28  _EBGLn2zi2n4G5Fd9kAsPg           107     7           4.32    99.0   20.0   \n",
            "29  Hh6wHTcUH36yfl-HomfvEQ             3     0           3.00     2.0    1.0   \n",
            "30  ahVMMbtgrml8cXHQ4thA6Q             1     0           5.00     0.0    0.0   \n",
            "31  pnBUd-flzp0LQk7vOBHnng            13     0           4.08     3.0    0.0   \n",
            "32  KYSSaD9XX1aTUahkWK-IYA             7     0           5.00    13.0    7.0   \n",
            "33  7a9BEnN5NhzaaCXjP3wFIQ             5     0           3.00     9.0    4.0   \n",
            "34  uaN7sGhKZ_N8PPxDFmLdlA            67     4           4.12    61.0   20.0   \n",
            "35  4QblvEDrT_lKUm6r_ONFLA             1     0           4.00     0.0    0.0   \n",
            "36  6tlC85J2zsHpnrtHp3qX6Q           237     3           2.92   166.0   27.0   \n",
            "37  9KSf1o6MmaawRPz751s7gA             8     0           4.38     0.0    0.0   \n",
            "38  ccvQN5agFwnviWvDdG_f4Q             4     0           4.25     4.0    1.0   \n",
            "\n",
            "     cool   loc1   loc2   loc3   age     gender                  ethnicity  \\\n",
            "0     0.0  282.0    NaN    NaN  45.0  masculine  black or african american   \n",
            "1     3.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "2     0.0  282.0    NaN    NaN  28.0  masculine  black or african american   \n",
            "3     0.0  282.0    NaN    NaN  36.0  masculine  black or african american   \n",
            "4     0.0  282.0    NaN    NaN  27.0  masculine  black or african american   \n",
            "5    14.0  853.0  282.0    NaN  39.0  masculine  black or african american   \n",
            "6   110.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "7     1.0  282.0    NaN    NaN  27.0  masculine  black or african american   \n",
            "8    13.0  282.0  297.0    NaN  59.0  masculine  black or african american   \n",
            "9     0.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "10    0.0  282.0    NaN    NaN  47.0  masculine  black or african american   \n",
            "11    4.0  282.0  891.0  852.0  39.0  masculine  black or african american   \n",
            "12    0.0  282.0    NaN    NaN  38.0  masculine  black or african american   \n",
            "13   30.0  282.0  441.0    NaN  39.0  masculine  black or african american   \n",
            "14    0.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "15    0.0  282.0    NaN    NaN  27.0  masculine  black or african american   \n",
            "16    0.0  282.0    NaN    NaN  23.0  masculine  black or african american   \n",
            "17    0.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "18    0.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "19    0.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "20   41.0  282.0  297.0    NaN  39.0  masculine  black or african american   \n",
            "21    0.0  282.0  850.0    NaN  39.0  masculine  black or african american   \n",
            "22    6.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "23    1.0  282.0  152.0    NaN  45.0  masculine  black or african american   \n",
            "24    0.0  282.0    NaN    NaN  44.0  masculine  black or african american   \n",
            "25   16.0  282.0  441.0  850.0  39.0  masculine  black or african american   \n",
            "26    0.0  282.0    NaN    NaN  27.0  masculine  black or african american   \n",
            "27    0.0  282.0  891.0    NaN  39.0  masculine  black or african american   \n",
            "28   24.0  282.0  297.0    NaN  39.0  masculine  black or african american   \n",
            "29    0.0  282.0    NaN    NaN  24.0  masculine  black or african american   \n",
            "30    0.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "31    0.0  282.0  297.0    NaN  39.0  masculine  black or african american   \n",
            "32    6.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "33    3.0  282.0    NaN    NaN  39.0  masculine  black or african american   \n",
            "34   25.0  282.0  891.0  850.0  39.0  masculine  black or african american   \n",
            "35    0.0  282.0    NaN    NaN  44.0  masculine  black or african american   \n",
            "36   48.0  282.0  297.0    NaN  39.0  masculine  black or african american   \n",
            "37    1.0  280.0    NaN    NaN  27.0  masculine  black or african american   \n",
            "38    1.0  282.0    NaN    NaN  40.0  masculine  black or african american   \n",
            "\n",
            "    feminine  masculine  asian  black or african american  \\\n",
            "0          0          1      0                          1   \n",
            "1          0          1      0                          1   \n",
            "2          0          1      0                          1   \n",
            "3          0          1      0                          1   \n",
            "4          0          1      0                          1   \n",
            "5          0          1      0                          1   \n",
            "6          0          1      0                          1   \n",
            "7          0          1      0                          1   \n",
            "8          0          1      0                          1   \n",
            "9          0          1      0                          1   \n",
            "10         0          1      0                          1   \n",
            "11         0          1      0                          1   \n",
            "12         0          1      0                          1   \n",
            "13         0          1      0                          1   \n",
            "14         0          1      0                          1   \n",
            "15         0          1      0                          1   \n",
            "16         0          1      0                          1   \n",
            "17         0          1      0                          1   \n",
            "18         0          1      0                          1   \n",
            "19         0          1      0                          1   \n",
            "20         0          1      0                          1   \n",
            "21         0          1      0                          1   \n",
            "22         0          1      0                          1   \n",
            "23         0          1      0                          1   \n",
            "24         0          1      0                          1   \n",
            "25         0          1      0                          1   \n",
            "26         0          1      0                          1   \n",
            "27         0          1      0                          1   \n",
            "28         0          1      0                          1   \n",
            "29         0          1      0                          1   \n",
            "30         0          1      0                          1   \n",
            "31         0          1      0                          1   \n",
            "32         0          1      0                          1   \n",
            "33         0          1      0                          1   \n",
            "34         0          1      0                          1   \n",
            "35         0          1      0                          1   \n",
            "36         0          1      0                          1   \n",
            "37         0          1      0                          1   \n",
            "38         0          1      0                          1   \n",
            "\n",
            "    hispanic, latino, or spanish origin  middle eastern or north african  \\\n",
            "0                                     0                                0   \n",
            "1                                     0                                0   \n",
            "2                                     0                                0   \n",
            "3                                     0                                0   \n",
            "4                                     0                                0   \n",
            "5                                     0                                0   \n",
            "6                                     0                                0   \n",
            "7                                     0                                0   \n",
            "8                                     0                                0   \n",
            "9                                     0                                0   \n",
            "10                                    0                                0   \n",
            "11                                    0                                0   \n",
            "12                                    0                                0   \n",
            "13                                    0                                0   \n",
            "14                                    0                                0   \n",
            "15                                    0                                0   \n",
            "16                                    0                                0   \n",
            "17                                    0                                0   \n",
            "18                                    0                                0   \n",
            "19                                    0                                0   \n",
            "20                                    0                                0   \n",
            "21                                    0                                0   \n",
            "22                                    0                                0   \n",
            "23                                    0                                0   \n",
            "24                                    0                                0   \n",
            "25                                    0                                0   \n",
            "26                                    0                                0   \n",
            "27                                    0                                0   \n",
            "28                                    0                                0   \n",
            "29                                    0                                0   \n",
            "30                                    0                                0   \n",
            "31                                    0                                0   \n",
            "32                                    0                                0   \n",
            "33                                    0                                0   \n",
            "34                                    0                                0   \n",
            "35                                    0                                0   \n",
            "36                                    0                                0   \n",
            "37                                    0                                0   \n",
            "38                                    0                                0   \n",
            "\n",
            "    native hawaiian or pacific islander  white  \n",
            "0                                     0      0  \n",
            "1                                     0      0  \n",
            "2                                     0      0  \n",
            "3                                     0      0  \n",
            "4                                     0      0  \n",
            "5                                     0      0  \n",
            "6                                     0      0  \n",
            "7                                     0      0  \n",
            "8                                     0      0  \n",
            "9                                     0      0  \n",
            "10                                    0      0  \n",
            "11                                    0      0  \n",
            "12                                    0      0  \n",
            "13                                    0      0  \n",
            "14                                    0      0  \n",
            "15                                    0      0  \n",
            "16                                    0      0  \n",
            "17                                    0      0  \n",
            "18                                    0      0  \n",
            "19                                    0      0  \n",
            "20                                    0      0  \n",
            "21                                    0      0  \n",
            "22                                    0      0  \n",
            "23                                    0      0  \n",
            "24                                    0      0  \n",
            "25                                    0      0  \n",
            "26                                    0      0  \n",
            "27                                    0      0  \n",
            "28                                    0      0  \n",
            "29                                    0      0  \n",
            "30                                    0      0  \n",
            "31                                    0      0  \n",
            "32                                    0      0  \n",
            "33                                    0      0  \n",
            "34                                    0      0  \n",
            "35                                    0      0  \n",
            "36                                    0      0  \n",
            "37                                    0      0  \n",
            "38                                    0      0  \n",
            "                    user_id  review_count  fans  average_stars  useful  funny  \\\n",
            "0    mCrhj_CG3_pOmDFcfKotRQ            82     2           3.73    80.0   41.0   \n",
            "1    VlcasgkqiTuPi-nVT7rEtw           279    13           4.41   241.0   63.0   \n",
            "2    IufKA98FFrk-mxaQ-ZqTLg           230    35           3.79    57.0   12.0   \n",
            "3    BifmW6A70mvRDpPc-wIfmQ            50     2           3.74     7.0    1.0   \n",
            "4    yaXV3MdlG5TezkCq8RXHIw           256    31           4.16    80.0   49.0   \n",
            "..                      ...           ...   ...            ...     ...    ...   \n",
            "120  gZ8VNx7wHuaOd24Bx71saQ           183     9           3.50   233.0   68.0   \n",
            "121  8DSuxkm7SyX9fgQTw2tuaw            31     2           3.56     4.0    0.0   \n",
            "122  hhvkQEAUxO-lSLeJo2nUSA            45     4           4.09    29.0    2.0   \n",
            "123  _KbeQpVBtZQbxOoUg_Eveg            54     1           3.96    11.0    0.0   \n",
            "124  h-HDAty1FM4wlUTRDCBtLQ             6     0           4.50     0.0    0.0   \n",
            "\n",
            "     cool   loc1   loc2   loc3   age    gender  \\\n",
            "0    51.0  282.0  297.0    NaN  27.0  feminine   \n",
            "1    98.0  282.0  297.0    NaN  22.0  feminine   \n",
            "2    33.0  282.0    NaN    NaN  16.0  feminine   \n",
            "3     1.0  282.0    NaN    NaN  36.0  feminine   \n",
            "4    72.0  441.0  282.0    NaN  27.0  feminine   \n",
            "..    ...    ...    ...    ...   ...       ...   \n",
            "120  44.0  282.0  152.0  850.0  39.0  feminine   \n",
            "121   1.0  282.0    NaN    NaN  39.0  feminine   \n",
            "122  13.0  282.0    NaN    NaN  23.0  feminine   \n",
            "123   4.0  282.0    NaN    NaN  45.0  feminine   \n",
            "124   1.0  282.0  297.0    NaN  28.0  feminine   \n",
            "\n",
            "                               ethnicity  feminine  masculine  asian  \\\n",
            "0                                  asian         1          0      1   \n",
            "1    hispanic, latino, or spanish origin         1          0      0   \n",
            "2                                  asian         1          0      1   \n",
            "3                                  asian         1          0      1   \n",
            "4                                  asian         1          0      1   \n",
            "..                                   ...       ...        ...    ...   \n",
            "120  hispanic, latino, or spanish origin         1          0      0   \n",
            "121      middle eastern or north african         1          0      0   \n",
            "122  hispanic, latino, or spanish origin         1          0      0   \n",
            "123                                asian         1          0      1   \n",
            "124  hispanic, latino, or spanish origin         1          0      0   \n",
            "\n",
            "     black or african american  hispanic, latino, or spanish origin  \\\n",
            "0                            0                                    0   \n",
            "1                            0                                    1   \n",
            "2                            0                                    0   \n",
            "3                            0                                    0   \n",
            "4                            0                                    0   \n",
            "..                         ...                                  ...   \n",
            "120                          0                                    1   \n",
            "121                          0                                    0   \n",
            "122                          0                                    1   \n",
            "123                          0                                    0   \n",
            "124                          0                                    1   \n",
            "\n",
            "     middle eastern or north african  native hawaiian or pacific islander  \\\n",
            "0                                  0                                    0   \n",
            "1                                  0                                    0   \n",
            "2                                  0                                    0   \n",
            "3                                  0                                    0   \n",
            "4                                  0                                    0   \n",
            "..                               ...                                  ...   \n",
            "120                                0                                    0   \n",
            "121                                1                                    0   \n",
            "122                                0                                    0   \n",
            "123                                0                                    0   \n",
            "124                                0                                    0   \n",
            "\n",
            "     white  \n",
            "0        0  \n",
            "1        0  \n",
            "2        0  \n",
            "3        0  \n",
            "4        0  \n",
            "..     ...  \n",
            "120      0  \n",
            "121      0  \n",
            "122      0  \n",
            "123      0  \n",
            "124      0  \n",
            "\n",
            "[125 rows x 21 columns]\n",
            "                   user_id  review_count  fans  average_stars  useful  funny  \\\n",
            "0   mXbIphHQwWKeAM-RwWkaaQ           268     5           3.94    16.0    4.0   \n",
            "1   vH0RHYdDnzIXSUWmZbBo7A           101     2           4.03     7.0    2.0   \n",
            "2   7cjfOlHVy3EIIJ53rvM44Q           148     6           3.90     3.0    1.0   \n",
            "3   XnlQAgKJUAQZMCxGIX9P5Q           274    19           3.88    29.0    9.0   \n",
            "4   SaSXRzjHx3SMTNnW90PetA           381    44           3.96   968.0  485.0   \n",
            "..                     ...           ...   ...            ...     ...    ...   \n",
            "67  UuM1kcMYTQXRHjUl13RQHg             2     0           4.00     0.0    0.0   \n",
            "68  EuGkxiM0fgk6eQ7NzpNjKw            23     1           4.13    11.0    0.0   \n",
            "69  sErm5Rn-LDEvoQVZPrM7yA             2     0           3.00     0.0    0.0   \n",
            "70  N1YxlI1isrMfQox0JZ746g            25     1           3.12     5.0    2.0   \n",
            "71  r4WBKoN5_F6fsyJo8OEMZA           130     1           4.25    81.0   20.0   \n",
            "\n",
            "     cool   loc1   loc2   loc3   age     gender  \\\n",
            "0     0.0  282.0  891.0    NaN  27.0  masculine   \n",
            "1     4.0  282.0  297.0    NaN  45.0  masculine   \n",
            "2     6.0  282.0  891.0    NaN  39.0  masculine   \n",
            "3    27.0  282.0    NaN    NaN  39.0  masculine   \n",
            "4   584.0  441.0  891.0  152.0  27.0  masculine   \n",
            "..    ...    ...    ...    ...   ...        ...   \n",
            "67    0.0  282.0    NaN    NaN  39.0  masculine   \n",
            "68    2.0  282.0    NaN    NaN  23.0  masculine   \n",
            "69    0.0  282.0    NaN    NaN  39.0  masculine   \n",
            "70    1.0  280.0    NaN    NaN  55.0  masculine   \n",
            "71   26.0  280.0  850.0    NaN  29.0  masculine   \n",
            "\n",
            "                              ethnicity  feminine  masculine  asian  \\\n",
            "0   hispanic, latino, or spanish origin         0          1      0   \n",
            "1   hispanic, latino, or spanish origin         0          1      0   \n",
            "2                                 asian         0          1      1   \n",
            "3   hispanic, latino, or spanish origin         0          1      0   \n",
            "4                                 asian         0          1      1   \n",
            "..                                  ...       ...        ...    ...   \n",
            "67  hispanic, latino, or spanish origin         0          1      0   \n",
            "68  hispanic, latino, or spanish origin         0          1      0   \n",
            "69  hispanic, latino, or spanish origin         0          1      0   \n",
            "70  hispanic, latino, or spanish origin         0          1      0   \n",
            "71                                asian         0          1      1   \n",
            "\n",
            "    black or african american  hispanic, latino, or spanish origin  \\\n",
            "0                           0                                    1   \n",
            "1                           0                                    1   \n",
            "2                           0                                    0   \n",
            "3                           0                                    1   \n",
            "4                           0                                    0   \n",
            "..                        ...                                  ...   \n",
            "67                          0                                    1   \n",
            "68                          0                                    1   \n",
            "69                          0                                    1   \n",
            "70                          0                                    1   \n",
            "71                          0                                    0   \n",
            "\n",
            "    middle eastern or north african  native hawaiian or pacific islander  \\\n",
            "0                                 0                                    0   \n",
            "1                                 0                                    0   \n",
            "2                                 0                                    0   \n",
            "3                                 0                                    0   \n",
            "4                                 0                                    0   \n",
            "..                              ...                                  ...   \n",
            "67                                0                                    0   \n",
            "68                                0                                    0   \n",
            "69                                0                                    0   \n",
            "70                                0                                    0   \n",
            "71                                0                                    0   \n",
            "\n",
            "    white  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "..    ...  \n",
            "67      0  \n",
            "68      0  \n",
            "69      0  \n",
            "70      0  \n",
            "71      0  \n",
            "\n",
            "[72 rows x 21 columns]\n",
            "[40.082361015785864, 21.551132463967054, 13.726835964310228, 8.44200411805079, 2.6767330130404945, 8.579272477693891, 4.941660947151681]\n",
            "                     user_id  group_id\n",
            "0     mCrhj_CG3_pOmDFcfKotRQ       5.0\n",
            "1     VlcasgkqiTuPi-nVT7rEtw       5.0\n",
            "2     WCZFsrIfQaN8Fg2S1ersWw       3.0\n",
            "3     mXbIphHQwWKeAM-RwWkaaQ       6.0\n",
            "4     5_vvuAY9sOVbJtBon8ce2A       3.0\n",
            "...                      ...       ...\n",
            "1452  03_OcS8SbBcO4RTx_jYAUw       0.0\n",
            "1453  vpCdKYNXUjs5o-tVSmwGAg       0.0\n",
            "1454  vVBNWuQO9m5Utm69PGoRCw       0.0\n",
            "1455  h7zHYM8LbPWmXx0ZEqOOAw       0.0\n",
            "1456  4wbMeUS9tp2QGqyNhj3YTg       0.0\n",
            "\n",
            "[1457 rows x 2 columns]\n",
            "[40.082361015785864, 21.551132463967054, 13.726835964310228, 8.44200411805079, 2.6767330130404945, 8.579272477693891, 4.941660947151681]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.154397\n",
            "1        1  0.166999\n",
            "2        2  0.161123\n",
            "3        3    0.1837\n",
            "4        4  0.167272\n",
            "5        5  0.198869\n",
            "6        6   0.18161\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.164338\n",
            "1        1  0.172597\n",
            "2        2   0.16815\n",
            "3        3  0.156284\n",
            "4        4  0.161605\n",
            "5        5  0.164898\n",
            "6        6  0.165932\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.166256\n",
            "1        1  0.171357\n",
            "2        2  0.165085\n",
            "3        3   0.16125\n",
            "4        4  0.162405\n",
            "5        5  0.161614\n",
            "6        6   0.16108\n",
            "\n",
            "\n",
            "[0.1543970847223955, 0.16699926794063943, 0.16112317204761203, 0.18369979256270563, 0.16727195458823846, 0.19886899004720887, 0.18161035091576458]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0   0.059449\n",
            "1        1   0.112122\n",
            "2        2  0.0803206\n",
            "3        3   0.145311\n",
            "4        4   0.104751\n",
            "5        5   0.160842\n",
            "6        6   0.124015\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0648968\n",
            "1        1   0.140055\n",
            "2        2  0.0907299\n",
            "3        3   0.119135\n",
            "4        4    0.08903\n",
            "5        5   0.118803\n",
            "6        6   0.130873\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0   0.056052\n",
            "1        1   0.106657\n",
            "2        2  0.0868353\n",
            "3        3   0.119752\n",
            "4        4  0.0853739\n",
            "5        5   0.116626\n",
            "6        6    0.11153\n",
            "\n",
            "\n",
            "[0.059448973506218895, 0.11212220913904947, 0.08032057639580877, 0.14531133371102323, 0.10475057757327043, 0.16084197610630718, 0.12401521365008199]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX8ibEsAIxCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline1_yelp(business_id, users, reviews):\n",
        "  ### READ THE RANKING FROM CSV\n",
        "  df_rel_ranking = pd.read_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "  df_date_ranking = pd.read_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "  df_rand_ranking = pd.read_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = df_rel_ranking.rename(columns={'User_Id':'user_id', 'Position':'position'})\n",
        "  df_date_ranking = df_date_ranking.rename(columns={'User_Id':'user_id', 'Position':'position'})\n",
        "  df_rand_ranking = df_rand_ranking.rename(columns={'User_Id':'user_id', 'Position':'position'})\n",
        "\n",
        "  df_rel_ranking = drop_unnamed(df_rel_ranking)\n",
        "  df_date_ranking = drop_unnamed(df_date_ranking)\n",
        "  df_rand_ranking = drop_unnamed(df_rand_ranking)\n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline1_dataset(business_id, users, reviews):\n",
        "  ### READ THE RANKING FROM CSV\n",
        "  df_rel_ranking = pd.read_csv(\"rel_ranking_\" + business_id + \".csv\")\n",
        "  df_date_ranking = pd.read_csv(\"date_ranking_\" + business_id + \".csv\")\n",
        "  df_rand_ranking = pd.read_csv(\"rand_ranking_\" + business_id + \".csv\")\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = filter_user_from_dataset(df_rel_ranking, users)\n",
        "  df_date_ranking = filter_user_from_dataset(df_date_ranking, users)\n",
        "  df_rand_ranking = filter_user_from_dataset(df_rand_ranking, users)\n",
        "\n",
        "  df_rel_ranking = integrate_review_info(df_rel_ranking, reviews, business_id)\n",
        "  df_date_ranking = integrate_review_info(df_date_ranking, reviews, business_id)\n",
        "  df_rand_ranking = integrate_review_info(df_rand_ranking, reviews, business_id)  \n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  df_rel_ranking.to_csv('dataset_rel_ranking_' + business_id + '.csv')\n",
        "  df_date_ranking.to_csv('dataset_date_ranking_' + business_id + '.csv')\n",
        "  df_rand_ranking.to_csv('dataset_rand_ranking_' + business_id + '.csv')\n",
        "  upload_file('dataset_rel_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "  upload_file('dataset_date_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "  upload_file('dataset_rand_ranking_' + business_id + '.csv',\n",
        "                  FOLDER[business_id]['FID_RANKING'])\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline2(df_ranking_by_relevance, reviews, business, id, N_of_groups, local_list_of_attributes, method):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "\n",
        "  #df_vectors = create_vectors(df_ranking_by_relevance, reviews, business, id, local_list_of_attributes, method)\n",
        "  #df_vectors = create_vectors_only_demographics(df_ranking_by_relevance, id, local_list_of_attributes, method)\n",
        "  \n",
        "\n",
        "  # ------ SINGLE ATTRIBUTE ---------\n",
        "  if len(local_list_of_attributes) == 1:\n",
        "    attribute = local_list_of_attributes[0]\n",
        "    df_groups = create_groups_by_percentile(df_ranking_by_relevance, attribute, N_of_groups, id)\n",
        "    method = attribute\n",
        "  \n",
        "  # ------ MULTIPLE ATTRIBUTES ---------\n",
        "  # TO BUILD VECTORS, it saves a csv file\n",
        "  elif method == 'kmeans':\n",
        "    df_groups = create_groups_by_kmeans_clustering(df_ranking_by_relevance,N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'spectral':\n",
        "    df_groups = create_groups_by_spectral_clustering(df_ranking_by_relevance, N_of_groups, id)\n",
        "  elif method == 'dbscan':\n",
        "    df_groups = create_groups_by_dbscan_clustering(df_ranking_by_relevance,N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'balanced_kmeans':\n",
        "    df_groups = create_groups_by_balanced_kmeans_clustering(N_of_groups,local_list_of_attributes,id)\n",
        "  elif method == 'custom':\n",
        "    df_groups = create_groups_custom(N_of_groups,local_list_of_attributes,id)\n",
        "  \n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "  \n",
        "  print(df_groups)\n",
        "  print(percents)\n",
        "  return df_groups, percents\n",
        "\n",
        "\n",
        "def pipeline3(business_id, method, df_ranking_by_relevance, df_ranking_by_date, \n",
        "              df_ranking_by_random, reviews, percents, list_of_attributes):\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  print(\"----------------DEMOGRAPHIC PARITY EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures = print_demographic_parity_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                       \"random_\", list_of_attributes)\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_demgr_' + method + '_' + business_id, list_of_attributes, method, business_id)\n",
        "\n",
        "  print(\"----------------DISPARATE IMPACT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                   reviews, \"yelp_\", list_of_attributes)\n",
        "  \n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                   reviews, \"date_\", list_of_attributes)\n",
        "  \n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                     reviews, \"random_\", list_of_attributes)\n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_dispimp_' + method + '_' + business_id, list_of_attributes, method, business_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDtdqvj_BUBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_custom(N_of_groups,list_of_attributes,id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  print('df_vectors len =', len(df_vectors.index))\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "  df_no_info = df_vectors[df_vectors['age'].isnull()]['user_id']\n",
        "  df_yes_info = df_vectors[df_vectors['age'].notnull()]\n",
        "  print('1209 =',len(df_yes_info.index))\n",
        "  print('883 =',len(df_no_info.index))\n",
        "\n",
        "  # ASSIGN GROUP ID\n",
        "  # GRUPPI PER ETNIA\n",
        "  '''bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "  neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "  altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "  '''\n",
        "\n",
        "  # GRUPPI PER ETNIA, GENERE\n",
        "  bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "  b_under_39 = bianchi[(bianchi['age']<39)]\n",
        "  b_over_39 = bianchi[(bianchi['age']>=39)]\n",
        "  b_fem = bianchi[bianchi['gender']=='feminine'].reset_index(drop=True)\n",
        "  b_mas = bianchi[bianchi['gender']=='masculine'].reset_index(drop=True)\n",
        "  \n",
        "  neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "  n_under_39 = neri[(neri['age']<39)]\n",
        "  n_over_39 = neri[(neri['age']>=39)]\n",
        "  n_fem = neri[neri['gender']=='feminine'].reset_index(drop=True)\n",
        "  n_mas = neri[neri['gender']=='masculine'].reset_index(drop=True)\n",
        "  \n",
        "  altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "  a_under_39 = altri[(altri['age']<39)]\n",
        "  a_over_39 = altri[(altri['age']>=39)]\n",
        "  a_fem = altri[altri['gender']=='feminine'].reset_index(drop=True)\n",
        "  a_mas = altri[altri['gender']=='masculine'].reset_index(drop=True)\n",
        "\n",
        "  print(b_fem)\n",
        "  print(b_mas)\n",
        "  print(n_fem)\n",
        "  print(n_mas)\n",
        "  print(a_fem)\n",
        "  print(a_mas)\n",
        "\n",
        "  new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in b_fem.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "    \n",
        "  for j, user in b_mas.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "  \n",
        "  for j, user in n_fem.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "    \n",
        "  for j, user in n_mas.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 4\n",
        "    \n",
        "  for j, user in a_fem.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 5\n",
        "    \n",
        "  for j, user in a_mas.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 6\n",
        "\n",
        "  # ADD THE EXCLUDED IN LAST C\n",
        "  new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "  new_df_users = drop_unnamed(new_df_users)\n",
        "  new_df_users = new_df_users.fillna(0)\n",
        "\n",
        "  destination = set_file_destination(local_list_of_attributes, 'custom', id)\n",
        "\n",
        "  new_df_users.to_csv('groups_custom_' + id + '.csv')\n",
        "  upload_file('groups_custom_' + id + '.csv',destination)\n",
        "  return new_df_users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_c2w7U3DNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_balanced_kmeans_clustering(N_of_groups, list_of_attributes, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "  df_no_info = df_vectors[df_vectors['age'].isnull()]['user_id']\n",
        "  df_yes_info = df_vectors[df_vectors['age'].notnull()]\n",
        "  number_of_users = len(df_yes_info.index)\n",
        "  five_percent = (number_of_users*5)//100\n",
        "  one_cluster_size = (number_of_users//(N_of_groups-1))-five_percent\n",
        "  clusters_size = [one_cluster_size]*(N_of_groups-1)\n",
        "\n",
        "  list_vectors = df_yes_info[list_of_attributes].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0 \n",
        "\n",
        "  # #############################################################################\n",
        "  # COMPUTE BALANCED KMEANS WITH MIN_FLOW\n",
        "  # see https://adared.ch/constrained-k-means-implementation-in-python/\n",
        "  (centroids, labels, f) = constrained_kmeans(X, clusters_size)\n",
        "  print('Centroids: ', centroids)\n",
        "  print('labels: ', labels)\n",
        "\n",
        "  # Save centroids\n",
        "  temp_df = pd.DataFrame(data=centroids)\n",
        "  temp_df.columns = list_of_attributes\n",
        "  destination = set_file_destination(local_list_of_attributes, 'balanced_kmeans', id)\n",
        "  temp_df.to_csv(\"centroids_balanced_kmeans_\" + id + \".csv\", float_format=\"%.3f\")\n",
        "  upload_file(\"centroids_balanced_kmeans_\" + id + \".csv\",destination)\n",
        "\n",
        "  new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[j, 'group_id'] = labels[j]\n",
        "  \n",
        "  # ADD THE EXCLUDED IN LAST C\n",
        "  new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "  new_df_users = new_df_users.fillna(N_of_groups-1)\n",
        "\n",
        "  new_df_users.to_csv('groups_balanced_kmeans_' + id + '.csv')\n",
        "  upload_file('groups_balanced_kmeans_' + id + '.csv',destination)\n",
        "  return new_df_users\n",
        "  # #############################################################################\n",
        "  # BALANCED KMEANS WITH PYTORCH DOESN'T WORK\n",
        "  # see https://github.com/giannisdaras/balanced_kmeans/tree/master\n",
        "  #N = len(df_users.index)\n",
        "  #cluster_size = N // N_of_groups\n",
        "  #choices, centers = kmeans_equal(X, num_clusters=N_of_groups, cluster_size=cluster_size)\n",
        "  #print(centers)\n",
        "  #print(choices)\n",
        "\n",
        "\n",
        "def constrained_kmeans(data, demand, maxiter=None, fixedprec=1e9):\n",
        "\tdata = np.array(data)\n",
        "\t\n",
        "\tmin_ = np.min(data, axis = 0)\n",
        "\tmax_ = np.max(data, axis = 0)\n",
        "\t\n",
        "\tC = min_ + np.random.random((len(demand), data.shape[1])) * (max_ - min_)\n",
        "\tM = np.array([-1] * len(data), dtype=np.int)\n",
        "\t\n",
        "\titercnt = 0\n",
        "\twhile True:\n",
        "\t\titercnt += 1\n",
        "\t\tprint(itercnt)\n",
        "\t\t# memberships\n",
        "\t\tg = nx.DiGraph()\n",
        "\t\tg.add_nodes_from(range(0, data.shape[0]), demand=-1) # points\n",
        "\t\tfor i in range(0, len(C)):\n",
        "\t\t\tg.add_node(len(data) + i, demand=demand[i])\n",
        "\t\t\n",
        "\t\t# Calculating cost...\n",
        "\t\tcost = np.array([np.linalg.norm(np.tile(data.T,\n",
        "                                          len(C)).T - np.tile(C, len(data)).reshape(len(C) * len(data),\n",
        "                                                                                            C.shape[1]), axis=1)])\n",
        "\t\t# Preparing data_to_C_edges...\n",
        "\t\tdata_to_C_edges = np.concatenate((np.tile([range(0, data.shape[0])], len(C)).T,\n",
        "                                    np.tile(np.array([range(data.shape[0], data.shape[0] + C.shape[0])]).T,\n",
        "                                            len(data)).reshape(len(C) * len(data), 1), cost.T * fixedprec),\n",
        "                                   axis=1).astype(np.uint64)\n",
        "\t\t# Adding to graph\n",
        "\t\tg.add_weighted_edges_from(data_to_C_edges)\n",
        "\t\t\n",
        "\n",
        "\t\ta = len(data) + len(C)\n",
        "\t\tg.add_node(a, demand=len(data)-np.sum(demand))\n",
        "\t\tC_to_a_edges = np.concatenate((np.array([range(len(data), len(data) + len(C))]).T, np.tile([[a]], len(C)).T), axis=1)\n",
        "\t\tg.add_edges_from(C_to_a_edges)\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t# Calculating min cost flow...\n",
        "\t\tf = nx.min_cost_flow(g)\n",
        "\t\t\n",
        "\t\t# assign\n",
        "\t\tM_new = np.ones(len(data), dtype=np.int) * -1\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tp = sorted(f[i].items(), key=lambda x: x[1])[-1][0]\n",
        "\t\t\tM_new[i] = p - len(data)\n",
        "\t\t\t\n",
        "\t\t# stop condition\n",
        "\t\tif np.all(M_new == M):\n",
        "\t\t\t# Stop\n",
        "\t\t\treturn (C, M, f)\n",
        "\t\t\t\n",
        "\t\tM = M_new\n",
        "\t\t\t\n",
        "\t\t# compute new centers\n",
        "\t\tfor i in range(len(C)):\n",
        "\t\t\tC[i, :] = np.mean(data[M==i, :], axis=0)\n",
        "\t\t\t\n",
        "\t\tif maxiter is not None and itercnt >= maxiter:\n",
        "\t\t\t# Max iterations reached\n",
        "\t\t\treturn (C, M, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQKajNw47W7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_dbscan_clustering(df_users, N_of_groups, list_of_attributes, id):\n",
        "  # #############################################################################\n",
        "  # Generate data\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  list_vectors = df_vectors[list_of_attributes].values.tolist()\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0\n",
        "\n",
        "  # #############################################################################\n",
        "  # Compute DBSCAN\n",
        "  i = 0\n",
        "  while i<21:\n",
        "    db = DBSCAN(eps=2, min_samples=i).fit(X)\n",
        "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "    core_samples_mask[db.core_sample_indices_] = True\n",
        "    labels = db.labels_\n",
        "\n",
        "    # Number of clusters in labels, ignoring noise if present.\n",
        "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise_ = list(labels).count(-1)\n",
        "\n",
        "    print('Estimated number of clusters: %d' % n_clusters_)\n",
        "    print('Estimated number of noise points: %d' % n_noise_)\n",
        "    i = i+1\n",
        "  #print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
        "  #print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
        "  #print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
        "  #print(\"Adjusted Rand Index: %0.3f\"\n",
        "  #      % metrics.adjusted_rand_score(labels_true, labels))\n",
        "  #print(\"Adjusted Mutual Information: %0.3f\"\n",
        "  #      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
        "  print(\"Silhouette Coefficient: %0.3f\"\n",
        "        % metrics.silhouette_score(X, labels))\n",
        "  \n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[j, 'group_id'] = labels[j]\n",
        "  new_df_users.to_csv('groups_dbscan_' + id + '.csv')\n",
        "  destination = set_file_destination(local_list_of_attributes, 'dbscan', id)\n",
        "  upload_file('groups_dbscan_' + id + '.csv',destination)\n",
        "  \n",
        "  #SEARCH FOR CLUSTER DESCRIPTION AND SAME SIZE CLUSTERING\n",
        "\n",
        "  # #############################################################################\n",
        "  # Plot result\n",
        "\n",
        "  # Black removed and is used for noise instead.\n",
        "  unique_labels = set(labels)\n",
        "  colors = [plt.cm.Spectral(each)\n",
        "            for each in np.linspace(0, 1, len(unique_labels))]\n",
        "  for k, col in zip(unique_labels, colors):\n",
        "      if k == -1:\n",
        "          # Black used for noise.\n",
        "          col = [0, 0, 0, 1]\n",
        "\n",
        "      class_member_mask = (labels == k)\n",
        "\n",
        "      xy = X[class_member_mask & core_samples_mask]\n",
        "      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "              markeredgecolor='k', markersize=14)\n",
        "\n",
        "      xy = X[class_member_mask & ~core_samples_mask]\n",
        "      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "              markeredgecolor='k', markersize=6)\n",
        "\n",
        "  plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "  plt.show()\n",
        "  ####################################################################\n",
        "  return new_df_users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRs30-Z9heM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_kmeans_clustering(df_users, N_of_groups, list_of_attributes, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  #if(list_of_attributes == ['age', 'gender', 'ethnicity']):\n",
        "    #age, feminine, masculine, asian, black, hispanic, arabs, hawaiian, white\n",
        "    #init_centroids = np.array([\n",
        "    #                 [20, 1, -10, -10, -10, -10, -10, -10, 1],\n",
        "    #                 [60, -10, 1, -10, -10, -10, -10, -10, 1],\n",
        "    #                 [39, 0, 0, -10, -10, 1, 1, 1, -30],\n",
        "    #                 [60, 0, 0, 1, -10, -10, -10, -10, -30],\n",
        "    #                 [20, 0, 0, -10, 1, -10, -10, -10, -30],\n",
        "    #                 [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
        "    #                np.float64)\n",
        "\n",
        "  # FOR NOW WE EXCLUDE TEXTUAL ATTRIBUTES\n",
        "  text_attribute_list = ['gender', 'ethnicity']\n",
        "  df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "  \n",
        "  local_list_of_attributes = list_of_attributes\n",
        "  # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "  # subtraction\n",
        "  temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "  list_of_attributes = temp\n",
        "  list_of_attributes = list_of_attributes + dummy_columns_name\n",
        "\n",
        "  list_vectors = df_vectors[list_of_attributes].values.tolist()\n",
        "  # ex. ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool']\n",
        "  # ex. ['loc1', 'loc2', 'loc3']\n",
        "  # Convert list of lists in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  \n",
        "  X = np.array(list_of_arrays)\n",
        "  where_are_NaNs = np.isnan(X)\n",
        "  X[where_are_NaNs] = 0\n",
        "  kmeans = KMeans(n_clusters=N_of_groups, random_state=0).fit(X) #init=init_centroids\n",
        "  labels = kmeans.labels_\n",
        "  # kmeans.predict([[0, 0, 20], [12, 3, 5]]))\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  temp_df = pd.DataFrame(data=centroids)\n",
        "  temp_df.columns = list_of_attributes\n",
        "  destination = set_file_destination(local_list_of_attributes, 'kmeans', id)\n",
        "  temp_df.to_csv(\"centroids_kmeans_\" + id + \".csv\", float_format=\"%.3f\")\n",
        "  upload_file(\"centroids_kmeans_\" + id + \".csv\",destination)\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = labels[i]\n",
        "  new_df_users.to_csv('groups_kmeans_' + id + '.csv')\n",
        "  upload_file('groups_kmeans_' + id + '.csv',destination)\n",
        "  return new_df_users\n",
        "\n",
        "\n",
        "def generate_dummies(df, text_attribute_list):\n",
        "  dummy_columns = []\n",
        "  for attr in text_attribute_list:\n",
        "    gender_dummies = pd.get_dummies(df[attr])\n",
        "    dummy_columns = dummy_columns + list(gender_dummies.columns)\n",
        "    df = pd.merge(df, gender_dummies, how=\"left\",left_index=True, right_index=True)\n",
        "    \n",
        "    # drop all the unnamed columns\n",
        "    df = drop_unnamed(df)\n",
        "  return df, dummy_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU_zGBsWzAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_spectral_clustering(df_users, N_of_groups, id):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "  text_attribute_list = ['top_location']\n",
        "  df_vectors = one_hot_encoding(df_vectors, text_attribute_list)\n",
        "  \n",
        "  list_vectors = df_vectors[['review_count', 'fans', 'average_stars', 'top_location',\n",
        "                             'useful', 'funny', 'cool']].values.tolist()\n",
        "  # Convert list of list in list of arrays\n",
        "  list_of_arrays = []\n",
        "  for vect in list_vectors:\n",
        "      current_array = np.array([])\n",
        "      for value in vect:\n",
        "          temp = np.array(value).flatten()\n",
        "          current_array = np.concatenate((current_array, temp))\n",
        "      list_of_arrays.append(current_array)\n",
        "  mat = cosine_similarity(list_of_arrays)\n",
        "  group_array = SpectralClustering(n_clusters=N_of_groups, affinity='precomputed').fit_predict(mat)\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for i, user in new_df_users.iterrows():\n",
        "    new_df_users.loc[i, 'group_id'] = group_array[i]\n",
        "  new_df_users.to_csv('groups_spectral_' + id + '.csv')\n",
        "  return new_df_users\n",
        "\n",
        "def one_hot_encoding(df, text_attribute_list):\n",
        "  for attr in text_attribute_list:\n",
        "    text = list(df[attr])\n",
        "    vectorizer = CountVectorizer().fit_transform(text)\n",
        "    embeddings = vectorizer.toarray()\n",
        "    for i, user in df.iterrows():\n",
        "      df.at[i, attr] = embeddings[i]\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awAE4AWWxEFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_groups_by_percentile(df_users, attribute, N_of_groups, business_id):\n",
        "  values = df_users[attribute].tolist()\n",
        "\n",
        "  print(df_users)\n",
        "  print(len(df_users[df_users['fans']<=3].index))\n",
        "  # Create array of percents to define limit of group \n",
        "  scale = 100/N_of_groups\n",
        "  percents = []\n",
        "  current_percent = 0\n",
        "  while current_percent < 100:\n",
        "    current_percent = current_percent + scale\n",
        "    '''if current_percent > 100:\n",
        "      current_percent = 100\n",
        "      percents.append(current_percent)\n",
        "      break'''\n",
        "    percents.append(current_percent)\n",
        "  # ex. percents = [20, 40, 60, 80, 100]\n",
        "  print(percents)\n",
        "\n",
        "  # Create array with limit values (max) of each group\n",
        "  i = 0\n",
        "  limit_values = []\n",
        "  while i < len(percents):\n",
        "    limit_values.append(np.percentile(values, percents[i]))\n",
        "    i = i + 1\n",
        "  # ex. limit_values = [5.0, 13.0, 27.0, 73.0, 10022.0]\n",
        "  print(limit_values)\n",
        "  with open('descr_groups_' + attribute + '_' + business_id + '.txt', 'w') as output:\n",
        "    output.write(str(percents) + '\\n' + str(limit_values))\n",
        "\n",
        "  new_df_users = df_users[['user_id']]\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  \n",
        "  for i, user in df_users.iterrows():\n",
        "    id_group = 0\n",
        "\n",
        "    if (user[attribute] >= 0) and (user[attribute] <= limit_values[id_group]):\n",
        "      new_df_users.loc[i, 'group_id'] = id_group\n",
        "    id_group = id_group + 1\n",
        "    while id_group < len(percents):\n",
        "      if (user[attribute] > limit_values[id_group - 1]) and (user[attribute] <= limit_values[id_group]):\n",
        "        new_df_users.loc[i, 'group_id'] = id_group\n",
        "      id_group = id_group + 1\n",
        "  \n",
        "  destination = set_file_destination([attribute], '', business_id)\n",
        "  new_df_users.to_csv('groups_' + attribute + '_' + business_id + '.csv')\n",
        "  upload_file('groups_' + attribute + '_' + business_id + '.csv',destination)\n",
        "  return new_df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbpLTiPSWuL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_disparate_impact_exposure(business_id, method,  df_ranking, reviews,\n",
        "                                    filename, list_of_attributes):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp = disparate_impact_exposure(df_groups[df_groups['group_id'] == i], df_ranking, reviews, business_id)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  exposures.to_csv('exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures\n",
        "\n",
        "\n",
        "def print_demographic_parity_exposure(business_id, method, df_ranking, filename, list_of_attributes):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp = demographic_parity_exposure(df_groups[df_groups['group_id'] == i], df_ranking)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  exposures.to_csv('exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures\n",
        "\n",
        "\n",
        "def disparate_impact_exposure(df_group, ranking, reviews, business_id):\n",
        "    if len(df_group.index) == 0:\n",
        "      return 0\n",
        "    return dmp_sommatory(df_group, ranking, business_id, reviews)/(len(df_group.index))\n",
        "\n",
        "\n",
        "def demographic_parity_exposure(df_group, ranking):\n",
        "    if len(df_group.index) == 0:\n",
        "      return 0\n",
        "    return demgr_sommatory(df_group, ranking)/(len(df_group.index))\n",
        "\n",
        "\n",
        "def dmp_sommatory(df_group, ranking, business_id, reviews):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        #Integrate reviews info for clicks counting\n",
        "        df_temp = reviews[(reviews['business_id'] == business_id) & (reviews['user_id'] == user_id)]\n",
        "        # FUNZIONA SOLO NEL DATASET\n",
        "        useful = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['useful']\n",
        "        funny = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['funny']\n",
        "        cool = df_temp[df_temp['date'] == df_temp['date'].max()].iloc[0]['cool']\n",
        "        counts = useful + funny + cool + 1\n",
        "        base = 2  # con 10 i valori sono troppo bassi\n",
        "        counts = math.log(counts, base) \n",
        "\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        addend = exp(position) * counts\n",
        "        sum = sum + addend\n",
        "    return sum\n",
        "\n",
        "\n",
        "def demgr_sommatory(df_group, ranking):\n",
        "    sum = 0\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        position = ranking.loc[ranking['user_id'] == user_id, 'position'].tolist()[0] # at??\n",
        "        sum = sum + exp(position)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/(np.log(1 + position))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPGOC05EljLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_plots(yelp_exposures, date_exposures, random_exposures, percents, title,\n",
        "              list_of_attributes, method, business_id):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  print(y1)\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='yelp')\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date')\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0),this_range)\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Iq3ciMMcZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO UPDATE CENTROIDS\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA',\n",
        "          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg'] \n",
        "method = 'kmeans'\n",
        "for id in id_list:\n",
        "  pd.options.display.float_format = '{:.3f}'.format\n",
        "  centroids = pd.read_csv('centroids_' + method + '_' + id + '.csv')\n",
        "  values = [float(x) for x in centroids.columns.values]\n",
        "  centroids.loc[-1] = values # adding a row\n",
        "  centroids.index = centroids.index + 1  # shifting index\n",
        "  centroids.sort_index(inplace=True)\n",
        "  centroids.columns = ['loc1', 'loc2', 'loc3']  \n",
        "  centroids.to_csv('centroids_' + method + '_' + id + '.csv', float_format='%.3f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22E0H3eY8Vb3",
        "colab_type": "code",
        "outputId": "5116c13a-fb95-4c8f-a24f-ef34ddfd0c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TO UPDATE EXPOSURE\n",
        "id_list = ['WbJ1LRQdOuYYlRLyTkuuxw','T2tEMLpTeSMxLKpxwFdS3g','ALwAlxItASeEs2vYAeLXHA']\n",
        "#          'OVTZNSkSfbl3gVB9XQIJfw','Sovgwq-E-n6wLqNh3X_rXg'] #RICALCOLARE DA CAPO\n",
        "# id_list = ['WbJ1LRQdOuYYlRLyTkuuxw']\n",
        "for id in id_list:\n",
        "  df_rel_ranking = pd.read_csv('dataset_rel_ranking_'+id+'.csv')\n",
        "  df_date_ranking = pd.read_csv('dataset_date_ranking_'+id+'.csv')\n",
        "  df_rand_ranking = pd.read_csv('dataset_rand_ranking_'+id+'.csv')\n",
        "\n",
        "  method = 'kmeans'\n",
        "  N_of_groups = 5\n",
        "  df_groups = pd.read_csv('groups_' + method + '_'+id+'.csv')\n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "\n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, reviews, percents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[92.94037011651817, 0.06854009595613435, 1.0281014393420151, 5.68882796435915, 0.2741603838245374]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.162933\n",
            "1        1  0.220105\n",
            "2        2  0.240493\n",
            "3        3  0.200643\n",
            "4        4  0.185087\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.16376\n",
            "1        1  0.152814\n",
            "2        2  0.172153\n",
            "3        3  0.201048\n",
            "4        4  0.169292\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.16606\n",
            "1        1  0.156778\n",
            "2        2  0.165594\n",
            "3        3  0.164798\n",
            "4        4  0.165478\n",
            "\n",
            "\n",
            "[0.16293274380030887, 0.2201045822301589, 0.2404932137794746, 0.20064345885486787, 0.1850865033952374]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.070566\n",
            "1        1   1.11977\n",
            "2        2  0.714228\n",
            "3        3  0.329566\n",
            "4        4  0.366383\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0740012\n",
            "1        1   0.777435\n",
            "2        2   0.466203\n",
            "3        3    0.39857\n",
            "4        4   0.300483\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0673801\n",
            "1        1   0.797605\n",
            "2        2   0.473072\n",
            "3        3    0.28051\n",
            "4        4   0.303709\n",
            "\n",
            "\n",
            "[0.0705660245314008, 1.1197738832848634, 0.714228081159378, 0.3295657039859884, 0.3663834882008638]\n",
            "[93.1098696461825, 0.0931098696461825, 0.74487895716946, 0.186219739292365, 5.865921787709497]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.173895\n",
            "1        1  0.621335\n",
            "2        2   0.21503\n",
            "3        3  0.228349\n",
            "4        4  0.183252\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.173915\n",
            "1        1  0.144723\n",
            "2        2  0.182823\n",
            "3        3  0.171159\n",
            "4        4  0.196404\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.175771\n",
            "1        1  0.151612\n",
            "2        2  0.170644\n",
            "3        3  0.167176\n",
            "4        4  0.168518\n",
            "\n",
            "\n",
            "[0.17389513349465582, 0.6213349345596119, 0.2150297612553226, 0.2283487263806268, 0.18325220317761595]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0948298\n",
            "1        1    3.07822\n",
            "2        2   0.537432\n",
            "3        3   0.469229\n",
            "4        4   0.283128\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0   0.10486\n",
            "1        1  0.716986\n",
            "2        2  0.406378\n",
            "3        3  0.326073\n",
            "4        4  0.334458\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id   exposure\n",
            "0        0  0.0938564\n",
            "1        1   0.751116\n",
            "2        2   0.378691\n",
            "3        3   0.318491\n",
            "4        4   0.263216\n",
            "\n",
            "\n",
            "[0.09482980330121404, 3.0782152403097, 0.5374321645605649, 0.46922881898353463, 0.28312772973154576]\n",
            "[93.4054054054054, 0.10810810810810811, 0.9729729729729729, 5.297297297297297, 0.21621621621621623]\n",
            "\n",
            "++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\n",
            "\n",
            "----------------DEMOGRAPHIC PARITY EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.176476\n",
            "1        1  0.200383\n",
            "2        2  0.208942\n",
            "3        3  0.228386\n",
            "4        4  0.460329\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.178995\n",
            "1        1  0.234594\n",
            "2        2  0.224002\n",
            "3        3   0.19136\n",
            "4        4  0.194405\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.180793\n",
            "1        1  0.166559\n",
            "2        2  0.185239\n",
            "3        3   0.16935\n",
            "4        4  0.165265\n",
            "\n",
            "\n",
            "[0.17647606219677034, 0.20038343021591398, 0.20894189258758333, 0.22838557398304843, 0.4603293537434925]\n",
            "----------------DISPARATE IMPACT EXP------------------\n",
            "\n",
            "------------Ranking by Yelp filter:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.114704\n",
            "1        1   1.01081\n",
            "2        2  0.727906\n",
            "3        3  0.548388\n",
            "4        4   2.27585\n",
            "\n",
            "\n",
            "------------Ranking by Date:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.124164\n",
            "1        1   1.18339\n",
            "2        2  0.761529\n",
            "3        3  0.483666\n",
            "4        4  0.957387\n",
            "\n",
            "\n",
            "------------Ranking Random:------------\n",
            "\n",
            "  group_id  exposure\n",
            "0        0  0.111639\n",
            "1        1  0.840188\n",
            "2        2  0.633305\n",
            "3        3  0.414278\n",
            "4        4  0.814873\n",
            "\n",
            "\n",
            "[0.11470445783168728, 1.0108129969980315, 0.7279060122864212, 0.5483878282837408, 2.2758477067956013]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FcLo9mPIeHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO UPDATE PLOTS\n",
        "id = 'WbJ1LRQdOuYYlRLyTkuuxw'\n",
        "grouping = 'review_count'\n",
        "exp = 'demgr'\n",
        "y = pd.read_csv('exp_' + exp + '_' + grouping + '_yelp_' + id + '.csv')\n",
        "d = pd.read_csv('exp_' + exp + '_' + grouping + '_date_' + id + '.csv')\n",
        "r = pd.read_csv('exp_' + exp + '_' + grouping + '_random_' + id + '.csv')\n",
        "get_plots(y,d,r, [93.42, 0.17, 0.003, 5.81, 0.188], \"title\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40JdFbw0Qqmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FILTER RESTAURANTS\n",
        "is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "restaurants = business[is_restaurant]\n",
        "\n",
        "# Filter closed restaurants\n",
        "restaurants = restaurants[restaurants['is_open'] == 1]\n",
        "\n",
        "# Order restaurants by review_count\n",
        "restaurants = restaurants.sort_values('review_count')\n",
        "\n",
        "restaurants.to_csv('restaurants_input.csv')\n",
        "# NOW CHOOSE THE RESTAURANTS AND SAVE THEIR IDS INTO TXT FILE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUr0D9xWp5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_objective_groups_by_size(df_users, attribute, range):\n",
        "    # order df_users by review_count\n",
        "    df_users = df_users.sort_values(attribute).reset_index(drop=True)\n",
        "    group_list = [[]]\n",
        "    i = 0\n",
        "    group_index = 0\n",
        "    prec = -1\n",
        "    for index, user in df_users.iterrows():\n",
        "        if i == range:\n",
        "            i = 0\n",
        "            group_list.append([])\n",
        "            group_index = group_index + 1\n",
        "        temp = user[attribute]\n",
        "        if temp != prec:\n",
        "            prec = temp\n",
        "            i = i + 1\n",
        "        group_list[group_index].append(user)\n",
        "    print(\"Number of groups created: \" + str(group_index+1))\n",
        "    return group_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYumnpYHecF",
        "colab_type": "code",
        "outputId": "e54f518e-730b-47d2-c8dd-a165910cde70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "### UNIQUE VALUE OF state, city AND categories OF BUSINESS\n",
        "# To filter only RESTAURANTS\n",
        "# is_restaurant = business['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "# restaurants = business[is_restaurant]\n",
        "\n",
        "print('UNIQUE VALUE OF \\'state\\' ATTRIBUTE IN RESTAURANTS = ', business['state'].nunique())\n",
        "print('UNIQUE VALUE OF \\'city\\' ATTRIBUTE IN RESTAURANTS = ', business['city'].nunique())\n",
        "df_cat = business[['categories']]\n",
        "distinct_categories_list = []\n",
        "for index, row in df_cat.iterrows():\n",
        "  if row['categories'] != None:\n",
        "    lst = [item.strip() for item in row['categories'].split(',')]\n",
        "    distinct_categories_list = list(set(distinct_categories_list + lst))\n",
        "print('UNIQUE VALUE OF \\'categories\\' ATTRIBUTE distinct values ', len(distinct_categories_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIQUE VALUE OF 'state' ATTRIBUTE IN RESTAURANTS =  36\n",
            "UNIQUE VALUE OF 'city' ATTRIBUTE IN RESTAURANTS =  1204\n",
            "UNIQUE VALUE OF 'categories' ATTRIBUTE distinct values  1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05AeyBn2gGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRINT THE state OF EACH user FOR HIS RESTAURANT reviews\n",
        "def statistics(users, reviews, business):\n",
        "  res = pd.DataFrame()\n",
        "  cont = 0\n",
        "  prec_user_id = '0'\n",
        "\n",
        "  for i, user in users.iterrows():\n",
        "    if cont == 1000:\n",
        "      break\n",
        "    user_id = user['user_id']  # 4, 9, 16, 21!, 22!, 23, 24\n",
        "    # Get all reviews of one user\n",
        "    # 'https://www.yelp.com/user_details_reviews_self?userid=NQffx45eJaeqhFcMadKUQA&rec_pagestart=90'??? no\n",
        "    user_reviews = reviews[reviews['user_id'] == user_id]\n",
        "    # Get all restaurants\n",
        "    business_ids = user_reviews[['business_id']]\n",
        "    result = business_ids.merge(business)\n",
        "    is_restaurant = result['categories'].str.contains('Restaurants', regex=False, na=False)\n",
        "    user_restaurant_reviews = result[is_restaurant]\n",
        "    user_restaurant_reviews['user_id'] = user_id\n",
        "    #user_restaurant_reviews.to_csv('user_restaurant_reviews' + str(cont) + '.csv')\n",
        "    res = res.append(user_restaurant_reviews.groupby(['user_id', 'state'])['user_id'].count().reset_index(name=\"review_count\"))\n",
        "    if (user_id != prec_user_id) and not user_restaurant_reviews.empty:\n",
        "      cont = cont + 1\n",
        "    # print(res)\n",
        "    prec_user_id = user_id\n",
        "\n",
        "  res.to_csv('result.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0M0ETqjMrqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_df_users(ranking):\n",
        "  df_ranking = pd.DataFrame(ranking, columns=['position', 'user_id', 'review_count', 'date']).sort_values('position')\n",
        "  return df_ranking"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6gUlnqBZ9eoa",
        "colab": {}
      },
      "source": [
        "# TODO: get review_count from web page\n",
        "def get_ranking(url):\n",
        "\n",
        "  i=1\n",
        "  reviews_text = []\n",
        "  reviews_date = []\n",
        "  reviews_userid = []\n",
        "  x=0\n",
        "\n",
        "  while 1:\n",
        "    if x == 0:\n",
        "      page_content = requests.get(url)\n",
        "    else:\n",
        "        page_content = requests.get(url + '?start=' + str(x))\n",
        "    x = x + 20\n",
        "    \n",
        "    tree = html.fromstring(page_content.content)\n",
        "    recommended_reviews_text = \"Recommended Reviews\"\n",
        "    reviews_list = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/*' % recommended_reviews_text)\n",
        "    if not reviews_list:\n",
        "      break\n",
        "    else:\n",
        "      # Index for ranking\n",
        "      j=1\n",
        "      for review in reviews_list:\n",
        "          reviews_text.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[p]/p/span' % (recommended_reviews_text, j))[0].text))\n",
        "          reviews_date.append((i, tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div[last()]/div[1]/div/div[2]/span' % (recommended_reviews_text, j))[0].text))\n",
        "          user_link = tree.xpath('//section[div[div[h3[text()=\"%s\"]]]]/div/div/ul/li[%d]/div/div/div/div/div/div/div/a/@href' % (recommended_reviews_text, j))[0]\n",
        "          reviews_userid.append((i,user_link[user_link.find('=')+1:]))\n",
        "          i = i + 1\n",
        "          j = j + 1\n",
        "\n",
        "\n",
        "  print(reviews_text)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_text)))\n",
        "  print(reviews_date)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_date)))\n",
        "  print(reviews_userid)\n",
        "  print(\"N. of reviews = \" + str(len(reviews_userid)))\n",
        "  return reviews_userid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHl6bORqNzeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_users_in_dataset(users):\n",
        "    data = []\n",
        "    with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            data.append(json_data)\n",
        "    df = pd.DataFrame(data)\n",
        "    df_ranking = pd.DataFrame(users, columns=['Position', 'user_id'])\n",
        "    df_merged = df_ranking.merge(df, on='user_id')\n",
        "    # temp = df[df['user_id'].isin(users)]\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def exists_in_dataset(userid):\n",
        "  found = False\n",
        "  with open(\"user.json\", 'r') as file:\n",
        "        for line in file:\n",
        "            json_data = json.loads(line)\n",
        "            if json_data['user_id'] == userid:\n",
        "              found = True\n",
        "  if found:\n",
        "    print(\"Trovato\")\n",
        "  else:\n",
        "    print(\"Non trovato\")\n",
        "  return found\n",
        "\n",
        "\n",
        "def find_users_in_chopped_dataset(users):\n",
        "    found = False\n",
        "    data = []\n",
        "    i = 1\n",
        "    j = 100000\n",
        "    df_users = pd.DataFrame()\n",
        "    while not found:\n",
        "        path = \"user\" + str(i) + \"-\" + str(j) + \".json\"\n",
        "        try:\n",
        "            with open(path, 'r') as file:\n",
        "                for line in file:\n",
        "                    json_data = json.loads(line)\n",
        "                    data.append(json_data)\n",
        "            df = pd.DataFrame(data)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Cannot find all users into the dataset\")\n",
        "            return 0\n",
        "        else:\n",
        "            temp = df[df['user_id'].isin(users)]  # df of users of the ranking\n",
        "            df_users = df_users.append(temp)\n",
        "            if df_users.shape[0] == len(users):\n",
        "                found = True\n",
        "            else:\n",
        "                i = i + 100000\n",
        "                j = j + 100000\n",
        "    return df_users\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}