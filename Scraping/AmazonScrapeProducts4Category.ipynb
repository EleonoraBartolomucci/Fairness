{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AmazonScrapeProducts4Category.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/Scraping/AmazonScrapeProducts4Category.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD5i5eJBlePm",
        "outputId": "947c37e7-d2cf-4573-ab18-5f8b20ac4a4d"
      },
      "source": [
        "!pip3 install python-dateutil lxml requests selectorlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (2.8.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Collecting selectorlib\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/3e/7ad0a01b07c066cf79c431324970869345e4d249242d70f20e939a5c630b/selectorlib-0.16.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: pyyaml>=3.12 in /usr/local/lib/python3.6/dist-packages (from selectorlib) (3.13)\n",
            "Collecting parsel>=1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from selectorlib) (7.1.2)\n",
            "Collecting cssselect>=0.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting w3lib>=1.19.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Installing collected packages: cssselect, w3lib, parsel, selectorlib\n",
            "Successfully installed cssselect-1.1.0 parsel-1.6.0 selectorlib-0.16.0 w3lib-1.22.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl0nPnD7VUxo"
      },
      "source": [
        "#IMPORT E FUNZIONI DI SUPPORTO\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "#from clarifai.rest import ClarifaiApp\n",
        "#from clarifai.rest import ApiError\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import pandas as pd\n",
        "import requests\n",
        "from lxml import html\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import math\n",
        "import io\n",
        "import math\n",
        "from google.colab import files\n",
        "from selectorlib import Extractor\n",
        "import requests \n",
        "from time import sleep\n",
        "from dateutil import parser as dateparser\n",
        "\n",
        "\n",
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "drive = authenticate()\n",
        "\n",
        "\n",
        "def create_folder_in_drive(gdrive, folder_name, parent_folder_id):\n",
        "  folder_metadata = {'title': folder_name,'mimeType': 'application/vnd.google-apps.folder',\n",
        "                    'parents': [{\"kind\": \"drive#fileLink\", \"id\": parent_folder_id}]\n",
        "                    }\n",
        "  folder = gdrive.CreateFile(folder_metadata)\n",
        "  folder.Upload()\n",
        "  print(folder)\n",
        "  # Return folder informations\n",
        "  print('title: %s, id: %s' % (folder['title'], folder['id']))\n",
        "  return folder['id']\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "  cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "  return df[cols]\n",
        "\n",
        "\n",
        "def upload_file(filename, folder_id):\n",
        "  drive = authenticate()\n",
        "  fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "  drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Check if file already exists in Google Drive (prevents duplicates)\n",
        "  for file in fileList:\n",
        "      if file['title'] == filename:  # The file already exists, then overwrite it\n",
        "          fileID = file['id']\n",
        "          drive_file = drive.CreateFile({'id': fileID, 'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Upload user picture on Google Drive\n",
        "  drive_file.SetContentFile(filename)  # path of local file content\n",
        "  drive_file.Upload()  # Upload the file.\n",
        "  return drive_file['id']\n",
        "\n",
        "\n",
        "def downloadUser(business_id, user_id, photo_folder, counter):\n",
        "    authenticate()\n",
        "    \n",
        "    filename = user_id + '.jpg'\n",
        "    url = 'https://www.yelp.com/user_details?userid=' + user_id\n",
        "    folder_id = photo_folder\n",
        "\n",
        "    # CHECK DUPLICATE\n",
        "    fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "    exists = False\n",
        "    # Check if file already exists in Google Drive (prevents duplicates)\n",
        "    for file in fileList:\n",
        "        if file['title'] == filename:  # The file already exists\n",
        "            exists = True\n",
        "    \n",
        "    if not exists:\n",
        "      try:\n",
        "        # Find user picture from web page\n",
        "        page_content = requests.get(url)\n",
        "        page_content.raise_for_status()\n",
        "        print(page_content)\n",
        "        tree = html.fromstring(page_content.content)\n",
        "        if len(tree.xpath('//*[@id=\"wrap\"]/div[2]/div[1]/div/div[2]/div[1]/div/div/div/a/img/@src')) == 0:\n",
        "          print('PICTURE NOT FOUND of user '+ user_id)\n",
        "        else: # Picture found\n",
        "          image_url = tree.xpath('//*[@id=\"wrap\"]/div[2]/div[1]/div/div[2]/div[1]/div/div/div/a/img/@src')[0]\n",
        "\n",
        "          drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "          \n",
        "          response = requests.get(image_url, stream=True)\n",
        "          \n",
        "          # Create a local copy of user picture\n",
        "          with open(filename, 'wb') as out_file:\n",
        "              shutil.copyfileobj(response.raw, out_file)\n",
        "          del response\n",
        "\n",
        "          # Upload user picture on Google Drive\n",
        "          drive_file.SetContentFile(filename)\n",
        "          drive_file.Upload()\n",
        "          if os.path.exists(filename):\n",
        "              os.remove(filename)\n",
        "          else:\n",
        "              print(\"The file does not exist\")\n",
        "          print(user_id + ' downloaded')\n",
        "      except:\n",
        "        #code = page_content.status_code\n",
        "        #if code==503:\n",
        "        #  print(\"ERROR 503\")\n",
        "        #if code==404:\n",
        "        #  print(\"User not found\")\n",
        "        print(\"Error\")\n",
        "\n",
        "def downloadUserWithProxy(business_id, user_id, photo_folder, counter, ip_addresses):\n",
        "    authenticate()\n",
        "    \n",
        "    filename = user_id + '.jpg'\n",
        "    url = 'https://www.yelp.com/user_details?userid=' + user_id\n",
        "    folder_id = photo_folder\n",
        "\n",
        "    # CHECK DUPLICATE\n",
        "    fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "    exists = False\n",
        "    # Check if file already exists in Google Drive (prevents duplicates)\n",
        "    for file in fileList:\n",
        "        if file['title'] == filename:  # The file already exists\n",
        "            exists = True\n",
        "    \n",
        "    if not exists:\n",
        "      downloaded = False\n",
        "      while len(ip_addresses) != 0 and not downloaded:\n",
        "        proxy_index = 0\n",
        "        proxy = {\"http\": ip_addresses[proxy_index], \"https\": ip_addresses[proxy_index]}\n",
        "        try:\n",
        "          # Check the proxy address\n",
        "          #response = requests.get('https://httpbin.org/ip',proxies=proxy, timeout=5)\n",
        "          #print(response.json())\n",
        "          \n",
        "          # Find user picture from web page\n",
        "          page_content = requests.get(url, proxies=proxy, timeout=10)\n",
        "          print(page_content)\n",
        "          if page_content.status_code != 503:\n",
        "            tree = html.fromstring(page_content.content)\n",
        "            if len(tree.xpath('//*[@id=\"wrap\"]/div[2]/div[1]/div/div[2]/div[1]/div/div/div/a/img/@src')) == 0:\n",
        "              print('PICTURE NOT FOUND of user '+ user_id)\n",
        "            else: # Picture found\n",
        "              image_url = tree.xpath('//*[@id=\"wrap\"]/div[2]/div[1]/div/div[2]/div[1]/div/div/div/a/img/@src')[0]\n",
        "\n",
        "              drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "              \n",
        "              response = requests.get(image_url, proxies=proxy, timeout=30, stream=True)\n",
        "              \n",
        "              # Create a local copy of user picture\n",
        "              with open(filename, 'wb') as out_file:\n",
        "                  shutil.copyfileobj(response.raw, out_file)\n",
        "              del response\n",
        "\n",
        "              # Upload user picture on Google Drive\n",
        "              drive_file.SetContentFile(filename)\n",
        "              drive_file.Upload()\n",
        "              if os.path.exists(filename):\n",
        "                  os.remove(filename)\n",
        "              else:\n",
        "                  print(\"The file does not exist\")\n",
        "              print(user_id + ' downloaded')\n",
        "              downloaded = True\n",
        "              ip_addresses.insert(0, ip_addresses.pop(proxy_index))\n",
        "          else:\n",
        "            print(\"Skipping. Connnection error\")\n",
        "            del ip_addresses[proxy_index]\n",
        "            print(\"Proxy rimanenti: \")\n",
        "            print(ip_addresses)\n",
        "            #restart_runtime()\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          print(\"Skipping. Connnection error\")\n",
        "          del ip_addresses[proxy_index]\n",
        "          print(\"Proxy rimanenti: \")\n",
        "          print(ip_addresses)\n",
        "      if len(ip_addresses)==0:\n",
        "        print(\"Nessun proxy disponibile\")\n",
        "\n",
        "\n",
        "def get_proxies():\n",
        "    url = 'https://free-proxy-list.net/'\n",
        "    response = requests.get(url)\n",
        "    parser = html.fromstring(response.content)\n",
        "    proxies = []\n",
        "    for i in parser.xpath('//tbody/tr')[:500]:\n",
        "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "            #Grabbing IP and corresponding PORT\n",
        "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
        "            proxies.append(proxy)\n",
        "    print(proxies)\n",
        "    proxies.append('18.223.213.237:3838')\n",
        "    proxies.append('18.223.103.221:3838')\n",
        "    proxies.append('117.69.230.23:9999')\n",
        "    with open('http_proxies.txt', 'w') as f:\n",
        "      for item in proxies:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "    return proxies\n",
        "\n",
        "def scrape(e, url):    \n",
        "  headers = {\n",
        "      'authority': 'www.amazon.com',\n",
        "      'pragma': 'no-cache',\n",
        "      'cache-control': 'no-cache',\n",
        "      'dnt': '1',\n",
        "      'upgrade-insecure-requests': '1',\n",
        "      'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
        "      'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "      'sec-fetch-site': 'none',\n",
        "      'sec-fetch-mode': 'navigate',\n",
        "      'sec-fetch-dest': 'document',\n",
        "      'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
        "  }\n",
        "\n",
        "  # Download the page using requests\n",
        "  print(\"Downloading %s\"%url)\n",
        "  r = requests.get(url, headers=headers)\n",
        "  # Simple check to check if page was blocked (Usually 503)\n",
        "  if r.status_code > 500:\n",
        "      if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
        "          print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
        "      else:\n",
        "          print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
        "      return None\n",
        "  # Pass the HTML of the page and create \n",
        "  return e.extract(r.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YK_zjWSp6I"
      },
      "source": [
        "#carico i file YML\n",
        "download = drive.CreateFile({'id': '12DmvHhjrBHI8rolxkkIeLYF6_ZynuUxT'})\n",
        "download.GetContentFile('AmazonReviews.yml')\n",
        "\n",
        "download2 = drive.CreateFile({'id': '1PuYdOat85pFtE1XC6__WIxeIe66BQuEp'})\n",
        "download2.GetContentFile('AmazonProducts.yml')\n",
        "\n",
        "download3 = drive.CreateFile({'id': '1s2JwxZqTHALk1VKqpe_shwMxLUIhKP-V'})\n",
        "download3.GetContentFile('CategorySearch.yml')\n",
        "\n",
        "download4 = drive.CreateFile({'id': '1YKZmpUdqKzCA2xOgYTgRU_QGso3NG1Xx'})\n",
        "download4.GetContentFile('AmazonUserPage.yml')\n",
        "\n",
        "# Create Extractors by reading from the YAML file\n",
        "userExtractor = Extractor.from_yaml_file('AmazonUserPage.yml')\n",
        "reviewsExtractor = Extractor.from_yaml_file('AmazonReviews.yml')\n",
        "productExtractor = Extractor.from_yaml_file('AmazonProducts.yml')\n",
        "categoryExtractor = Extractor.from_yaml_file('CategorySearch.yml') \n",
        "\n",
        "#inizializza queries\n",
        "download = drive.CreateFile({'id': '1V-L7IqaPVcBp4RvWKnUXt00SHrsT1e9O'})\n",
        "download.GetContentFile('queries.csv')\n",
        "\n",
        "queries = pd.read_csv(\"queries.csv\")\n",
        "queries = drop_unnamed(queries)\n",
        "\n",
        "for i, x in queries.iterrows():\n",
        "  if x[\"products_gtree\"]==-1:\n",
        "    df = pd.DataFrame(columns=[\"position\",\"asin\",\"url\"])\n",
        "    df.to_csv(str(x[\"cardinal\"])+ \"_gtree.csv\")\n",
        "    id_new_folder = create_folder_in_drive(drive, str(x[\"cardinal\"]), '13N3YrLs4LD8i4uZ5JJqrkEVCVUPq1Yje')\n",
        "    id_new_folder2 = create_folder_in_drive(drive, \"reviews_folder\", id_new_folder)\n",
        "    id_new_file = upload_file(str(x[\"cardinal\"])+ \"_gtree.csv\",id_new_folder)\n",
        "    id_new_folder3 = create_folder_in_drive(drive, \"fairness_data\", id_new_folder)\n",
        "    id_new_folder4 = create_folder_in_drive(drive, \"face_data\", id_new_folder)\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'products_folder'] = id_new_folder\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'products_gtree'] = id_new_file\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'reviews_folder'] = id_new_folder2\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'fairness_data'] = id_new_folder3\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'face_data'] = id_new_folder4\n",
        "    queries = drop_unnamed(queries)\n",
        "    queries_csv = queries.to_csv(\"queries.csv\")\n",
        "    upload_file(\"queries.csv\",\"1BJCILjUPI5gUIdkgT7nWwU_uz0VZhY7V\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHCmK_YITC8c"
      },
      "source": [
        "query=1\n",
        "products_gtree_id = queries.loc[queries[\"cardinal\"]==query,\"products_gtree\"].tolist()[0]\n",
        "products_folder_id = queries.loc[queries[\"cardinal\"]==query,\"products_folder\"].tolist()[0]\n",
        "reviews_folder_id = queries.loc[queries[\"cardinal\"]==query,\"reviews_folder\"].tolist()[0]\n",
        "fairness_data_id = queries.loc[queries[\"cardinal\"]==query,\"fairness_data\"].tolist()[0]\n",
        "face_data_id = queries.loc[queries[\"cardinal\"]==query,\"face_data\"].tolist()[0]\n",
        "\n",
        "download = drive.CreateFile({'id': products_gtree_id})\n",
        "download.GetContentFile(str(query)+'_gtree.csv')\n",
        "\n",
        "gtree = pd.read_csv(str(query)+'_gtree.csv')\n",
        "gtree = drop_unnamed(gtree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfwwRoHhTF2u"
      },
      "source": [
        "gtree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efmp9zqmTKnf"
      },
      "source": [
        "#MANUAL SCRAPE REVIEWS FOR PRODUCT\n",
        "\n",
        "asinList = gtree['asin'].tolist()\n",
        "for asin in asinList:\n",
        "  \n",
        "  drive = authenticate()\n",
        "  for counter in range(300):\n",
        "    print(counter)\n",
        "\n",
        "    gtree = pd.read_csv(str(query)+'_gtree.csv')\n",
        "    nextPage = gtree.loc[gtree['asin']==asin,\"nextPage\"].tolist()[0]\n",
        "    product_url = gtree.loc[gtree['asin']==asin,\"url\"].tolist()[0]\n",
        "    reviews_data_gtree_id = gtree.loc[gtree['asin']==asin,\"reviews_data\"].tolist()[0]\n",
        "\n",
        "    if not nextPage =='-1':\n",
        "\n",
        "      reviews_df = pd.DataFrame(columns=[\"name\",\"profileUrl\",\"reviewText\",\"voteText\",\"dateText\"])\n",
        "\n",
        "      if reviews_data_gtree_id=='-1' or reviews_data_gtree_id==-1:\n",
        "        review_csv = reviews_df.to_csv(asin+\"_reviews.csv\")\n",
        "        file_id = upload_file(asin+\"_reviews.csv\", reviews_folder_id)\n",
        "        gtree.loc[gtree['asin']==asin,\"reviews_data\"] = file_id\n",
        "        gtree = drop_unnamed(gtree)\n",
        "        gtree.to_csv(str(query)+'_gtree.csv')  \n",
        "        upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "      else:\n",
        "        download = drive.CreateFile({'id': reviews_data_gtree_id})\n",
        "        download.GetContentFile(asin+\"_reviews.csv\")\n",
        "\n",
        "        reviews_df = pd.read_csv(asin+\"_reviews.csv\")\n",
        "        reviews_df = drop_unnamed(reviews_df)\n",
        "\n",
        "      if product_url == nextPage: #inizia da capo\n",
        "        nextPage = \"https://www.amazon.com/product-reviews/\"+asin+\"/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
        "        temp_data=scrape(reviewsExtractor, nextPage)\n",
        "      else: #riprende da tot reviews\n",
        "        temp_data=scrape(reviewsExtractor, nextPage)\n",
        "\n",
        "      if 'nextPage' in temp_data.keys():\n",
        "        if temp_data['nextPage'] is not None:\n",
        "          gtree = pd.read_csv(str(query)+'_gtree.csv')\n",
        "          gtree.loc[gtree['asin']==asin,'nextPage'] = \"https://www.amazon.com\" + str(temp_data['nextPage'])\n",
        "          gtree = drop_unnamed(gtree)\n",
        "          gtree.to_csv(str(query)+'_gtree.csv')\n",
        "          upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "      print(temp_data)\n",
        "\n",
        "      while 'nextPage' in temp_data.keys():\n",
        "        print(\"nextPage\")\n",
        "        print(temp_data['nextPage'])\n",
        "        if temp_data['reviews'] is not None:\n",
        "          for review in temp_data['reviews']:\n",
        "            new_row = {\n",
        "                \"name\":review[\"name\"],\n",
        "                \"profileUrl\":review[\"profileUrl\"],\n",
        "                \"reviewText\":review[\"reviewText\"],\n",
        "                \"voteText\":review[\"voteText\"],\n",
        "                \"dateText\":review[\"dateText\"]\n",
        "            }\n",
        "            reviews_df = reviews_df.append(new_row,ignore_index=True)\n",
        "          reviews_df = drop_unnamed(reviews_df)\n",
        "          reviews_df.to_csv(asin+\"_reviews.csv\")\n",
        "          upload_file(asin+\"_reviews.csv\", reviews_folder_id)\n",
        "\n",
        "          if temp_data['nextPage'] is not None:#forse puoi portarlo fuori dal primo if\n",
        "            try:\n",
        "              nextPage_data = scrape(reviewsExtractor, \"https://www.amazon.com\" + str(temp_data['nextPage']))\n",
        "              print(nextPage_data)\n",
        "              gtree.loc[gtree['asin']==asin,'nextPage'] = \"https://www.amazon.com\" + temp_data['nextPage']\n",
        "              #gtree = drop_unnamed(gtree)\n",
        "              #gtree.to_csv(str(query)+'_gtree.csv')\n",
        "              #upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "            except:\n",
        "              gtree.loc[gtree['asin']==asin,'nextPage'] = \"https://www.amazon.com\" + temp_data['nextPage']\n",
        "              gtree = drop_unnamed(gtree)\n",
        "              gtree.to_csv(str(query)+'_gtree.csv')\n",
        "              upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "              nextPage_data={}\n",
        "              print(\"CANT SCRAPE NEXT PAGE\")\n",
        "          try:\n",
        "            pageNumber = temp_data['nextPage'].split(\"pageNumber=\")[1][0:3]\n",
        "            if pageNumber == str(501):\n",
        "              print(\"!!!501!!!\")\n",
        "              gtree.loc[gtree['asin']==asin,'nextPage'] = -1\n",
        "              gtree = drop_unnamed(gtree)\n",
        "              gtree.to_csv(str(query)+'_gtree.csv')\n",
        "              upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "          except:\n",
        "            print(\"boh\")\n",
        "        ###########################################################\n",
        "        if 'end' in temp_data.keys():\n",
        "          if temp_data['end'] is not None:\n",
        "            gtree.loc[gtree['asin']==asin,'nextPage'] = -1\n",
        "            gtree = drop_unnamed(gtree)\n",
        "            gtree.to_csv(str(query)+'_gtree.csv')\n",
        "            upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "        ###########################################################\n",
        "        if temp_data['nextPage'] is None:\n",
        "          nextPage_data={}\n",
        "        temp_data = nextPage_data\n",
        "        sleep(5)\n",
        "      gtree = drop_unnamed(gtree)\n",
        "      gtree.to_csv(str(query)+'_gtree.csv')\n",
        "      upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "      print(\"extraction completed for \"+asin)\n",
        "    else:\n",
        "      print(asin + \" alredy completed\")\n",
        "print(\"END END END\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Om7Wbx0QTMtf",
        "outputId": "e45aaacb-3351-4e71-8ebc-e9d296f1471f"
      },
      "source": [
        "#SEGNA ASIN COME COMPLETATO\n",
        "query=1\n",
        "asin=\"\"#inserire asin completato\n",
        "\n",
        "download = drive.CreateFile({'id': '1V-L7IqaPVcBp4RvWKnUXt00SHrsT1e9O'})\n",
        "download.GetContentFile('queries.csv')\n",
        "\n",
        "queries = pd.read_csv(\"queries.csv\")\n",
        "queries = drop_unnamed(queries)\n",
        "\n",
        "products_gtree_id = queries.loc[queries[\"cardinal\"]==query,\"products_gtree\"].tolist()[0]\n",
        "products_folder_id = queries.loc[queries[\"cardinal\"]==query,\"products_folder\"].tolist()[0]\n",
        "reviews_folder_id = queries.loc[queries[\"cardinal\"]==query,\"reviews_folder\"].tolist()[0]\n",
        "\n",
        "download = drive.CreateFile({'id': products_gtree_id})\n",
        "download.GetContentFile(str(query)+'_gtree.csv')\n",
        "\n",
        "gtree = pd.read_csv(str(query)+'_gtree.csv')\n",
        "gtree = drop_unnamed(gtree)\n",
        "\n",
        "gtree.loc[gtree[\"asin\"]==asin,\"nextPage\"]='-1'\n",
        "gtree = drop_unnamed(gtree)\n",
        "gtree.to_csv(str(query)+'_gtree.csv')\n",
        "upload_file(str(query)+'_gtree.csv',products_folder_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1ptG24HpoP5Nj5amLD7-VkYDJRxLBD9et'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}