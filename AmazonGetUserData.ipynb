{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AmazonGetUserData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDz+dITK91Q+uiH86+oVqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/AmazonGetUserData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD5i5eJBlePm",
        "outputId": "6c8bad10-d1fa-42c1-fe35-f816bcbe1596"
      },
      "source": [
        "!pip3 install python-dateutil lxml requests selectorlib gender_guesser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (2.8.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Collecting selectorlib\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/3e/7ad0a01b07c066cf79c431324970869345e4d249242d70f20e939a5c630b/selectorlib-0.16.0-py2.py3-none-any.whl\n",
            "Collecting gender_guesser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fb/3f2aac40cd2421e164cab1668e0ca10685fcf896bd6b3671088f8aab356e/gender_guesser-0.4.0-py2.py3-none-any.whl (379kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Collecting parsel>=1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from selectorlib) (7.1.2)\n",
            "Requirement already satisfied: pyyaml>=3.12 in /usr/local/lib/python3.6/dist-packages (from selectorlib) (3.13)\n",
            "Collecting cssselect>=0.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting w3lib>=1.19.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Installing collected packages: cssselect, w3lib, parsel, selectorlib, gender-guesser\n",
            "Successfully installed cssselect-1.1.0 gender-guesser-0.4.0 parsel-1.6.0 selectorlib-0.16.0 w3lib-1.22.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl0nPnD7VUxo"
      },
      "source": [
        "#IMPORT E FUNZIONI DI SUPPORTO\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "#from clarifai.rest import ClarifaiApp\n",
        "#from clarifai.rest import ApiError\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import pandas as pd\n",
        "import requests\n",
        "from lxml import html\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import math\n",
        "import io\n",
        "import math\n",
        "from google.colab import files\n",
        "from selectorlib import Extractor\n",
        "import requests \n",
        "from time import sleep\n",
        "from dateutil import parser as dateparser\n",
        "import gender_guesser.detector as gender\n",
        "\n",
        "\n",
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "drive = authenticate()\n",
        "\n",
        "\n",
        "def create_folder_in_drive(gdrive, folder_name, parent_folder_id):\n",
        "  folder_metadata = {'title': folder_name,'mimeType': 'application/vnd.google-apps.folder',\n",
        "                    'parents': [{\"kind\": \"drive#fileLink\", \"id\": parent_folder_id}]\n",
        "                    }\n",
        "  folder = gdrive.CreateFile(folder_metadata)\n",
        "  folder.Upload()\n",
        "  print(folder)\n",
        "  # Return folder informations\n",
        "  print('title: %s, id: %s' % (folder['title'], folder['id']))\n",
        "  return folder['id']\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "  cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "  return df[cols]\n",
        "\n",
        "\n",
        "def upload_file(filename, folder_id):\n",
        "  drive = authenticate()\n",
        "  fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "  drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Check if file already exists in Google Drive (prevents duplicates)\n",
        "  for file in fileList:\n",
        "      if file['title'] == filename:  # The file already exists, then overwrite it\n",
        "          fileID = file['id']\n",
        "          drive_file = drive.CreateFile({'id': fileID, 'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Upload user picture on Google Drive\n",
        "  drive_file.SetContentFile(filename)  # path of local file content\n",
        "  drive_file.Upload()  # Upload the file.\n",
        "  return drive_file['id']\n",
        "\n",
        "\n",
        "def downloadUser(business_id, user_id, photo_folder, counter):\n",
        "    authenticate()\n",
        "    \n",
        "    filename = user_id + '.jpg'\n",
        "    url = 'https://www.yelp.com/user_details?userid=' + user_id\n",
        "    folder_id = photo_folder\n",
        "\n",
        "    # CHECK DUPLICATE\n",
        "    fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "    exists = False\n",
        "    # Check if file already exists in Google Drive (prevents duplicates)\n",
        "    for file in fileList:\n",
        "        if file['title'] == filename:  # The file already exists\n",
        "            exists = True\n",
        "    \n",
        "    if not exists:\n",
        "      try:\n",
        "        # Find user picture from web page\n",
        "        page_content = requests.get(url)\n",
        "        page_content.raise_for_status()\n",
        "        print(page_content)\n",
        "        tree = html.fromstring(page_content.content)\n",
        "        if len(tree.xpath('//*[@id=\"wrap\"]/div[2]/div[1]/div/div[2]/div[1]/div/div/div/a/img/@src')) == 0:\n",
        "          print('PICTURE NOT FOUND of user '+ user_id)\n",
        "        else: # Picture found\n",
        "          image_url = tree.xpath('//*[@id=\"wrap\"]/div[2]/div[1]/div/div[2]/div[1]/div/div/div/a/img/@src')[0]\n",
        "\n",
        "          drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "          \n",
        "          response = requests.get(image_url, stream=True)\n",
        "          \n",
        "          # Create a local copy of user picture\n",
        "          with open(filename, 'wb') as out_file:\n",
        "              shutil.copyfileobj(response.raw, out_file)\n",
        "          del response\n",
        "\n",
        "          # Upload user picture on Google Drive\n",
        "          drive_file.SetContentFile(filename)\n",
        "          drive_file.Upload()\n",
        "          if os.path.exists(filename):\n",
        "              os.remove(filename)\n",
        "          else:\n",
        "              print(\"The file does not exist\")\n",
        "          print(user_id + ' downloaded')\n",
        "      except:\n",
        "        #code = page_content.status_code\n",
        "        #if code==503:\n",
        "        #  print(\"ERROR 503\")\n",
        "        #if code==404:\n",
        "        #  print(\"User not found\")\n",
        "        print(\"Error\")\n",
        "\n",
        "def downloadUserWithProxy(business_id, user_id, photo_folder, counter, ip_addresses):\n",
        "    authenticate()\n",
        "    \n",
        "    filename = user_id + '.jpg'\n",
        "    url = 'https://www.yelp.com/user_details?userid=' + user_id\n",
        "    folder_id = photo_folder\n",
        "\n",
        "    # CHECK DUPLICATE\n",
        "    fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "    exists = False\n",
        "    # Check if file already exists in Google Drive (prevents duplicates)\n",
        "    for file in fileList:\n",
        "        if file['title'] == filename:  # The file already exists\n",
        "            exists = True\n",
        "    \n",
        "    if not exists:\n",
        "      downloaded = False\n",
        "      while len(ip_addresses) != 0 and not downloaded:\n",
        "        proxy_index = 0\n",
        "        proxy = {\"http\": ip_addresses[proxy_index], \"https\": ip_addresses[proxy_index]}\n",
        "        try:\n",
        "          # Check the proxy address\n",
        "          #response = requests.get('https://httpbin.org/ip',proxies=proxy, timeout=5)\n",
        "          #print(response.json())\n",
        "          \n",
        "          # Find user picture from web page\n",
        "          page_content = requests.get(url, proxies=proxy, timeout=10)\n",
        "          print(page_content)\n",
        "          if page_content.status_code != 503:\n",
        "            tree = html.fromstring(page_content.content)\n",
        "            if len(tree.xpath('//*[@id=\"wrap\"]/div[2]/div[1]/div/div[2]/div[1]/div/div/div/a/img/@src')) == 0:\n",
        "              print('PICTURE NOT FOUND of user '+ user_id)\n",
        "            else: # Picture found\n",
        "              image_url = tree.xpath('//*[@id=\"wrap\"]/div[2]/div[1]/div/div[2]/div[1]/div/div/div/a/img/@src')[0]\n",
        "\n",
        "              drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "              \n",
        "              response = requests.get(image_url, proxies=proxy, timeout=30, stream=True)\n",
        "              \n",
        "              # Create a local copy of user picture\n",
        "              with open(filename, 'wb') as out_file:\n",
        "                  shutil.copyfileobj(response.raw, out_file)\n",
        "              del response\n",
        "\n",
        "              # Upload user picture on Google Drive\n",
        "              drive_file.SetContentFile(filename)\n",
        "              drive_file.Upload()\n",
        "              if os.path.exists(filename):\n",
        "                  os.remove(filename)\n",
        "              else:\n",
        "                  print(\"The file does not exist\")\n",
        "              print(user_id + ' downloaded')\n",
        "              downloaded = True\n",
        "              ip_addresses.insert(0, ip_addresses.pop(proxy_index))\n",
        "          else:\n",
        "            print(\"Skipping. Connnection error\")\n",
        "            del ip_addresses[proxy_index]\n",
        "            print(\"Proxy rimanenti: \")\n",
        "            print(ip_addresses)\n",
        "            #restart_runtime()\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          print(\"Skipping. Connnection error\")\n",
        "          del ip_addresses[proxy_index]\n",
        "          print(\"Proxy rimanenti: \")\n",
        "          print(ip_addresses)\n",
        "      if len(ip_addresses)==0:\n",
        "        print(\"Nessun proxy disponibile\")\n",
        "\n",
        "\n",
        "def get_proxies():\n",
        "    url = 'https://free-proxy-list.net/'\n",
        "    response = requests.get(url)\n",
        "    parser = html.fromstring(response.content)\n",
        "    proxies = []\n",
        "    for i in parser.xpath('//tbody/tr')[:500]:\n",
        "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "            #Grabbing IP and corresponding PORT\n",
        "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
        "            proxies.append(proxy)\n",
        "    print(proxies)\n",
        "    proxies.append('18.223.213.237:3838')\n",
        "    proxies.append('18.223.103.221:3838')\n",
        "    proxies.append('117.69.230.23:9999')\n",
        "    with open('http_proxies.txt', 'w') as f:\n",
        "      for item in proxies:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "    return proxies\n",
        "\n",
        "def scrape(e, url):    \n",
        "  headers = {\n",
        "      'authority': 'www.amazon.com',\n",
        "      'pragma': 'no-cache',\n",
        "      'cache-control': 'no-cache',\n",
        "      'dnt': '1',\n",
        "      'upgrade-insecure-requests': '1',\n",
        "      'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
        "      'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "      'sec-fetch-site': 'none',\n",
        "      'sec-fetch-mode': 'navigate',\n",
        "      'sec-fetch-dest': 'document',\n",
        "      'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
        "  }\n",
        "\n",
        "  # Download the page using requests\n",
        "  print(\"Downloading %s\"%url)\n",
        "  r = requests.get(url, headers=headers)\n",
        "  # Simple check to check if page was blocked (Usually 503)\n",
        "  if r.status_code > 500:\n",
        "      if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
        "          print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
        "      else:\n",
        "          print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
        "      return None\n",
        "  # Pass the HTML of the page and create \n",
        "  return e.extract(r.text)\n",
        "\n",
        "#####################################################################################################\n",
        "def detect_gender(id, userName):\n",
        "  d = gender.Detector(case_sensitive=False)\n",
        "  gendersList = []\n",
        "  nameList = []\n",
        "\n",
        "  genderResponse = d.get_gender(userName)\n",
        "\n",
        "  if genderResponse==\"unknown\":\n",
        "    nameList = userName.split(\" \")\n",
        "  print(nameList)\n",
        "  for nameElement in nameList:\n",
        "    if (d.get_gender(nameElement)!=\"unknown\") & (d.get_gender(nameElement)!=\"andy\"):\n",
        "      if (d.get_gender(nameElement)==\"mostly_female\"):\n",
        "        gendersList.append('female')\n",
        "      else:\n",
        "        if (d.get_gender(nameElement)==\"mostly_male\"):\n",
        "          gendersList.append('male')\n",
        "        else:\n",
        "          gendersList.append(d.get_gender(nameElement))\n",
        "  print(gendersList!=0)\n",
        "  if len(gendersList)!=0:\n",
        "    male = 0\n",
        "    female = 0\n",
        "    for e in gendersList:\n",
        "      if e==\"male\":\n",
        "        male = male + 1\n",
        "      else:\n",
        "        female = female + 1\n",
        "    if male==female:\n",
        "      genderResponse = gendersList[0]\n",
        "    else:\n",
        "      if male>female:\n",
        "        genderResponse = \"male\"\n",
        "      else:\n",
        "        genderResponse = \"female\"\n",
        "  #else:\n",
        "    #genderResponse == \"unknown\"\n",
        " \n",
        "  df_result = pd.DataFrame(columns=['user_id', 'age', 'gender', 'ethnicity'])\n",
        " \n",
        "  if genderResponse != \"unknown\":\n",
        "    if genderResponse == \"mostly_female\":\n",
        "      genderResponse = \"female\"\n",
        "    if genderResponse == \"mostly_male\":\n",
        "      genderResponse = \"male\"\n",
        "    if genderResponse == \"andy\":\n",
        "      genderResponse = np.NaN\n",
        "    new_row = {'user_id':id,\n",
        "                'age':'-2',\n",
        "                'gender':genderResponse,\n",
        "                'ethnicity':np.NaN}\n",
        "    df_result = df_result.append(new_row, ignore_index=True)\n",
        "  else:\n",
        "    #se non ci sono volti, stampa una stringa di default anziché vuoto\n",
        "    new_row = {'user_id':id,\n",
        "              'age':'-2',\n",
        "              'gender':np.NaN,\n",
        "              'ethnicity':np.NaN}\n",
        "    df_result = df_result.append(new_row, ignore_index=True)\n",
        "  return df_result\n",
        "#####################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eixZ1KEu-MUD"
      },
      "source": [
        "#carico i file YML\n",
        "download = drive.CreateFile({'id': '12DmvHhjrBHI8rolxkkIeLYF6_ZynuUxT'})\n",
        "download.GetContentFile('AmazonReviews.yml')\n",
        "\n",
        "download2 = drive.CreateFile({'id': '1PuYdOat85pFtE1XC6__WIxeIe66BQuEp'})\n",
        "download2.GetContentFile('AmazonProducts.yml')\n",
        "\n",
        "download3 = drive.CreateFile({'id': '1s2JwxZqTHALk1VKqpe_shwMxLUIhKP-V'})\n",
        "download3.GetContentFile('CategorySearch.yml')\n",
        "\n",
        "download4 = drive.CreateFile({'id': '1YKZmpUdqKzCA2xOgYTgRU_QGso3NG1Xx'})\n",
        "download4.GetContentFile('AmazonUserPage.yml')\n",
        "\n",
        "# Create Extractors by reading from the YAML file\n",
        "userExtractor = Extractor.from_yaml_file('AmazonUserPage.yml')\n",
        "reviewsExtractor = Extractor.from_yaml_file('AmazonReviews.yml')\n",
        "productExtractor = Extractor.from_yaml_file('AmazonProducts.yml')\n",
        "categoryExtractor = Extractor.from_yaml_file('CategorySearch.yml') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NXhM0--wUj7"
      },
      "source": [
        "#inizializza queries\n",
        "download = drive.CreateFile({'id': '1V-L7IqaPVcBp4RvWKnUXt00SHrsT1e9O'})\n",
        "download.GetContentFile('queries.csv')\n",
        "\n",
        "queries = pd.read_csv(\"queries.csv\")\n",
        "queries = drop_unnamed(queries)\n",
        "\n",
        "for i, x in queries.iterrows():\n",
        "  if x[\"products_gtree\"]==-1:\n",
        "    df = pd.DataFrame(columns=[\"position\",\"asin\",\"url\"])\n",
        "    df.to_csv(str(x[\"cardinal\"])+ \"_gtree.csv\")\n",
        "    id_new_folder = create_folder_in_drive(drive, str(x[\"cardinal\"]), '13N3YrLs4LD8i4uZ5JJqrkEVCVUPq1Yje')\n",
        "    id_new_folder2 = create_folder_in_drive(drive, \"reviews_folder\", id_new_folder)\n",
        "    id_new_file = upload_file(str(x[\"cardinal\"])+ \"_gtree.csv\",id_new_folder)\n",
        "    id_new_folder3 = create_folder_in_drive(drive, \"fairness_data\", id_new_folder)\n",
        "    id_new_folder4 = create_folder_in_drive(drive, \"face_data\", id_new_folder)\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'products_folder'] = id_new_folder\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'products_gtree'] = id_new_file\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'reviews_folder'] = id_new_folder2\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'fairness_data'] = id_new_folder3\n",
        "    queries.loc[queries[\"cardinal\"]==x[\"cardinal\"], 'face_data'] = id_new_folder4\n",
        "    queries = drop_unnamed(queries)\n",
        "    queries_csv = queries.to_csv(\"queries.csv\")\n",
        "    upload_file(\"queries.csv\",\"1BJCILjUPI5gUIdkgT7nWwU_uz0VZhY7V\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaaaBusUwVH_"
      },
      "source": [
        "query=1\n",
        "products_gtree_id = queries.loc[queries[\"cardinal\"]==query,\"products_gtree\"].tolist()[0]\n",
        "products_folder_id = queries.loc[queries[\"cardinal\"]==query,\"products_folder\"].tolist()[0]\n",
        "reviews_folder_id = queries.loc[queries[\"cardinal\"]==query,\"reviews_folder\"].tolist()[0]\n",
        "fairness_data_id = queries.loc[queries[\"cardinal\"]==query,\"fairness_data\"].tolist()[0]\n",
        "face_data_id = queries.loc[queries[\"cardinal\"]==query,\"face_data\"].tolist()[0]\n",
        "\n",
        "download = drive.CreateFile({'id': products_gtree_id})\n",
        "download.GetContentFile(str(query)+'_gtree.csv')\n",
        "\n",
        "gtree = pd.read_csv(str(query)+'_gtree.csv')\n",
        "gtree = drop_unnamed(gtree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgyD0U7GtwEd"
      },
      "source": [
        "asinList = gtree['asin'].tolist()\n",
        "asin_count = 0\n",
        "for asin in asinList:\n",
        "  asin_count = asin_count+1\n",
        "  drive = authenticate()\n",
        "\n",
        "  products_reviews_id = gtree.loc[gtree[\"asin\"]==asin,\"reviews_data\"].tolist()[0]\n",
        "  download = drive.CreateFile({'id': products_reviews_id})\n",
        "  download.GetContentFile(asin+'_reviews.csv')\n",
        "  reviews_df = pd.read_csv(asin+'_reviews.csv')\n",
        "  reviews_df = drop_unnamed(reviews_df)\n",
        "\n",
        "  gfolder_clarifai_id = gtree.loc[gtree[\"asin\"]==asin,\"gfolder_clarifai\"].tolist()[0]\n",
        "  \n",
        "  genderize_df = pd.DataFrame(columns=[\"user_id\",\"age\",\"gender\",\"ethnicity\"])\n",
        "\n",
        "  print(reviews_df)\n",
        "  count = 0\n",
        "  for i,review in reviews_df.iterrows():\n",
        "    count = count + 1\n",
        "    user_id=-1\n",
        "    if not pd.isna(review[\"profileUrl\"]):\n",
        "      user_id = review[\"profileUrl\"].split(\"/\")[3]\n",
        "      user = \"{'profileUrl':'https://www.amazon.com/gp/profile/\" + review[\"profileUrl\"].split(\"/\")[3] +\"/ref=cm_cr_getr_d_gw_btm?ie=UTF8','profileImg':'https://www.amazon.com/avatar/default/\" + review[\"profileUrl\"].split(\"/\")[3] + \"?square=true&max_width=460'}\"\n",
        "    else:\n",
        "      user = \"{}\"\n",
        "      user_id = \"missingAccount.\" + review[\"name\"]\n",
        "\n",
        "    print(count, user_id)\n",
        "    minidf_result = detect_gender(user_id, str(review[\"name\"])) \n",
        "    genderize_df = genderize_df.append(minidf_result, ignore_index=True)\n",
        "    \n",
        "  print(genderize_df)\n",
        "  genderize_df = drop_unnamed(genderize_df)\n",
        "  genderize_df.to_csv(str(asin_count) + ' - '+ asin+'.csv')\n",
        "  upload_file(str(asin_count) + ' - '+ asin+'.csv',\"1o_ajBxjV1RO9_DyrDUTiBTJN6RyX9yk_\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Zwa57IRRmv"
      },
      "source": [
        "drive = authenticate()\n",
        "genderize_df = drop_unnamed(genderize_df)\n",
        "genderize_df.to_csv(str(1) + ' - '+ asin+'.csv')\n",
        "upload_file(str(1) + ' - '+ asin+'.csv',\"1o_ajBxjV1RO9_DyrDUTiBTJN6RyX9yk_\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdp8iUek_DXk"
      },
      "source": [
        "asinList = gtree['asin'].tolist()\n",
        "for asin in asinList:\n",
        "  drive = authenticate()\n",
        "  product_position = gtree.loc[gtree[\"asin\"]==asin,\"position\"].tolist()[0]\n",
        "\n",
        "  products_reviews_id = gtree.loc[gtree[\"asin\"]==asin,\"reviews_data\"].tolist()[0]\n",
        "\n",
        "  download = drive.CreateFile({'id': products_reviews_id})\n",
        "  download.GetContentFile(asin+'_reviews.csv')\n",
        "\n",
        "  reviews_df = pd.read_csv(asin+'_reviews.csv')\n",
        "  reviews_df = drop_unnamed(reviews_df)\n",
        "\n",
        "  gfolder_clarifai_id = gtree.loc[gtree[\"asin\"]==asin,\"gfolder_clarifai\"].tolist()[0]\n",
        "  gfolder_rankings_id = gtree.loc[gtree[\"asin\"]==asin,\"gfolder_rankings\"].tolist()[0]\n",
        "\n",
        "  if gfolder_clarifai_id == -1 or gfolder_clarifai_id == '-1' or gfolder_clarifai_id=='-1.0':\n",
        "    df = pd.DataFrame(columns=[\"user_id\",\"age\",\"gender\",\"ethnicity\"])\n",
        "    df.to_csv(str(product_position) + ' - '+ asin+'.csv')\n",
        "    file_id = upload_file(str(product_position) + ' - '+ asin+'.csv',face_data_id)\n",
        "    gtree.loc[gtree[\"asin\"]==asin,\"gfolder_clarifai\"]=file_id\n",
        "    gtree = drop_unnamed(gtree)\n",
        "    gtree.to_csv(str(query)+'_gtree.csv')\n",
        "    upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "\n",
        "  if gfolder_rankings_id == -1 or gfolder_rankings_id == '-1' or gfolder_rankings_id=='-1.0':\n",
        "    print(\"OK\")\n",
        "    folder_id = create_folder_in_drive(drive,str(product_position) + ' - '+ asin, fairness_data_id)\n",
        "    gtree.loc[gtree[\"asin\"]==asin,\"gfolder_rankings\"]=folder_id\n",
        "    gtree = drop_unnamed(gtree)\n",
        "    gtree.to_csv(str(query)+'_gtree.csv')\n",
        "    upload_file(str(query)+'_gtree.csv',products_folder_id)\n",
        "    \n",
        "  gfolder_clarifai_id = gtree.loc[gtree[\"asin\"]==asin,\"gfolder_clarifai\"].tolist()[0]\n",
        "  gfolder_rankings_id = gtree.loc[gtree[\"asin\"]==asin,\"gfolder_rankings\"].tolist()[0]\n",
        "\n",
        "  df = pd.DataFrame(columns=[\"position\",\"user_id\",\"user\",\"comment\",\"feedback\",\"date\",\"name\",\"country\"])\n",
        "  for i,review in reviews_df.iterrows():\n",
        "    #user_id\n",
        "    user_id=-1\n",
        "    if not pd.isna(review[\"profileUrl\"]):\n",
        "      user_id = review[\"profileUrl\"].split(\"/\")[3]\n",
        "      user = \"{'profileUrl':'https://www.amazon.com/gp/profile/\" + review[\"profileUrl\"].split(\"/\")[3] +\"/ref=cm_cr_getr_d_gw_btm?ie=UTF8','profileImg':'https://www.amazon.com/avatar/default/\" + review[\"profileUrl\"].split(\"/\")[3] + \"?square=true&max_width=460'}\"\n",
        "    else:\n",
        "      user = \"{}\"\n",
        "      user_id = \"missingAccount.\" + review[\"name\"]\n",
        "    #comment\n",
        "    if not pd.isna(review[\"reviewText\"]):\n",
        "      comment = \"{'text':'\"+review[\"reviewText\"]+\"'}\"\n",
        "    else:\n",
        "      comment = \"{}\"\n",
        "    #feedback\n",
        "    if not pd.isna(review[\"voteText\"]):\n",
        "      vote = review[\"voteText\"].split(\" \")[0]\n",
        "      if vote == \"One\":\n",
        "        vote = 1\n",
        "      else: vote = int(vote.replace(',', ''))\n",
        "    else:\n",
        "      vote = 0\n",
        "    feedback = \"{'counts': {'useful':\"+str(vote)+\"}}\"\n",
        "    #date & country\n",
        "    if not pd.isna(review[\"dateText\"]):\n",
        "      year = review[\"dateText\"].split(\" on \")[1].split(\", \")[1]\n",
        "      month = review[\"dateText\"].split(\" on \")[1].split(\", \")[0].split(\" \")[0]\n",
        "      if month == \"January\":\n",
        "        month = \"01\"\n",
        "      if month == \"February\":\n",
        "        month = \"02\"\n",
        "      if month == \"March\":\n",
        "        month = \"03\"\n",
        "      if month == \"April\":\n",
        "        month = \"04\"\n",
        "      if month == \"May\":\n",
        "        month = \"05\"\n",
        "      if month == \"June\":\n",
        "        month = \"06\"\n",
        "      if month == \"July\":\n",
        "        month = \"07\"\n",
        "      if month == \"August\":\n",
        "        month = \"08\"\n",
        "      if month == \"September\":\n",
        "        month = \"09\"\n",
        "      if month == \"October\":\n",
        "        month = \"10\"\n",
        "      if month == \"November\":\n",
        "        month = \"11\"\n",
        "      if month == \"December\":\n",
        "        month = \"12\"\n",
        "      day = review[\"dateText\"].split(\" on \")[1].split(\", \")[0].split(\" \")[1]\n",
        "      country = review[\"dateText\"].split(\" on \")[0].split(\"in \")[1]\n",
        "      date = str(year) + \"-\" + str(month) + \"-\" + str(day)\n",
        "    else:\n",
        "      date = -1\n",
        "      country = -1\n",
        "    \n",
        "    new_row = {\n",
        "        \"position\":i+1,\n",
        "        \"user_id\":user_id,\n",
        "        \"user\":user,\n",
        "        \"comment\":comment,\n",
        "        \"feedback\":feedback,\n",
        "        \"date\":date,\n",
        "        \"name\":review[\"name\"],\n",
        "        \"country\":country\n",
        "    }\n",
        "    df = df.append(new_row,ignore_index=True)\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv(\"ranking_\"+asin+\".csv\")\n",
        "  upload_file(\"ranking_\"+asin+\".csv\",gfolder_rankings_id)\n",
        "\n",
        "  #gender-guesser DATA\n",
        "\n",
        "  genderize_df = pd.DataFrame(columns=[\"user_id\",\"age\",\"gender\",\"ethnicity\"])\n",
        "  count = 0\n",
        "  for i,review in df.iterrows():\n",
        "    count = count + 1\n",
        "    print(count, review[\"user_id\"])\n",
        "    minidf_result = detect_gender(review[\"user_id\"], str(review[\"name\"])) \n",
        "    genderize_df = genderize_df.append(minidf_result, ignore_index=True)\n",
        "    \n",
        "  genderize_df = drop_unnamed(genderize_df)\n",
        "  genderize_df.to_csv(str(product_position) + ' - '+ asin+'.csv')\n",
        "  upload_file(str(product_position) + ' - '+ asin+'.csv',face_data_id)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}