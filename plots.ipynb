{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plots.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/plots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj80wj3Dy2hR"
      },
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import datetime\n",
        "import math\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics as st\n",
        "from scipy import stats\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#!pip install balanced_kmeans\n",
        "#from balanced_kmeans import kmeans\n",
        "#from balanced_kmeans import kmeans_equal\n",
        "\n",
        "import networkx as nx\n",
        "import time\n",
        "\n",
        "# CONSTANTS\n",
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nQut-dmy4J7"
      },
      "source": [
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "drive = authenticate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yunryvwcy7WZ"
      },
      "source": [
        "def upload_file(filename, folder_id):\n",
        "  drive = authenticate()\n",
        "  fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "  drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Check if file already exists in Google Drive (prevents duplicates)\n",
        "  for file in fileList:\n",
        "      if file['title'] == filename:  # The file already exists, then overwrite it\n",
        "          fileID = file['id']\n",
        "          drive_file = drive.CreateFile({'id': fileID, 'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Upload user picture on Google Drive\n",
        "  drive_file.SetContentFile(filename)  # path of local file content\n",
        "  drive_file.Upload()  # Upload the file.\n",
        "  return drive_file['id']\n",
        "  \n",
        "\n",
        "def create_folder_in_drive(gdrive, folder_name, parent_folder_id):\n",
        "  folder_metadata = {'title': folder_name,'mimeType': 'application/vnd.google-apps.folder',\n",
        "                    'parents': [{\"kind\": \"drive#fileLink\", \"id\": parent_folder_id}]\n",
        "                    }\n",
        "  folder = gdrive.CreateFile(folder_metadata)\n",
        "  folder.Upload()\n",
        "  print(folder)\n",
        "  # Return folder informations\n",
        "  print('title: %s, id: %s' % (folder['title'], folder['id']))\n",
        "  return folder['id']\n",
        "\n",
        "\n",
        "# READ JSON\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "  cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "  return df[cols]\n",
        "\n",
        "\n",
        "# JOIN USER FROM YELP WITH USER FROM DATASET AND PRINT LOST USERS\n",
        "def filter_user_from_dataset(df, users):\n",
        "  df = df[['position','user_id','date','location']] # Tolgo alcune colonne perch√© prendo quelle del dataset\n",
        "  df_merged = df.merge(users, on='user_id')\n",
        "  total_review = df['position'].max()\n",
        "  print('Utenti persi: totali ' + str(total_review) + ' - utenti nel dataset ' +\n",
        "        str(len(df_merged.index)) + ' = ' + str(total_review - len(df_merged.index)))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "  df_merged = drop_unnamed(df_merged)\n",
        "  return df_merged\n",
        "\n",
        "\n",
        "def compute_groups_percents(df_groups, N_of_groups):\n",
        "  total = len(df_groups.index)\n",
        "  percents = []\n",
        "  i = 0\n",
        "  while i < N_of_groups:\n",
        "    current_length = len(df_groups[df_groups['group_id'] == i].index)\n",
        "    percents.append((current_length/total)*100)\n",
        "    i = i + 1\n",
        "  print(percents)\n",
        "  return percents\n",
        "\n",
        "\n",
        "# ADD REVIEW INFO IN RANKING DATAFRAME\n",
        "def integrate_review_info(df, reviews, business_id):\n",
        "  business_reviews = reviews[reviews['business_id'] == business_id]\n",
        "  df_merged = business_reviews.merge(df, on='user_id')\n",
        "  del df_merged['date_y']  # it's date from yelp website (not updated in dataset)\n",
        "  df_merged = df_merged.rename(columns={'date_x':'date'})\n",
        "  # drop duplicates reviews from same user\n",
        "  df_merged = df_merged.sort_values('date').drop_duplicates('user_id',keep='last')\n",
        "  df_merged = df_merged.sort_values(by=['position']).reset_index(drop=True)\n",
        "  print('Review perse: ', len(df.index) - len(df_merged.index))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "  df_merged = drop_unnamed(df_merged)\n",
        "  df_merged = df_merged[['position', 'user_id', 'review_id', 'date', 'name', 'location', 'text', 'fans', 'average_stars', 'review_count', 'business_id']]\n",
        "  \n",
        "  return df_merged\n",
        "\n",
        "import ast\n",
        "def reorder_user_data(df):\n",
        "  df['review_count'] = np.NaN\n",
        "  df['friend_count'] = np.NaN\n",
        "  df['location'] = np.NaN\n",
        "  df['photo_count'] = np.NaN\n",
        "  df['useful_votes'] = np.NaN\n",
        "  df['funny_votes'] = np.NaN\n",
        "  df['cool_votes'] = np.NaN\n",
        "  df['text'] = \"\"\n",
        "  for i, user in df.iterrows():\n",
        "    print(i)\n",
        "    user_data = user['user']\n",
        "    user_data_converted = ast.literal_eval(user_data)\n",
        "    df.loc[i, 'review_count'] = user_data_converted[\"reviewCount\"]\n",
        "    df.loc[i, 'friend_count'] = user_data_converted[\"friendCount\"]\n",
        "    df.loc[i, 'location'] = user_data_converted['displayLocation']\n",
        "    df.loc[i, 'photo_count'] = user_data_converted['photoCount']\n",
        "    comment_data = user['comment']\n",
        "    comment_data_converted = ast.literal_eval(comment_data)\n",
        "    df.loc[i, 'text'] = comment_data_converted[\"text\"]\n",
        "    feedback_data = user['feedback']\n",
        "    feedback_data_converted = ast.literal_eval(feedback_data)\n",
        "    counts_data = feedback_data_converted['counts']\n",
        "    #counts_data_converted = ast.literal_eval(counts_data)\n",
        "    df.loc[i, 'useful_votes'] = counts_data['useful']\n",
        "    df.loc[i, 'funny_votes'] = counts_data['funny']\n",
        "    df.loc[i, 'cool_votes'] = counts_data['cool']\n",
        "  print('REORDER-----------------------------------------')\n",
        "  print(df)\n",
        "  df = drop_unnamed(df)\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "def generate_dummies(df, text_attribute_list):\n",
        "  dummy_columns = []\n",
        "  for attr in text_attribute_list:\n",
        "    gender_dummies = pd.get_dummies(df[attr])\n",
        "    dummy_columns = dummy_columns + list(gender_dummies.columns)\n",
        "    df = pd.merge(df, gender_dummies, how=\"left\",left_index=True, right_index=True)\n",
        "    \n",
        "    # drop all the unnamed columns\n",
        "    df = drop_unnamed(df)\n",
        "  return df, dummy_columns\n",
        "\n",
        "\n",
        "def create_vectors_yelp(df_users, business_id, list_of_attributes, method, destination):\n",
        "  vectors = df_users[['user_id', 'review_count', 'friend_count', 'location', 'photo_count']]\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  # SENTIMENT\n",
        "  #vectors = get_sentiment(vectors, business_id)\n",
        "\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "\n",
        "def get_sentiment(vectors, id):\n",
        "  df_sentiment = pd.read_csv('sentiment_' + id + '.csv')\n",
        "  new_vectors = pd.merge(vectors, df_sentiment, on='user_id', how='left')\n",
        "  new_vectors = drop_unnamed(new_vectors)\n",
        "  return new_vectors\n",
        "\n",
        "\n",
        "def get_demographics(vectors, id):\n",
        "  demographics_file_id = gtree.loc[gtree['business_id']==id, 'gfolder_clarifai'].tolist()[0]\n",
        "  download = drive.CreateFile({'id': demographics_file_id})\n",
        "  download.GetContentFile('demographics_'  + id + '.csv')\n",
        "  df_demographics = pd.read_csv('demographics_' + id + '.csv')\n",
        "  df_demographics = df_demographics.sort_values(['user_id', 'age'], ascending=[True, False])\n",
        "  df_demographics = df_demographics.drop_duplicates('user_id',keep='first').reset_index(drop=True)\n",
        "  new_vectors = pd.merge(vectors, df_demographics, on='user_id', how='left')\n",
        "  new_vectors = drop_unnamed(new_vectors)\n",
        "  return new_vectors\n",
        "\n",
        "\n",
        "\n",
        "def pipeline1(business_id, alluser):\n",
        "  ### READ THE RANKING FROM GTREE\n",
        "  local_ranking_folder = gtree.loc[gtree['business_id']==id, 'gfolder_rankings'].tolist()[0]\n",
        "  print(local_ranking_folder)\n",
        "  local_ranking_files = drive.ListFile({'q': \"'\" + local_ranking_folder + \"' in parents and trashed=false\"}).GetList()\n",
        "  download = drive.CreateFile({'id': local_ranking_files[0]['id']})\n",
        "  download.GetContentFile(local_ranking_files[0]['title'])\n",
        "  print(local_ranking_files[0]['id'])\n",
        "  df_date_ranking = pd.read_csv(local_ranking_files[0]['title'])\n",
        "\n",
        "  download = drive.CreateFile({'id': local_ranking_files[1]['id']})\n",
        "  download.GetContentFile(local_ranking_files[1]['title'])\n",
        "  print(local_ranking_files[1]['id'])\n",
        "  df_rand_ranking = pd.read_csv(local_ranking_files[1]['title'])\n",
        "\n",
        "  download = drive.CreateFile({'id': local_ranking_files[2]['id']})\n",
        "  download.GetContentFile(local_ranking_files[2]['title'])\n",
        "  print(local_ranking_files[2]['id'])\n",
        "  df_rel_ranking = pd.read_csv(local_ranking_files[2]['title'])\n",
        "  \n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = reorder_user_data(df_rel_ranking)\n",
        "  df_date_ranking = reorder_user_data(df_date_ranking)\n",
        "  df_rand_ranking = reorder_user_data(df_rand_ranking)\n",
        "    \n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Yelp filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline2(df_ranking_by_relevance, id, N_of_groups,\n",
        "              local_list_of_attributes, method, alluser, destination, assumption):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "\n",
        "  if alluser:\n",
        "    df_vectors = create_vectors_yelp(df_ranking_by_relevance, id, local_list_of_attributes,\n",
        "                                     method, destination)\n",
        "    \n",
        "  if method == 'custom':\n",
        "    df_groups, destination = create_groups_custom(N_of_groups,local_list_of_attributes,id, destination, assumption)\n",
        "  \n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "  \n",
        "  print(df_groups)\n",
        "  print(percents)\n",
        "  return df_groups, percents, destination\n",
        "\n",
        "\n",
        "def pipeline3(business_id, method, df_ranking_by_relevance, df_ranking_by_date, \n",
        "              df_ranking_by_random, percents, list_of_attributes,\n",
        "              alluser, destination, df_groups):\n",
        "  \n",
        "  df_result = pd.DataFrame(columns=['Exposure_method', 'Context', 'Means', 'P-value'])\n",
        "  df_result.to_csv('result.csv')\n",
        "  id_new_file = upload_file('result.csv', destination)\n",
        "\n",
        "\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  print(\"----------------DEMOGRAPHIC PARITY EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                    method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes, destination)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                    method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes, destination)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                      method, df_ranking_by_random,\n",
        "                                                       \"rand_\", list_of_attributes, destination)\n",
        "  \n",
        "  array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"demgr_yelp_\", business_id, array_group_id, destination) # group_id\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"demgr_date_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                    'scatter_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "\n",
        "  print(\"----------------DISPARATE IMPACT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                   \"yelp_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                   \"date_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                     \"rand_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"dispimp_yelp_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"dispimp_date_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                    'scatter_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "\n",
        "\n",
        "def create_groups_custom(N_of_groups, list_of_attributes, id, destination, assumption):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "\n",
        "  local_list_of_attributes = list_of_attributes\n",
        "\n",
        "  if set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity', 'review_sentiment'])):\n",
        "    text_attribute_list = ['gender', 'ethnicity']\n",
        "    df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "    # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "    # subtraction\n",
        "    temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "    list_of_attributes = temp + dummy_columns_name\n",
        "    #serve per la descrizione dei gruppi, da fare\n",
        "\n",
        "    #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "    df_no_info = df_vectors[df_vectors[local_list_of_attributes[0]].isnull()]\n",
        "    df_yes_info = df_vectors[df_vectors[local_list_of_attributes[0]].notnull()]\n",
        "    print('All users =', len(df_vectors))\n",
        "    print('Users with info =',len(df_yes_info.index))\n",
        "    print('User without info =',len(df_no_info.index))\n",
        "  \n",
        "  else:\n",
        "    df_no_info = pd.DataFrame()\n",
        "    df_yes_info = df_vectors\n",
        "\n",
        "  \n",
        "  # GRUPPI PER GENERE\n",
        "  if local_list_of_attributes == ['gender']:\n",
        "    if assumption=='':\n",
        "      femmine = df_yes_info[df_yes_info['gender']=='feminine']\n",
        "      maschi = df_yes_info[df_yes_info['gender']=='masculine']\n",
        "      new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "      new_df_users['group_id'] = np.NaN\n",
        "      for j, user in femmine.iterrows():\n",
        "        new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "        \n",
        "      for j, user in maschi.iterrows():\n",
        "        new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    else:\n",
        "      new_df_users, destination = do_assumptions(df_yes_info, df_no_info,\n",
        "                                                 destination, assumption)\n",
        "      df_no_info = df_no_info.iloc[0:0]\n",
        "\n",
        "  # GRUPPI PER REVIEW SENTIMENT\n",
        "  if local_list_of_attributes == ['review_sentiment']:\n",
        "    df_yes_info = df_yes_info[['user_id', 'review_sentiment']]\n",
        "    print(df_yes_info)\n",
        "    positivi = df_yes_info[df_yes_info['review_sentiment'].str.startswith('positive')]\n",
        "    negativi = df_yes_info[df_yes_info['review_sentiment'].str.startswith('negative')]\n",
        "    neutrali = df_yes_info[df_yes_info['review_sentiment'].str.startswith('neutral')]\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in positivi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in negativi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in neutrali.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "    \n",
        "  # GRUPPI PER ETNIA\n",
        "  if local_list_of_attributes == ['ethnicity']:\n",
        "    bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "    neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "    altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in bianchi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in neri.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in altri.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "\n",
        "  # GRUPPI PER ETNIA, GENERE\n",
        "  if local_list_of_attributes == ['gender','ethnicity']:\n",
        "    bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "    b_under_39 = bianchi[(bianchi['age']<39)]\n",
        "    b_over_39 = bianchi[(bianchi['age']>=39)]\n",
        "    b_fem = bianchi[bianchi['gender']=='feminine'].reset_index(drop=True)\n",
        "    b_mas = bianchi[bianchi['gender']=='masculine'].reset_index(drop=True)\n",
        "    \n",
        "    neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "    n_under_39 = neri[(neri['age']<39)]\n",
        "    n_over_39 = neri[(neri['age']>=39)]\n",
        "    n_fem = neri[neri['gender']=='feminine'].reset_index(drop=True)\n",
        "    n_mas = neri[neri['gender']=='masculine'].reset_index(drop=True)\n",
        "    \n",
        "    altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "    a_under_39 = altri[(altri['age']<39)]\n",
        "    a_over_39 = altri[(altri['age']>=39)]\n",
        "    a_fem = altri[altri['gender']=='feminine'].reset_index(drop=True)\n",
        "    a_mas = altri[altri['gender']=='masculine'].reset_index(drop=True)\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in b_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in b_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in n_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "      \n",
        "    for j, user in n_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 4\n",
        "      \n",
        "    for j, user in a_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 5\n",
        "      \n",
        "    for j, user in a_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 6\n",
        "\n",
        "  if not df_no_info.empty:\n",
        "    # ADD THE EXCLUDED IN LAST C\n",
        "    new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "    new_df_users = drop_unnamed(new_df_users)\n",
        "    new_df_users = new_df_users.fillna(0)\n",
        "\n",
        "  #destination = set_file_destination(local_list_of_attributes, 'custom', id)\n",
        "\n",
        "  new_df_users.to_csv('groups_custom_' + id + '.csv')\n",
        "  upload_file('groups_custom_' + id + '.csv',destination)\n",
        "  return new_df_users, destination\n",
        "\n",
        "\n",
        "def do_assumptions(df_yes, df_no_info, destination, assumption):\n",
        "  total = len(df_yes.index)+len(df_no_info.index)\n",
        "  already_men = len(df_yes[df_yes['gender']=='masculine'].values.tolist())\n",
        "  # ALL IN ONE GROUP\n",
        "  if assumption=='all_men':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'All_men', destination)\n",
        "    df_no = df_no_info\n",
        "    df_no['gender'].fillna('masculine', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  if assumption=='all_women':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'All_women', destination)\n",
        "    df_no = df_no_info\n",
        "    df_no['gender'] = 'feminine'\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # 50%\n",
        "  if assumption=='50':\n",
        "    men_size = total//2\n",
        "    id_new_folder = create_folder_in_drive(drive, '50and50', destination)\n",
        "    df_no = df_no_info\n",
        "    while (already_men+len(df_no[df_no['gender']=='masculine'].values.tolist()))<=men_size:\n",
        "      index_found = df_no.index[df_no['gender'].isnull()].tolist()[0]\n",
        "      df_no.loc[index_found, 'gender'] = 'masculine'\n",
        "    df_no['gender'].fillna('feminine', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # EQUALLY DISTRIBUTED\n",
        "  if assumption=='equal':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'Equally_distributed', destination)\n",
        "    df_no = df_no_info\n",
        "    even = 0\n",
        "    for ind, row in df_no.iterrows():\n",
        "      if even==0:\n",
        "        even=1\n",
        "        df_no.loc[ind,'gender']='masculine'\n",
        "      else:\n",
        "        df_no.loc[ind,'gender']='feminine'\n",
        "        even=0\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # MAINTAIN PROPORTION\n",
        "  if assumption=='proportioned':\n",
        "    #male_size:partial=new_male_size:total\n",
        "    partial = len(df_yes.index)\n",
        "    men_size = (total*(len(df_yes[df_yes['gender']=='masculine'].reset_index(drop=True).index)))//partial\n",
        "    id_new_folder = create_folder_in_drive(drive, 'Maintaining_proportion', destination)\n",
        "    df_no = df_no_info\n",
        "    while (already_men+len(df_no[df_no['gender']=='masculine'].values.tolist()))<=men_size:\n",
        "      index_found = df_no.index[df_no['gender'].isnull()].tolist()[0]\n",
        "      df_no.loc[index_found, 'gender'] = 'masculine'\n",
        "    df_no['gender'].fillna('feminine', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  femmine = df[df['gender']=='feminine']\n",
        "  maschi = df[df['gender']=='masculine']\n",
        "\n",
        "  new_df_users = df[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in femmine.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 0\n",
        "  for j, user in maschi.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "  return new_df_users, id_new_folder\n",
        "\n",
        "\n",
        "def print_disparate_impact_exposure(business_id, method,  df_ranking,\n",
        "                                    filename, list_of_attributes, alluser, destination):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = disparate_impact_exposure(df_groups[df_groups['group_id'] == i], df_ranking,user_exposures, business_id, i, alluser)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  filePath = stat_significance_inter_groups(df_groups, 'dispimp_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "\n",
        "  exposures.to_csv('exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "\n",
        "\n",
        "def print_demographic_parity_exposure(business_id, method, df_ranking, filename,\n",
        "                                      list_of_attributes, destination):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = demographic_parity_exposure(df_groups[df_groups['group_id'] == i], df_ranking, user_exposures, i)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  filePath = stat_significance_inter_groups(df_groups, 'demgr_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "  '''array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  couples = list(itertools.combinations(array_group_id,2))\n",
        "  couples = [(x,y) for (x,y) in couples if x!=y]\n",
        "  print(couples) # [(0,1), (0,2), (1,2)]\n",
        "  filePath = 'stat_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  for id1,id2 in couples:\n",
        "    group1 = user_exposures[user_exposures['group_id'] == id1]['exposure']\n",
        "    group2 = user_exposures[user_exposures['group_id'] == id2]['exposure']\n",
        "    result = stats.ttest_ind(group1, group2)\n",
        "    percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "    percent2 = 100*(len(group2.index)/len(user_exposures.index))\n",
        "    print('+++++++++TEST STATISTICAL SIGNIFICANT+++++++++')\n",
        "    print('GROUP '+ str(id1) +':')\n",
        "    print(user_exposures[user_exposures['group_id'] == id1])\n",
        "    print('GROUP '+ str(id2) +':')\n",
        "    print(user_exposures[user_exposures['group_id'] == id2])\n",
        "    print('RESULT:')\n",
        "    print(result)\n",
        "    with open(filePath, 'a') as output:\n",
        "      output.write('Percent group '+ str(id1) +': ' + str(percent1) + '\\nPercent group '+\n",
        "                   str(id2) +': ' + str(percent2) + '\\nResult: ')\n",
        "      output.write(str(result)+'\\n\\n')'''\n",
        "\n",
        "  exposures.to_csv('exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "\n",
        "\n",
        "def stat_significance_inter_groups(df_groups, filename, business_id, method, df_user_exposures, destination):\n",
        "  array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  couples = list(itertools.combinations(array_group_id,2))\n",
        "  couples = [(x,y) for (x,y) in couples if x!=y]\n",
        "  print(\"Group couples:\", couples) # [(0,1), (0,2), (1,2)]\n",
        "  filePath = 'stat_groups_' + method + '_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  ciclo = 0\n",
        "  while ciclo < 3:\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('Without first and last ' + str(ciclo) + ' rows:\\n\\n')\n",
        "    user_exposures = df_user_exposures.iloc[ciclo:(len(df_user_exposures.index)-ciclo)]\n",
        "    print('Size:', len(user_exposures.index))\n",
        "    for id1,id2 in couples:\n",
        "      group1 = user_exposures[user_exposures['group_id'] == id1]['exposure']\n",
        "      group2 = user_exposures[user_exposures['group_id'] == id2]['exposure']\n",
        "      array1 = group1.values\n",
        "      array2 = group2.values\n",
        "      result = stats.ks_2samp(array1, array2)\n",
        "      percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "      percent2 = 100*(len(group2.index)/len(user_exposures.index))\n",
        "      with open(filePath, 'a') as output:\n",
        "        output.write('Percent GROUP '+ str(id1) +': ' + str(percent1) + '\\nPercent GROUP '+\n",
        "                    str(id2) +': ' + str(percent2) + '\\nResult: ')\n",
        "        output.write(str(result)+'\\n\\n')\n",
        "      if result[1] <= 0.05:\n",
        "        update_result_table(filename[:-6], filename[-5:-1], (int(id1),int(id2)),\n",
        "                          result[1], destination)\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('----------------------------------------\\n\\n')\n",
        "    ciclo = ciclo + 1\n",
        "  return filePath\n",
        "\n",
        "def stat_significance_inter_rankings(df_user_exposures, df_random_user_exposures, method,\n",
        "                                     filename, business_id, group_ids, destination):\n",
        "  filePath = 'stat_rankings_' + method + '_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  ciclo = 0\n",
        "  while ciclo < 3:\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('Without first and last ' + str(ciclo) + ' rows:\\n\\n')\n",
        "    user_exposures = df_user_exposures.iloc[ciclo:(len(df_user_exposures.index)-ciclo)]\n",
        "    random_user_exposures = df_random_user_exposures.iloc[ciclo:(len(df_random_user_exposures.index)-ciclo)]\n",
        "    for group_id in group_ids:\n",
        "      group1 = user_exposures[user_exposures['group_id'] == group_id]['exposure']\n",
        "      group2 = random_user_exposures[random_user_exposures['group_id'] == group_id]['exposure']\n",
        "      array1 = group1.values\n",
        "      array2 = group2.values\n",
        "      result = stats.ks_2samp(array1, array2)\n",
        "      percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "      percent2 = 100*(len(group2.index)/len(random_user_exposures.index))\n",
        "      print('+++++++++TEST STAT SIGNIFICANCE INTER RANKINGS+++++++++')\n",
        "      print('RESULT:')\n",
        "      print(result)\n",
        "      with open(filePath, 'a') as output:\n",
        "        output.write('Percent GROUP '+ str(group_id) +': ' + str(percent1) + '\\nResult: ')\n",
        "        output.write(str(result)+'\\n\\n')\n",
        "      if result[1] <= 0.05:\n",
        "        update_result_table(filename[:-6], group_id, (filename[-5:-1],'random'), result[1], destination)\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('----------------------------------------\\n\\n')\n",
        "    ciclo = ciclo + 1\n",
        "  return filePath\n",
        "\n",
        "\n",
        "def update_result_table(exp_method, context, means, p_value, destination):\n",
        "  file_list = drive.ListFile({'q': \"'\" + destination + \"' in parents and trashed=false\"}).GetList()\n",
        "  for file in file_list:\n",
        "    if file['title']=='result.csv':\n",
        "      id_result_file = file['id']\n",
        "  download = drive.CreateFile({'id': id_result_file}) # id file gtree.csv\n",
        "  download.GetContentFile('result.csv')\n",
        "  df = pd.read_csv('result.csv')\n",
        "  new_row = {'Exposure_method':exp_method, 'Context':context,\n",
        "             'Means':str(means[0])+' - '+str(means[1]), 'P-value':p_value}\n",
        "  df = df.append(new_row, ignore_index=True)\n",
        "  df = drop_unnamed(df)\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('result.csv')\n",
        "  upload_file('result.csv', destination) # id folder data\n",
        "\n",
        "\n",
        "def disparate_impact_exposure(df_group, ranking, user_exposures, business_id, group_index, alluser):\n",
        "  all_exposures = []\n",
        "  for i, user in df_group.iterrows():\n",
        "    user_id = user['user_id']\n",
        "\n",
        "    if alluser:\n",
        "      useful = ranking[ranking['user_id'] == user_id]['useful_votes'].values[0]\n",
        "      funny = ranking[ranking['user_id'] == user_id]['funny_votes'].values[0]\n",
        "      cool = ranking[ranking['user_id'] == user_id]['cool_votes'].values[0]\n",
        "    \n",
        "    counts = useful + funny + cool + 2\n",
        "    \n",
        "    base = 2  # con 10 i valori sono troppo bassi\n",
        "    counts = math.log(counts, base)\n",
        "    \n",
        "    position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "    \n",
        "    current_exp = exp(position) * counts\n",
        "    \n",
        "    all_exposures.append(current_exp)\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "  print('all_exposures size:', len(all_exposures))\n",
        "  mean = st.mean(all_exposures)\n",
        "  return mean, user_exposures\n",
        "\n",
        "\n",
        "def demographic_parity_exposure(df_group, ranking, user_exposures, group_index):\n",
        "    all_exposures = []\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "        current_exp = exp(position)\n",
        "        all_exposures.append(current_exp)\n",
        "        user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "        user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "    print('all_exposures size:', len(all_exposures))\n",
        "    mean = st.mean(all_exposures)\n",
        "    return mean, user_exposures\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/(np.log(1 + position))\n",
        "  \n",
        "\n",
        "\n",
        "def get_plots(yelp_exposures, date_exposures, random_exposures, percents, title,\n",
        "              list_of_attributes, method, business_id, alluser, errors, destination):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  print(y1)\n",
        "\n",
        "  '''y1_error = np.std(y1)\n",
        "  y2_error = np.std(y2)\n",
        "  y3_error = np.std(y3)'''\n",
        "  '''# I have missing user when using dataset OR using demographic attributes\n",
        "  if alluser and set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    y1_error = st.pstdev(y1)\n",
        "    y2_error = st.pstdev(y2)\n",
        "    y3_error = st.pstdev(y3)\n",
        "  else:\n",
        "    y1_error = st.stdev(y1)\n",
        "    y2_error = st.stdev(y2)\n",
        "    y3_error = st.stdev(y3)'''\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='yelp', yerr=errors[0], capsize=5)\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date', yerr=errors[1], capsize=5)\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random', yerr=errors[2], capsize=5)\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0),this_range)\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def get_scatter_plots(yelp_user_exposures, date_user_exposures, random_user_exposures, title,\n",
        "              list_of_attributes, method, business_id, alluser, destination):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 1.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_user_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_user_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_user_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_user_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_user_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_user_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  plt.scatter(x1,y1, s=10, color='red', label='yelp')\n",
        "  plt.scatter(x2,y2, s=10, color='green', label='date')\n",
        "  plt.scatter(x3,y3, s=10, color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  #this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0))\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}