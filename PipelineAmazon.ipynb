{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PipelineAmazon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8weNPPnruWghI2O8jOFpl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBartolomucci/Fairness/blob/master/PipelineAmazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD5i5eJBlePm",
        "outputId": "37346832-03a8-4810-faa4-fecfaf529f58"
      },
      "source": [
        "!pip3 install python-dateutil lxml requests selectorlib gender_guesser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (2.8.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting selectorlib\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/3e/7ad0a01b07c066cf79c431324970869345e4d249242d70f20e939a5c630b/selectorlib-0.16.0-py2.py3-none-any.whl\n",
            "Collecting gender_guesser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fb/3f2aac40cd2421e164cab1668e0ca10685fcf896bd6b3671088f8aab356e/gender_guesser-0.4.0-py2.py3-none-any.whl (379kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: pyyaml>=3.12 in /usr/local/lib/python3.7/dist-packages (from selectorlib) (3.13)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from selectorlib) (7.1.2)\n",
            "Collecting parsel>=1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
            "Collecting w3lib>=1.19.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting cssselect>=0.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: w3lib, cssselect, parsel, selectorlib, gender-guesser\n",
            "Successfully installed cssselect-1.1.0 gender-guesser-0.4.0 parsel-1.6.0 selectorlib-0.16.0 w3lib-1.22.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl0nPnD7VUxo"
      },
      "source": [
        "#IMPORT, FUNZIONI PRINCIPALI E METODI DI SUPPORTO\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "#from clarifai.rest import ClarifaiApp\n",
        "#from clarifai.rest import ApiError\n",
        "import ast\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import pandas as pd\n",
        "import requests\n",
        "from lxml import html\n",
        "import shutil\n",
        "import itertools\n",
        "import os\n",
        "import pickle\n",
        "import datetime\n",
        "import math\n",
        "import io\n",
        "import math\n",
        "from google.colab import files\n",
        "from selectorlib import Extractor\n",
        "import requests \n",
        "from time import sleep\n",
        "from dateutil import parser as dateparser\n",
        "import gender_guesser.detector as gender\n",
        "\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics as st\n",
        "from scipy import stats\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# CONSTANTS\n",
        "business_headers = ['index', 'business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n",
        "\n",
        "# AUTHENTICATE IN GOOGLE DRIVE\n",
        "def authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n",
        "drive = authenticate()\n",
        "\n",
        "\n",
        "def create_folder_in_drive(gdrive, folder_name, parent_folder_id):\n",
        "  folder_metadata = {'title': folder_name,'mimeType': 'application/vnd.google-apps.folder',\n",
        "                    'parents': [{\"kind\": \"drive#fileLink\", \"id\": parent_folder_id}]\n",
        "                    }\n",
        "  folder = gdrive.CreateFile(folder_metadata)\n",
        "  folder.Upload()\n",
        "  print(folder)\n",
        "  # Return folder informations\n",
        "  print('title: %s, id: %s' % (folder['title'], folder['id']))\n",
        "  return folder['id']\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "  cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "  return df[cols]\n",
        "\n",
        "\n",
        "def upload_file(filename, folder_id):\n",
        "  drive = authenticate()\n",
        "  fileList = drive.ListFile({'q': \"'\" + folder_id + \"' in parents and trashed=false\"}).GetList()\n",
        "  drive_file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Check if file already exists in Google Drive (prevents duplicates)\n",
        "  for file in fileList:\n",
        "      if file['title'] == filename:  # The file already exists, then overwrite it\n",
        "          fileID = file['id']\n",
        "          drive_file = drive.CreateFile({'id': fileID, 'title': filename, 'parents': [{'id': folder_id}]})\n",
        "  # Upload user picture on Google Drive\n",
        "  drive_file.SetContentFile(filename)  # path of local file content\n",
        "  drive_file.Upload()  # Upload the file.\n",
        "  return drive_file['id']\n",
        "\n",
        "def read_json(json_path):\n",
        "  data = []\n",
        "  with open(json_path, \"r\") as my_file: \n",
        "    for line in my_file:\n",
        "      line_json = json.loads(line)\n",
        "      data.append(line_json)\n",
        "  return data\n",
        "\n",
        "\n",
        "# PARSE JSON IN CSV\n",
        "def json2csv(csv_path, json_path):\n",
        "  data = read_json(json_path)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(csv_path)\n",
        "\n",
        "\n",
        "def drop_unnamed(df):\n",
        "  cols = [c for c in df.columns if c.lower()[:7] != 'unnamed']\n",
        "  return df[cols]\n",
        "\n",
        "# JOIN USER FROM AMAZON WITH USER FROM DATASET AND PRINT LOST USERS\n",
        "def filter_user_from_dataset(df, users):\n",
        "  df = df[['position','user_id','date','location']] # Tolgo alcune colonne perché prendo quelle del dataset\n",
        "  df_merged = df.merge(users, on='user_id')\n",
        "  total_review = df['position'].max()\n",
        "  print('Utenti persi: totali ' + str(total_review) + ' - utenti nel dataset ' +\n",
        "        str(len(df_merged.index)) + ' = ' + str(total_review - len(df_merged.index)))\n",
        "  df_merged['position'] = df_merged.index + 1\n",
        "  df_merged = drop_unnamed(df_merged)\n",
        "  return df_merged\n",
        "\n",
        "def compute_groups_percents(df_groups, N_of_groups):\n",
        "  total = len(df_groups.index)\n",
        "  percents = []\n",
        "  i = 0\n",
        "  while i < N_of_groups:\n",
        "    current_length = len(df_groups[df_groups['group_id'] == i].index)\n",
        "    percents.append((current_length/total)*100)\n",
        "    i = i + 1\n",
        "  print(percents)\n",
        "  return percents\n",
        "\n",
        "# ADD REVIEW INFO IN RANKING DATAFRAME\n",
        "#def integrate_review_info(df, reviews, business_id):\n",
        "  #business_reviews = reviews[reviews['business_id'] == business_id]\n",
        "  #df_merged = business_reviews.merge(df, on='user_id')\n",
        "  #del df_merged['date_y']  # it's date from amazon website (not updated in dataset)\n",
        "  #df_merged = df_merged.rename(columns={'date_x':'date'})\n",
        "  ## drop duplicates reviews from same user\n",
        "  #df_merged = df_merged.sort_values('date').drop_duplicates('user_id',keep='last')\n",
        "  #df_merged = df_merged.sort_values(by=['position']).reset_index(drop=True)\n",
        "  #print('Review perse: ', len(df.index) - len(df_merged.index))\n",
        "  #df_merged['position'] = df_merged.index + 1\n",
        "  #df_merged = drop_unnamed(df_merged)\n",
        "  #df_merged = df_merged[['position', 'user_id', 'review_id', 'date', 'name', 'location', 'text', 'fans', 'average_stars', 'review_count', 'business_id']]\n",
        "  \n",
        "  #return df_merged\n",
        "\n",
        "def reorder_user_data(df):\n",
        "  #df['review_count'] = np.NaN\n",
        "  #df['location'] = np.NaN\n",
        "  #df['useful_votes'] = np.NaN\n",
        "  #df['position'] = np.NaN\n",
        "  #df['text'] = \"\"\n",
        "  for i, user in df.iterrows():\n",
        "    print(i)\n",
        "    user_data = user['user']\n",
        "    user_data_converted = ast.literal_eval(user_data)\n",
        "    \n",
        "    #df.loc[i, 'review_count'] = i+1\n",
        "    #df.loc[i, 'friend_count'] = user_data_converted[\"friendCount\"]\n",
        "    #df.loc[i, 'location'] = user_data_converted['country']\n",
        "    #df.loc[i, 'photo_count'] = user_data_converted['photoCount']\n",
        "    comment_data = user['comment']\n",
        "    #comment_data_converted = ast.literal_eval(comment_data)\n",
        "    df.loc[i, 'text'] = comment_data[0]\n",
        "    feedback_data = user['feedback']\n",
        "    feedback_data_converted = ast.literal_eval(feedback_data)\n",
        "    counts_data = feedback_data_converted['counts']\n",
        "    #counts_data_converted = ast.literal_eval(counts_data)\n",
        "    df.loc[i, 'useful_votes'] = int(counts_data['useful'])\n",
        "    df.loc[i, 'position'] = int(user['position'])\n",
        "  print('REORDER-----------------------------------------')\n",
        "  print(df)\n",
        "  df = drop_unnamed(df)\n",
        "  return df\n",
        "\n",
        "def generate_dummies(df, text_attribute_list):\n",
        "  dummy_columns = []\n",
        "  for attr in text_attribute_list:\n",
        "    gender_dummies = pd.get_dummies(df[attr])\n",
        "    dummy_columns = dummy_columns + list(gender_dummies.columns)\n",
        "    df = pd.merge(df, gender_dummies, how=\"left\",left_index=True, right_index=True)\n",
        "    \n",
        "    # drop all the unnamed columns\n",
        "    df = drop_unnamed(df)\n",
        "  return df, dummy_columns\n",
        "\n",
        "def create_vectors_yelp(df_users, business_id, list_of_attributes, method, destination):\n",
        "  vectors = df_users[['user_id']]#, 'review_count', 'friend_count']]#, 'location', 'photo_count']]\n",
        "  # DEMOGRAPHICS\n",
        "  vectors = get_demographics(vectors, business_id)\n",
        "  # SENTIMENT\n",
        "  #vectors = get_sentiment(vectors, business_id)\n",
        "\n",
        "  print(vectors.columns)\n",
        "  # UPLOAD VECTORS IN DRIVE\n",
        "  vectors.to_csv('user_vectors_' + business_id + '.csv')\n",
        "  upload_file('user_vectors_' + business_id + '.csv',destination)\n",
        "  return vectors\n",
        "\n",
        "def get_sentiment(vectors, id):\n",
        "  df_sentiment = pd.read_csv('sentiment_' + id + '.csv')\n",
        "  new_vectors = pd.merge(vectors, df_sentiment, on='user_id', how='left')\n",
        "  new_vectors = drop_unnamed(new_vectors)\n",
        "  return new_vectors\n",
        "\n",
        "\n",
        "def get_demographics(vectors, id):\n",
        "  print(id)\n",
        "  demographics_file_id = gtree.loc[gtree['asin']==id, 'gfolder_clarifai'].tolist()[0]\n",
        "  download = drive.CreateFile({'id': demographics_file_id})\n",
        "  download.GetContentFile('demographics_'  + id + '.csv')\n",
        "  df_demographics = pd.read_csv('demographics_' + id + '.csv')\n",
        "  df_demographics = df_demographics.sort_values(['user_id', 'age'], ascending=[True, False])\n",
        "  df_demographics = df_demographics.drop_duplicates('user_id',keep='first').reset_index(drop=True)\n",
        "  new_vectors = pd.merge(vectors, df_demographics, on='user_id', how='left')\n",
        "  new_vectors = drop_unnamed(new_vectors)\n",
        "  return new_vectors\n",
        "\n",
        "def pipeline1(business_id, alluser):\n",
        "  ### READ THE RANKING FROM GTREE\n",
        "  local_ranking_folder = gtree.loc[gtree['asin']==id, 'gfolder_rankings'].tolist()[0]\n",
        "  local_ranking_files = drive.ListFile({'q': \"'\" + local_ranking_folder + \"' in parents and trashed=false\"}).GetList()\n",
        "\n",
        "  download = drive.CreateFile({'id': local_ranking_files[0]['id']})\n",
        "  download.GetContentFile(local_ranking_files[0]['title'])\n",
        "\n",
        "  tmp_rel_ranking = pd.read_csv(local_ranking_files[0]['title'])\n",
        "  tmp_rel_ranking = drop_unnamed(tmp_rel_ranking)\n",
        "  \n",
        "  tmp_date_ranking = tmp_rel_ranking.sort_values(by=['date'],ascending=True)\n",
        "  tmp_rand_ranking = tmp_rel_ranking.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  df_rel_ranking = pd.DataFrame(columns=['user_id', 'user', 'comment', 'feedback', 'date','name','country'])\n",
        "  counter = 0\n",
        "  for i, rank in tmp_rel_ranking.iterrows():\n",
        "    counter = counter + 1\n",
        "    row = {\n",
        "        'position':int(counter),\n",
        "        'user_id':rank['user_id'],\n",
        "        'user':rank['user'],\n",
        "        'comment':rank['comment'],\n",
        "        'feedback':rank['feedback'],\n",
        "        'date':rank['date'],\n",
        "        'name':rank['name'],\n",
        "        'country':rank['country']\n",
        "    }\n",
        "    df_rel_ranking = df_rel_ranking.append(row, ignore_index=True)\n",
        "\n",
        "  df_date_ranking = pd.DataFrame(columns=['user_id', 'user', 'comment', 'feedback', 'date','name','country'])\n",
        "  counter = 0\n",
        "  for i, rank in tmp_date_ranking.iterrows():\n",
        "    counter = counter + 1\n",
        "    row = {\n",
        "        'position':int(counter),\n",
        "        'user_id':rank['user_id'],\n",
        "        'user':rank['user'],\n",
        "        'comment':rank['comment'],\n",
        "        'feedback':rank['feedback'],\n",
        "        'date':rank['date'],\n",
        "        'name':rank['name'],\n",
        "        'country':rank['country']\n",
        "    }\n",
        "    df_date_ranking = df_date_ranking.append(row, ignore_index=True)\n",
        "\n",
        "  df_rand_ranking = pd.DataFrame(columns=['user_id', 'user', 'comment', 'feedback', 'date','name','country'])\n",
        "  counter = 0\n",
        "  for i, rank in tmp_rand_ranking.iterrows():\n",
        "    counter = counter + 1\n",
        "    row = {\n",
        "        'position':int(counter),\n",
        "        'user_id':rank['user_id'],\n",
        "        'user':rank['user'],\n",
        "        'comment':rank['comment'],\n",
        "        'feedback':rank['feedback'],\n",
        "        'date':rank['date'],\n",
        "        'name':rank['name'],\n",
        "        'country':rank['country']\n",
        "    }\n",
        "    df_rand_ranking = df_rand_ranking.append(row, ignore_index=True)\n",
        "  \n",
        "  print(\"Ranking by Amazon filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  ###\n",
        "\n",
        "  df_rel_ranking = reorder_user_data(df_rel_ranking)\n",
        "  df_date_ranking = reorder_user_data(df_date_ranking)\n",
        "  df_rand_ranking = reorder_user_data(df_rand_ranking)\n",
        "    \n",
        "\n",
        "  print(\"\\n++++++++++++++++ RANKING ++++++++++++++++++\\n\")\n",
        "\n",
        "  print(\"Ranking by Amazon filter:\")\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(df_rel_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking by Date:\")\n",
        "  print(df_date_ranking)\n",
        "  print('\\n')\n",
        "  print(\"Ranking Random:\")\n",
        "  print(df_rand_ranking)\n",
        "  print('\\n')\n",
        "\n",
        "  return df_rel_ranking, df_date_ranking, df_rand_ranking\n",
        "\n",
        "\n",
        "def pipeline2(df_ranking_by_relevance, id, N_of_groups,\n",
        "              local_list_of_attributes, method, alluser, destination, assumption):\n",
        "  print(\"\\n++++++++++++++++ GROUPS CREATION ++++++++++++++++++\\n\")\n",
        "  print(id)\n",
        "  if alluser:\n",
        "    df_vectors = create_vectors_yelp(df_ranking_by_relevance, id, local_list_of_attributes,\n",
        "                                     method, destination)\n",
        "    \n",
        "  if method == 'custom':\n",
        "    df_groups, destination = create_groups_custom(N_of_groups,local_list_of_attributes,id, destination, assumption)\n",
        "  \n",
        "  percents = compute_groups_percents(df_groups, N_of_groups)\n",
        "  \n",
        "  print(df_groups)\n",
        "  print(percents)\n",
        "  return df_groups, percents, destination\n",
        "\n",
        "\n",
        "def pipeline3(business_id, method, df_ranking_by_relevance, df_ranking_by_date, \n",
        "              df_ranking_by_random, percents, list_of_attributes,\n",
        "              alluser, destination, df_groups):\n",
        "  \n",
        "  df_result = pd.DataFrame(columns=['Exposure_method', 'Context', 'Means', 'P-value'])\n",
        "  df_result.to_csv('result.csv')\n",
        "  id_new_file = upload_file('result.csv', destination)\n",
        "\n",
        "\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  print(\"----------------DEMOGRAPHIC PARITY EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Amazon filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                    method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes, destination)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                    method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes, destination)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_demographic_parity_exposure(business_id,\n",
        "                                                      method, df_ranking_by_random,\n",
        "                                                       \"rand_\", list_of_attributes, destination)\n",
        "  \n",
        "  array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"demgr_yelp_\", business_id, array_group_id, destination) # group_id\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"demgr_date_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, random_exposures,\n",
        "                    'scatter_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "  get_distribution_plots(yelp_user_exposures, date_user_exposures, random_exposures,\n",
        "                    'distribution_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "  \n",
        "\n",
        "  print(\"----------------DISPARATE TREATMENT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Amazon filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_disparate_treatment_exposure(business_id,\n",
        "                                                    method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes, alluser, destination)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_disparate_treatment_exposure(business_id,\n",
        "                                                    method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes, alluser, destination)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_disparate_treatment_exposure(business_id,\n",
        "                                                      method, df_ranking_by_random,\n",
        "                                                       \"rand_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"disptr_yelp_\", business_id, array_group_id, destination) # group_id\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"disptr_date_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_disptr_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                    'scatter_disptr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "  get_distribution_plots(yelp_user_exposures, date_user_exposures, random_exposures,\n",
        "                    'distribution_disptr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(\"----------------DISPARATE IMPACT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_relevance,\n",
        "                                                   \"yelp_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_date,\n",
        "                                                   \"date_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_disparate_impact_exposure(business_id, method, df_ranking_by_random,\n",
        "                                                     \"rand_\", list_of_attributes, alluser, destination)\n",
        "  \n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"dispimp_yelp_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"dispimp_date_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                    'scatter_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "  get_distribution_plots(yelp_user_exposures, date_user_exposures, random_exposures,\n",
        "                    'distribution_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "\n",
        "\n",
        "def create_groups_custom(N_of_groups, list_of_attributes, id, destination, assumption):\n",
        "  df_vectors = pd.read_csv(\"user_vectors_\" + id + \".csv\")\n",
        "\n",
        "  local_list_of_attributes = list_of_attributes\n",
        "\n",
        "  if set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity', 'review_sentiment'])):\n",
        "    text_attribute_list = ['gender', 'ethnicity']\n",
        "    df_vectors, dummy_columns_name = generate_dummies(df_vectors, text_attribute_list)\n",
        "    # list_of_attributes - text_attribute_list + dummy_columns_name\n",
        "    # subtraction\n",
        "    temp = [item for item in list_of_attributes if item not in text_attribute_list]\n",
        "    list_of_attributes = temp + dummy_columns_name\n",
        "    #serve per la descrizione dei gruppi, da fare\n",
        "\n",
        "    #EXCLUDE USERS WITH NO INFO, CLUSTER THE REMAINING IN C-1\n",
        "    df_no_info = df_vectors[pd.isna(df_vectors['gender'])]\n",
        "    df_yes_info = df_vectors[pd.notna(df_vectors['gender'])]\n",
        "    print('All users =', len(df_vectors))\n",
        "    print('Users with info =',len(df_yes_info.index))\n",
        "    print('User without info =',len(df_no_info.index))\n",
        "  \n",
        "  else:\n",
        "    df_no_info = pd.DataFrame()\n",
        "    df_yes_info = df_vectors\n",
        "\n",
        "  \n",
        "  # GRUPPI PER GENERE\n",
        "  if local_list_of_attributes == ['gender']:\n",
        "    if assumption=='':\n",
        "      femmine = df_yes_info[df_yes_info['gender']=='female']\n",
        "      maschi = df_yes_info[df_yes_info['gender']=='male']\n",
        "      new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "      new_df_users['group_id'] = np.NaN\n",
        "      for j, user in femmine.iterrows():\n",
        "        new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "        \n",
        "      for j, user in maschi.iterrows():\n",
        "        new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    else:\n",
        "      new_df_users, destination = do_assumptions(df_yes_info, df_no_info,\n",
        "                                                 destination, assumption)\n",
        "      df_no_info = df_no_info.iloc[0:0]\n",
        "\n",
        "  # GRUPPI PER REVIEW SENTIMENT\n",
        "  if local_list_of_attributes == ['review_sentiment']:\n",
        "    df_yes_info = df_yes_info[['user_id', 'review_sentiment']]\n",
        "    print(df_yes_info)\n",
        "    positivi = df_yes_info[df_yes_info['review_sentiment'].str.startswith('positive')]\n",
        "    negativi = df_yes_info[df_yes_info['review_sentiment'].str.startswith('negative')]\n",
        "    neutrali = df_yes_info[df_yes_info['review_sentiment'].str.startswith('neutral')]\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in positivi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in negativi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in neutrali.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "    \n",
        "  # GRUPPI PER ETNIA\n",
        "  if local_list_of_attributes == ['ethnicity']:\n",
        "    bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "    neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "    altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in bianchi.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in neri.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in altri.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "\n",
        "  # GRUPPI PER ETNIA, GENERE\n",
        "  if local_list_of_attributes == ['gender','ethnicity']:\n",
        "    bianchi = df_yes_info[df_yes_info['ethnicity']=='white']\n",
        "    b_under_39 = bianchi[(bianchi['age']<39)]\n",
        "    b_over_39 = bianchi[(bianchi['age']>=39)]\n",
        "    b_fem = bianchi[bianchi['gender']=='female'].reset_index(drop=True)\n",
        "    b_mas = bianchi[bianchi['gender']=='male'].reset_index(drop=True)\n",
        "    \n",
        "    neri = df_yes_info[df_yes_info['ethnicity']=='black or african american']\n",
        "    n_under_39 = neri[(neri['age']<39)]\n",
        "    n_over_39 = neri[(neri['age']>=39)]\n",
        "    n_fem = neri[neri['gender']=='female'].reset_index(drop=True)\n",
        "    n_mas = neri[neri['gender']=='male'].reset_index(drop=True)\n",
        "    \n",
        "    altri = df_yes_info[(df_yes_info['ethnicity']=='hispanic, latino, or spanish origin') | (df_yes_info['ethnicity']=='asian') | (df_yes_info['ethnicity']=='middle eastern or north african') | (df_yes_info['ethnicity']=='native hawaiian or pacific islander')]\n",
        "    a_under_39 = altri[(altri['age']<39)]\n",
        "    a_over_39 = altri[(altri['age']>=39)]\n",
        "    a_fem = altri[altri['gender']=='female'].reset_index(drop=True)\n",
        "    a_mas = altri[altri['gender']=='male'].reset_index(drop=True)\n",
        "    new_df_users = df_yes_info[['user_id']].reset_index(drop=True)\n",
        "    new_df_users['group_id'] = np.NaN\n",
        "    for j, user in b_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "      \n",
        "    for j, user in b_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 2\n",
        "    \n",
        "    for j, user in n_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 3\n",
        "      \n",
        "    for j, user in n_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 4\n",
        "      \n",
        "    for j, user in a_fem.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 5\n",
        "      \n",
        "    for j, user in a_mas.iterrows():\n",
        "      new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 6\n",
        "\n",
        "  if not df_no_info.empty:\n",
        "    # ADD THE EXCLUDED IN LAST C\n",
        "    new_df_users = pd.merge(new_df_users,df_no_info,how='outer')\n",
        "    new_df_users = drop_unnamed(new_df_users)\n",
        "    new_df_users = new_df_users.fillna(0)\n",
        "\n",
        "  #destination = set_file_destination(local_list_of_attributes, 'custom', id)\n",
        "\n",
        "  new_df_users.to_csv('groups_custom_' + id + '.csv')\n",
        "  upload_file('groups_custom_' + id + '.csv',destination)\n",
        "  return new_df_users, destination\n",
        "\n",
        "\n",
        "def do_assumptions(df_yes, df_no_info, destination, assumption):\n",
        "  total = len(df_yes.index)+len(df_no_info.index)\n",
        "  already_men = len(df_yes[df_yes['gender']=='male'].values.tolist())\n",
        "  # ALL IN ONE GROUP\n",
        "  if assumption=='all_men':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'All_men', destination)\n",
        "    df_no = df_no_info\n",
        "    df_no['gender'].fillna('male', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  if assumption=='all_women':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'All_women', destination)\n",
        "    df_no = df_no_info\n",
        "    df_no['gender'] = 'female'\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # 50%\n",
        "  if assumption=='50':\n",
        "    men_size = total//2\n",
        "    id_new_folder = create_folder_in_drive(drive, '50and50', destination)\n",
        "    df_no = df_no_info\n",
        "    while (already_men+len(df_no[df_no['gender']=='male'].values.tolist()))<=men_size:\n",
        "      index_found = df_no.index[df_no['gender'].isnull()].tolist()[0]\n",
        "      df_no.loc[index_found, 'gender'] = 'male'\n",
        "    df_no['gender'].fillna('male', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # EQUALLY DISTRIBUTED\n",
        "  if assumption=='equal':\n",
        "    id_new_folder = create_folder_in_drive(drive, 'Equally_distributed', destination)\n",
        "    df_no = df_no_info\n",
        "    even = 0\n",
        "    for ind, row in df_no.iterrows():\n",
        "      if even==0:\n",
        "        even=1\n",
        "        df_no.loc[ind,'gender']='male'\n",
        "      else:\n",
        "        df_no.loc[ind,'gender']='female'\n",
        "        even=0\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  # MAINTAIN PROPORTION\n",
        "  if assumption=='proportioned':\n",
        "    #male_size:partial=new_male_size:total\n",
        "    partial = len(df_yes.index)\n",
        "    men_size = (total*(len(df_yes[df_yes['gender']=='male'].reset_index(drop=True).index)))//partial\n",
        "    id_new_folder = create_folder_in_drive(drive, 'Maintaining_proportion', destination)\n",
        "    df_no = df_no_info\n",
        "    while (already_men+len(df_no[df_no['gender']=='male'].values.tolist()))<=men_size:\n",
        "      index_found = df_no.index[df_no['gender'].isnull()].tolist()[0]\n",
        "      df_no.loc[index_found, 'gender'] = 'male'\n",
        "    df_no['gender'].fillna('female', inplace = True)\n",
        "    df = pd.concat([df_yes, df_no])\n",
        "  femmine = df[df['gender']=='female']\n",
        "  maschi = df[df['gender']=='male']\n",
        "\n",
        "  new_df_users = df[['user_id']].reset_index(drop=True)\n",
        "  new_df_users['group_id'] = np.NaN\n",
        "  for j, user in femmine.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 0\n",
        "  for j, user in maschi.iterrows():\n",
        "    new_df_users.loc[new_df_users['user_id'] == user['user_id'], 'group_id'] = 1\n",
        "  return new_df_users, id_new_folder\n",
        "\n",
        "\n",
        "def print_disparate_impact_exposure(business_id, method,  df_ranking,\n",
        "                                    filename, list_of_attributes, alluser, destination):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = disparate_impact_exposure(df_groups[df_groups['group_id'] == i], df_ranking,user_exposures, business_id, i, alluser)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  filePath = stat_significance_inter_groups(df_groups, 'dispimp_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "\n",
        "  exposures.to_csv('exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "\n",
        "\n",
        "def print_demographic_parity_exposure(business_id, method, df_ranking, filename,\n",
        "                                      list_of_attributes, destination):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "  print(df_ranking)\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = demographic_parity_exposure(df_groups[df_groups['group_id'] == i], df_ranking, user_exposures, i)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  filePath = stat_significance_inter_groups(df_groups, 'demgr_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "\n",
        "  exposures.to_csv('exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "  \n",
        "def print_disparate_treatment_exposure(business_id, method, df_ranking, filename,\n",
        "                                      list_of_attributes, alluser, destination):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = disparate_treatment_exposure(df_groups[df_groups['group_id'] == i], df_ranking,user_exposures, business_id, i, alluser)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_disptr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_disptr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  filePath = stat_significance_inter_groups(df_groups, 'disptr_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "  exposures.to_csv('exp_disptr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_disptr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "\n",
        "def stat_significance_inter_groups(df_groups, filename, business_id, method, df_user_exposures, destination):\n",
        "  array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  couples = list(itertools.combinations(array_group_id,2))\n",
        "  couples = [(x,y) for (x,y) in couples if x!=y]\n",
        "  print(\"Group couples:\", couples) # [(0,1), (0,2), (1,2)]\n",
        "  filePath = 'stat_groups_' + method + '_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  ciclo = 0\n",
        "  pvalues = []\n",
        "  while ciclo < 3:\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('Without first and last ' + str(ciclo) + ' rows:\\n\\n')\n",
        "    user_exposures = df_user_exposures.iloc[ciclo:(len(df_user_exposures.index)-ciclo)]\n",
        "    print('Size:', len(user_exposures.index))\n",
        "    for id1,id2 in couples:\n",
        "      group1 = user_exposures[user_exposures['group_id'] == id1]['exposure']\n",
        "      group2 = user_exposures[user_exposures['group_id'] == id2]['exposure']\n",
        "      array1 = group1.values\n",
        "      array2 = group2.values\n",
        "      result = stats.ks_2samp(array1, array2)\n",
        "      pvalues.append(result[1])\n",
        "      percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "      percent2 = 100*(len(group2.index)/len(user_exposures.index))\n",
        "      with open(filePath, 'a') as output:\n",
        "        output.write('Percent GROUP '+ str(id1) +': ' + str(percent1) + '\\nPercent GROUP '+\n",
        "                    str(id2) +': ' + str(percent2) + '\\nResult: ')\n",
        "        output.write(str(result)+'\\n\\n')\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('----------------------------------------\\n\\n')\n",
        "    ciclo = ciclo + 1\n",
        "  update_result_table_inter_groups(filename[:-6], filename[-5:-1], couples, pvalues, destination)\n",
        "  return filePath\n",
        "  \n",
        "def update_result_table_inter_groups(exp_method, ranking, couple_groups, p_values, destination):\n",
        "  file_list = drive.ListFile({'q': \"'\" + destination + \"' in parents and trashed=false\"}).GetList()\n",
        "  id_result_file = ''\n",
        "  for file in file_list:\n",
        "    if file['title']=='pvalue_inter_groups.csv':\n",
        "      id_result_file = file['id']\n",
        "  \n",
        "  if id_result_file == '':\n",
        "    df = pd.DataFrame(columns=['Exposure method', 'Ranking', 'Groups', 'P-value 1', 'P-value 2', 'P-value 3'])\n",
        "  else:\n",
        "    download = drive.CreateFile({'id': id_result_file})\n",
        "    download.GetContentFile('pvalue_inter_groups.csv')\n",
        "    df = pd.read_csv('pvalue_inter_groups.csv')\n",
        "\n",
        "  if ranking == \"yelp\":\n",
        "    ranking = \"Yelp\"\n",
        "  if ranking == \"date\":\n",
        "    ranking = \"Date\"\n",
        "  if ranking == \"rand\":\n",
        "    ranking = \"Random\"\n",
        "  if exp_method == \"demgr\":\n",
        "    exp_method = \"Demographic Parity\"\n",
        "  if exp_method == \"disptr\":\n",
        "    exp_method = \"Disparate Treatment\"\n",
        "  if exp_method == \"dispimp\":\n",
        "    exp_method = \"Disparate Impact\"\n",
        "  i = 0\n",
        "  for couple in couple_groups:\n",
        "    new_row = {'Exposure method':exp_method, 'Ranking':ranking, 'Groups':str(couple[0])+' - '+str(couple[1]), \n",
        "               'P-value 1':p_values[i], 'P-value 2':p_values[i+len(couple_groups)], 'P-value 3':p_values[i+(len(couple_groups)*2)]}\n",
        "    i = i + 1\n",
        "    df = df.append(new_row, ignore_index=True)\n",
        "    df = drop_unnamed(df)\n",
        "  \n",
        "  df.to_csv('pvalue_inter_groups.csv')\n",
        "  upload_file('pvalue_inter_groups.csv', destination)\n",
        "\n",
        "def stat_significance_inter_rankings(df_user_exposures, df_random_user_exposures, method,\n",
        "                                     filename, business_id, group_ids, destination):\n",
        "  filePath = 'stat_rankings_' + method + '_' + filename + business_id + '.txt'\n",
        "  if os.path.exists(filePath):\n",
        "    os.remove(filePath)\n",
        "  ciclo = 0\n",
        "  pvalues = []\n",
        "  while ciclo < 3:\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('Without first and last ' + str(ciclo) + ' rows:\\n\\n')\n",
        "    user_exposures = df_user_exposures.iloc[ciclo:(len(df_user_exposures.index)-ciclo)]\n",
        "    random_user_exposures = df_random_user_exposures.iloc[ciclo:(len(df_random_user_exposures.index)-ciclo)]\n",
        "    for group_id in group_ids:\n",
        "      group1 = user_exposures[user_exposures['group_id'] == group_id]['exposure']\n",
        "      group2 = random_user_exposures[random_user_exposures['group_id'] == group_id]['exposure']\n",
        "      array1 = group1.values\n",
        "      array2 = group2.values\n",
        "      result = stats.ks_2samp(array1, array2)\n",
        "      pvalues.append(result[1])\n",
        "      percent1 = 100*(len(group1.index)/len(user_exposures.index))\n",
        "      percent2 = 100*(len(group2.index)/len(random_user_exposures.index))\n",
        "      print('+++++++++TEST STAT SIGNIFICANCE INTER RANKINGS+++++++++')\n",
        "      print('RESULT:')\n",
        "      print(result)\n",
        "      with open(filePath, 'a') as output:\n",
        "        output.write('Percent GROUP '+ str(group_id) +': ' + str(percent1) + '\\nResult: ')\n",
        "        output.write(str(result)+'\\n\\n')\n",
        "    with open(filePath, 'a') as output:\n",
        "        output.write('----------------------------------------\\n\\n')\n",
        "    ciclo = ciclo + 1\n",
        "  update_result_table_inter_rankings(filename[:-6], group_ids, (filename[-5:-1],'Random'), pvalues, destination)\n",
        "  return filePath\n",
        "\n",
        "def update_result_table_inter_rankings(exp_method, group_ids, rankings, p_values, destination):\n",
        "  file_list = drive.ListFile({'q': \"'\" + destination + \"' in parents and trashed=false\"}).GetList()\n",
        "  id_result_file = ''\n",
        "  for file in file_list:\n",
        "    if file['title']=='pvalue_inter_rankings.csv':\n",
        "      id_result_file = file['id']\n",
        "  \n",
        "  if id_result_file == '':\n",
        "    df = pd.DataFrame(columns=['Exposure method', 'Rankings', 'Group', 'P-value 1', 'P-value 2', 'P-value 3'])\n",
        "  else:\n",
        "    download = drive.CreateFile({'id': id_result_file})\n",
        "    download.GetContentFile('pvalue_inter_rankings.csv')\n",
        "    df = pd.read_csv('pvalue_inter_rankings.csv')\n",
        "\n",
        "  if rankings[0] == \"yelp\":\n",
        "    ranking1 = \"Yelp\"\n",
        "  if rankings[0] == \"date\":\n",
        "    ranking1 = \"Date\"\n",
        "\n",
        "  ranking2 = \"Random\"\n",
        "\n",
        "  if exp_method == \"demgr\":\n",
        "    exp_method = \"Demographic Parity\"\n",
        "  if exp_method == \"disptr\":\n",
        "    exp_method = \"Disparate Treatment\"\n",
        "  if exp_method == \"dispimp\":\n",
        "    exp_method = \"Disparate Impact\"\n",
        "  i = 0\n",
        "  for group_id in group_ids:\n",
        "    new_row = {'Exposure method':exp_method, 'Rankings':ranking1+' - '+ranking2, 'Group': group_id,\n",
        "               'P-value 1':p_values[i], 'P-value 2':p_values[i+len(group_ids)], 'P-value 3':p_values[i+(len(group_ids)*2)]}\n",
        "    i = i + 1\n",
        "    df = df.append(new_row, ignore_index=True)\n",
        "    df = drop_unnamed(df)\n",
        "  \n",
        "  df.to_csv('pvalue_inter_rankings.csv')\n",
        "  upload_file('pvalue_inter_rankings.csv', destination)\n",
        "  \n",
        "def disparate_treatment_exposure(df_group, ranking, user_exposures, business_id, group_index, alluser):\n",
        "  all_exposures = []\n",
        "  for i, user in df_group.iterrows():\n",
        "    user_id = user['user_id']\n",
        "\n",
        "    if alluser:\n",
        "      useful = ranking[ranking['user_id'] == user_id]['useful_votes'].values[0]\n",
        "    \n",
        "    counts = useful + 2\n",
        "    \n",
        "    base = 2  # con 10 i valori sono troppo bassi\n",
        "    counts = math.log(counts, base)\n",
        "    \n",
        "    position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "    \n",
        "    current_exp = exp(position) / counts\n",
        "    \n",
        "    all_exposures.append(current_exp)\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "  print('all_exposures size:', len(all_exposures))\n",
        "  mean = st.mean(all_exposures)\n",
        "  return mean, user_exposures\n",
        "\n",
        "def update_result_table(exp_method, context, means, p_value, destination):\n",
        "  file_list = drive.ListFile({'q': \"'\" + destination + \"' in parents and trashed=false\"}).GetList()\n",
        "  for file in file_list:\n",
        "    if file['title']=='result.csv':\n",
        "      id_result_file = file['id']\n",
        "  download = drive.CreateFile({'id': id_result_file}) # id file gtree.csv\n",
        "  download.GetContentFile('result.csv')\n",
        "  df = pd.read_csv('result.csv')\n",
        "  new_row = {'Exposure_method':exp_method, 'Context':context,\n",
        "             'Means':str(means[0])+' - '+str(means[1]), 'P-value':p_value}\n",
        "  df = df.append(new_row, ignore_index=True)\n",
        "  df = drop_unnamed(df)\n",
        "  df = drop_unnamed(df)\n",
        "  df.to_csv('result.csv')\n",
        "  upload_file('result.csv', destination) # id folder data\n",
        "\n",
        "\n",
        "def disparate_impact_exposure(df_group, ranking, user_exposures, business_id, group_index, alluser):\n",
        "  primo = ranking['useful_votes'].values.tolist()\n",
        "  norm = 1\n",
        "  for x in primo:\n",
        "    temp = math.log(x+2, 2)\n",
        "    if temp > norm:\n",
        "      norm = temp\n",
        "  \n",
        "  all_exposures = []\n",
        "  for i, user in df_group.iterrows():\n",
        "    user_id = user['user_id']\n",
        "\n",
        "    if alluser:\n",
        "      useful = ranking[ranking['user_id'] == user_id]['useful_votes'].values[0]\n",
        "\n",
        "    \n",
        "    counts = useful + 2\n",
        "    \n",
        "    base = 2  # con 10 i valori sono troppo bassi\n",
        "    counts = math.log(counts, base)\n",
        "    #counts = counts/norm\n",
        "    \n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(df_group)\n",
        "    print(ranking)\n",
        "    position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "    \n",
        "    current_exp = exp(position) * counts\n",
        "    \n",
        "    all_exposures.append(current_exp)\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "  print('all_exposures size:', len(all_exposures))\n",
        "  mean = st.mean(all_exposures)\n",
        "  return mean, user_exposures\n",
        "\n",
        "\n",
        "def demographic_parity_exposure(df_group, ranking, user_exposures, group_index):\n",
        "    all_exposures = []\n",
        "    for i, user in df_group.iterrows():\n",
        "        user_id = user['user_id']\n",
        "        position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "        current_exp = exp(position)\n",
        "        all_exposures.append(current_exp)\n",
        "        user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "        user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "    print('all_exposures size:', len(all_exposures))\n",
        "    mean = st.mean(all_exposures)\n",
        "    return mean, user_exposures\n",
        "\n",
        "\n",
        "def exp(position):\n",
        "    if position == 'no match':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/(math.log(1 + position, 2))\n",
        "\n",
        "\t\t\n",
        "def get_plots(yelp_exposures, date_exposures, random_exposures, percents, title,\n",
        "              list_of_attributes, method, business_id, alluser, errors, destination):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 0.5 #1.2 \n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  print(y1)\n",
        "\n",
        "  '''y1_error = np.std(y1)\n",
        "  y2_error = np.std(y2)\n",
        "  y3_error = np.std(y3)'''\n",
        "  '''# I have missing user when using dataset OR using demographic attributes\n",
        "  if alluser and set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    y1_error = st.pstdev(y1)\n",
        "    y2_error = st.pstdev(y2)\n",
        "    y3_error = st.pstdev(y3)\n",
        "  else:\n",
        "    y1_error = st.stdev(y1)\n",
        "    y2_error = st.stdev(y2)\n",
        "    y3_error = st.stdev(y3)'''\n",
        "\n",
        "  plt.bar(x1,y1,width=width,align='center', color='red', label='amazon', yerr=errors[0], capsize=5)\n",
        "  plt.bar(x2,y2,width=width,align='center', color='green', label='date', yerr=errors[1], capsize=5)\n",
        "  plt.bar(x3,y3,width=width,align='center', color='blue', label='random', yerr=errors[2], capsize=5)\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  if title.startswith(\"plot_demgr_\"):\n",
        "    plt.ylabel('Exposure (Demographic Parity)')\n",
        "  elif title.startswith(\"plot_dispimp_\"):\n",
        "    #plt.ylabel('Exposure (Disparate Impact)')\n",
        "    plt.ylabel('Exposure (Disparate Impact Normalized)')\n",
        "  elif title.startswith(\"plot_disptr_\"):\n",
        "    plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  else: plt.ylabel('Exposure')\n",
        "\n",
        "  if len(list_of_attributes)==2:\n",
        "    id_labels=[\"u\",\"f/w\",\"m/w\",\"f/b\",\"m/b\",\"f/o\",\"m/o\"]\n",
        "  if len(list_of_attributes)==1:\n",
        "    id_labels=[\"unknown\",\"woman\",\"man\"]\n",
        "  this_range = [str(id_labels[int(id)]) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  print(\"this_range\")\n",
        "  print(this_range)\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1),this_range)\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.05))#0.1\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def get_scatter_plots(yelp_user_exposures, date_user_exposures, random_user_exposures, title,\n",
        "              list_of_attributes, method, business_id, alluser, destination):\n",
        "  width = 0.20\n",
        "  y_min = 0.0\n",
        "  y_max = 1.5\n",
        "\n",
        "  x1 = [el['group_id'] - width for i, el in yelp_user_exposures[['group_id']].iterrows()]\n",
        "  x2 = [el['group_id'] for i, el in date_user_exposures[['group_id']].iterrows()]\n",
        "  x3 = [el['group_id'] + width for i, el in random_user_exposures[['group_id']].iterrows()]\n",
        "\n",
        "  y1 = [el['exposure'] for i, el in yelp_user_exposures[['exposure']].iterrows()]\n",
        "  y2 = [el['exposure'] for i, el in date_user_exposures[['exposure']].iterrows()]\n",
        "  y3 = [el['exposure'] for i, el in random_user_exposures[['exposure']].iterrows()]\n",
        "\n",
        "  plt.scatter(x1,y1, s=10, color='red', label='amazon')\n",
        "  plt.scatter(x2,y2, s=10, color='green', label='date')\n",
        "  plt.scatter(x3,y3, s=10, color='blue', label='random')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.xlabel('Group id')\n",
        "  plt.ylabel('Exposure')\n",
        "  #this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "  plt.xticks(np.arange(min(x2), max(x2)+1, 1.0))\n",
        "  plt.yticks(np.arange(y_min, y_max, 0.1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "\n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  \n",
        "  #plt.show()\n",
        "  plt.savefig(title + '.png')\n",
        "  upload_file(title + '.png', destination)\n",
        "  plt.close()\n",
        "\n",
        "def get_distribution_plots(yelp_user_exposures, date_user_exposures, random_user_exposures, title,\n",
        "              list_of_attributes, method, business_id, alluser, destination):\n",
        "  print(yelp_user_exposures)\n",
        "  asdrubale = 0\n",
        "  if asdrubale == 1:\n",
        "    width = 0.20\n",
        "    y_min = 0.0\n",
        "    y_max = 1.5\n",
        "\n",
        "    y1 = [el['exposure'] for i, el in yelp_user_exposures[['exposure']].iterrows()]\n",
        "    y2 = [el['exposure'] for i, el in date_user_exposures[['exposure']].iterrows()]\n",
        "    y3 = [el['exposure'] for i, el in random_user_exposures[['exposure']].iterrows()]\n",
        "\n",
        "    x1 = y1\n",
        "    x1.reverse()\n",
        "    \n",
        "    x2 = y2\n",
        "    x2.reverse()\n",
        "\n",
        "    x3 = y3\n",
        "    x3.reverse()\n",
        "\n",
        "    print(x1)\n",
        "    print(y1)\n",
        "\n",
        "    plt.scatter(x1,y1, s=10, color='red', label='amazon')\n",
        "    plt.scatter(x2,y2, s=10, color='green', label='date')\n",
        "    plt.scatter(x3,y3, s=10, color='blue', label='random')\n",
        "    plt.legend(loc=\"upper center\")\n",
        "    #plt.xlabel('boh')\n",
        "    if title.startswith(\"plot_demgr_\"):\n",
        "      plt.ylabel('Exposure (Demographic Parity)')\n",
        "    elif title.startswith(\"plot_dispimp_\"):\n",
        "      plt.ylabel('Exposure (Disparate Impact)')\n",
        "      #plt.ylabel('Exposure (Disparate Impact Normalized)')\n",
        "    elif title.startswith(\"plot_disptr_\"):\n",
        "      plt.ylabel('Exposure (Disparate Treatment)')\n",
        "    else: plt.ylabel('Exposure')\n",
        "    #this_range = [str(int(id)) + \":\" + \"{:.{}f}\".format(percent,1) + \"%\" for id, percent in zip(np.arange(min(x2), max(x2)+1, 1.0), percents)]\n",
        "    plt.xticks(np.arange(y_max, y_min, 0.1))\n",
        "    plt.yticks(np.arange(y_min, y_max, 0.1))\n",
        "    axes = plt.gca()\n",
        "    axes.set_ylim([y_min,y_max])\n",
        "    axes.yaxis.grid()\n",
        "    \n",
        "    plt.savefig(title + '.png')\n",
        "    upload_file(title + '.png', destination)\n",
        "    plt.close()\n",
        "\n",
        "def pipeline3m(business_id, method, df_ranking_by_relevance, df_ranking_by_date, \n",
        "              df_ranking_by_random, percents, list_of_attributes,\n",
        "              alluser, destination, df_groups,product_position,votes,reviewers):\n",
        "  \n",
        "  df_result = pd.DataFrame(columns=['Exposure_method', 'Context', 'Means', 'P-value'])\n",
        "  df_result.to_csv('result.csv')\n",
        "  id_new_file = upload_file('result.csv', destination)\n",
        "\n",
        "\n",
        "  print(\"\\n++++++++++++++++ EXPOSURE CALCULATION ++++++++++++++++++\\n\")\n",
        "  \n",
        "  print(\"----------------DEMOGRAPHIC PARITY EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Amazon filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_demographic_parity_exposure_m(business_id,\n",
        "                                                    method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes, destination,product_position,votes,reviewers)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_demographic_parity_exposure_m(business_id,\n",
        "                                                    method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes, destination,product_position,votes,reviewers)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_demographic_parity_exposure_m(business_id,\n",
        "                                                      method, df_ranking_by_random,\n",
        "                                                       \"rand_\", list_of_attributes, destination,product_position,votes,reviewers)\n",
        "  \n",
        "  array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"demgr_yelp_\", business_id, array_group_id, destination) # group_id\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"demgr_date_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, random_exposures,\n",
        "                    'scatter_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "  get_distribution_plots(yelp_user_exposures, date_user_exposures, random_exposures,\n",
        "                    'distribution_demgr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "  \n",
        "\n",
        "  print(\"----------------DISPARATE TREATMENT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Amazon filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_disparate_treatment_exposure_m(business_id,\n",
        "                                                    method, df_ranking_by_relevance,\n",
        "                                                     \"yelp_\", list_of_attributes, alluser, destination,product_position,votes,reviewers)\n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_disparate_treatment_exposure_m(business_id,\n",
        "                                                    method, df_ranking_by_date,\n",
        "                                                     \"date_\", list_of_attributes, alluser, destination,product_position,votes,reviewers)\n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_disparate_treatment_exposure_m(business_id,\n",
        "                                                      method, df_ranking_by_random,\n",
        "                                                       \"rand_\", list_of_attributes, alluser, destination,product_position,votes,reviewers)\n",
        "  \n",
        "  array_group_id = np.arange(0,df_groups['group_id'].max()+1)\n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"disptr_yelp_\", business_id, array_group_id, destination) # group_id\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"disptr_date_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_disptr_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                    'scatter_disptr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "  get_distribution_plots(yelp_user_exposures, date_user_exposures, random_exposures,\n",
        "                    'distribution_disptr_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(\"----------------DISPARATE IMPACT EXP------------------\\n\")\n",
        "  print(\"------------Ranking by Yelp filter:------------\\n\")\n",
        "  yelp_exposures, yelp_user_exposures = print_disparate_impact_exposure_m(business_id, method, df_ranking_by_relevance,\n",
        "                                                   \"yelp_\", list_of_attributes, alluser, destination,product_position,votes,reviewers)\n",
        "  \n",
        "  print(\"------------Ranking by Date:------------\\n\")\n",
        "  date_exposures, date_user_exposures = print_disparate_impact_exposure_m(business_id, method, df_ranking_by_date,\n",
        "                                                   \"date_\", list_of_attributes, alluser, destination,product_position,votes,reviewers)\n",
        "  \n",
        "  print(\"------------Ranking Random:------------\\n\")\n",
        "  random_exposures, rand_user_exposures = print_disparate_impact_exposure_m(business_id, method, df_ranking_by_random,\n",
        "                                                     \"rand_\", list_of_attributes, alluser, destination,product_position,votes,reviewers)\n",
        "  \n",
        "  filePath = stat_significance_inter_rankings(yelp_user_exposures, rand_user_exposures, method,\n",
        "                                   \"dispimp_yelp_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "  filePath = stat_significance_inter_rankings(date_user_exposures, rand_user_exposures, method,\n",
        "                                   \"dispimp_date_\", business_id, array_group_id, destination)\n",
        "  upload_file(filePath, destination)\n",
        "\n",
        "  if alluser and not set(list_of_attributes).intersection(set(['age', 'gender', 'ethnicity'])):\n",
        "    yelp_error = st.pstdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.pstdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.pstdev(rand_user_exposures['exposure'])\n",
        "  else:\n",
        "    yelp_error = st.stdev(yelp_user_exposures['exposure'])\n",
        "    date_error = st.stdev(date_user_exposures['exposure'])\n",
        "    rand_error = st.stdev(rand_user_exposures['exposure'])\n",
        "  \n",
        "  get_plots(yelp_exposures, date_exposures, random_exposures, percents,\n",
        "            'plot_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "            method, business_id, alluser, [yelp_error, date_error, rand_error], destination)\n",
        "  get_scatter_plots(yelp_user_exposures, date_user_exposures, rand_user_exposures,\n",
        "                    'scatter_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "  get_distribution_plots(yelp_user_exposures, date_user_exposures, random_exposures,\n",
        "                    'distribution_dispimp_' + method + '_' + business_id, list_of_attributes,\n",
        "                    method, business_id, alluser, destination)\n",
        "#PRINTS  \n",
        "def print_disparate_impact_exposure_m(business_id, method,  df_ranking,\n",
        "                                    filename, list_of_attributes, alluser, destination,product_position,votes,reviewers):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = disparate_impact_exposure_m(df_groups[df_groups['group_id'] == i], df_ranking,user_exposures, business_id, i, alluser, product_position,votes,reviewers)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  \n",
        "  filePath = stat_significance_inter_groups(df_groups, 'dispimp_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "\n",
        "  exposures.to_csv('exp_dispimp_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_dispimp_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "\n",
        "def print_demographic_parity_exposure_m(business_id, method, df_ranking, filename,\n",
        "                                      list_of_attributes, destination,product_position,votes,reviewers):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "  print(df_ranking)\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = demographic_parity_exposure_m(df_groups[df_groups['group_id'] == i], df_ranking, user_exposures, i, product_position,votes,reviewers)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  filePath = stat_significance_inter_groups(df_groups, 'demgr_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "\n",
        "  exposures.to_csv('exp_demgr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_demgr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "  \n",
        "def print_disparate_treatment_exposure_m(business_id, method, df_ranking, filename,\n",
        "                                      list_of_attributes, alluser, destination,product_position,votes,reviewers):\n",
        "  df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  user_exposures['user_id'] = df_ranking['user_id']\n",
        "  while i <= df_groups['group_id'].max():\n",
        "      current_exp, user_exposures = disparate_treatment_exposure_m(df_groups[df_groups['group_id'] == i], df_ranking,user_exposures, business_id, i, alluser, product_position,votes,reviewers)\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "  \n",
        "  #destination = set_file_destination(list_of_attributes, method, business_id)\n",
        "  user_exposures.to_csv('user_exp_disptr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('user_exp_disptr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  filePath = stat_significance_inter_groups(df_groups, 'disptr_'+filename, business_id,\n",
        "                                            method, user_exposures, destination)\n",
        "  exposures.to_csv('exp_disptr_' + method + '_' + filename + business_id + '.csv')\n",
        "  upload_file('exp_disptr_' + method + '_' + filename + business_id + '.csv',\n",
        "                  destination)\n",
        "  upload_file(filePath, destination)\n",
        "  print(exposures)\n",
        "  print('\\n')\n",
        "  return exposures, user_exposures\n",
        "#EXPOSURES\n",
        "def disparate_impact_exposure_m(df_group, ranking, user_exposures, business_id, group_index, alluser,product_position,votes,reviewers):\n",
        "  primo = ranking['useful_votes'].values.tolist()\n",
        "  norm = 1\n",
        "  for x in primo:\n",
        "    temp = math.log(x+2, 2)\n",
        "    if temp > norm:\n",
        "      norm = temp\n",
        "\n",
        "  all_exposures = []\n",
        "  for i, user in df_group.iterrows():\n",
        "    user_id = user['user_id']\n",
        "\n",
        "    if alluser:\n",
        "      useful = ranking[ranking['user_id'] == user_id]['useful_votes'].values[0]\n",
        "\n",
        "    \n",
        "    counts = useful + 2\n",
        "    \n",
        "    base = 2  # con 10 i valori sono troppo bassi\n",
        "    counts = math.log(counts, base)\n",
        "    #counts = counts/norm\n",
        "    \n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(df_group)\n",
        "    print(ranking)\n",
        "    position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "    \n",
        "    current_exp = exp(position) * counts * ((votes[product_position-1]*reviewers[product_position-1])/max(reviewers))\n",
        "    \n",
        "    all_exposures.append(current_exp)\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "  print('all_exposures size:', len(all_exposures))\n",
        "  mean = st.mean(all_exposures)\n",
        "  return mean, user_exposures\n",
        "\n",
        "\n",
        "def demographic_parity_exposure_m(df_group, ranking, user_exposures, group_index,product_position,votes,reviewers):\n",
        "  #(stellette*n.review)/(n.max di review per un item)\n",
        "  all_exposures = []\n",
        "  for i, user in df_group.iterrows():\n",
        "      user_id = user['user_id']\n",
        "      position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "      current_exp = exp(position) * ((votes[product_position-1]*reviewers[product_position-1])/max(reviewers))\n",
        "      all_exposures.append(current_exp)\n",
        "      user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "      user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "  print('all_exposures size:', len(all_exposures))\n",
        "  mean = st.mean(all_exposures)\n",
        "  return mean, user_exposures\n",
        "\n",
        "def disparate_treatment_exposure_m(df_group, ranking, user_exposures, business_id, group_index, alluser,product_position,votes,reviewers):\n",
        "  all_exposures = []\n",
        "  for i, user in df_group.iterrows():\n",
        "    user_id = user['user_id']\n",
        "\n",
        "    if alluser:\n",
        "      useful = ranking[ranking['user_id'] == user_id]['useful_votes'].values[0]\n",
        "    \n",
        "    counts = useful + 2\n",
        "    \n",
        "    base = 2  # con 10 i valori sono troppo bassi\n",
        "    counts = math.log(counts, base)\n",
        "    \n",
        "    position = ranking[ranking['user_id'] == user_id]['position'].values[0]\n",
        "    \n",
        "    current_exp = exp(position) * ((votes[product_position-1]*reviewers[product_position-1])/max(reviewers)) / counts \n",
        "    \n",
        "    all_exposures.append(current_exp)\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'exposure'] = current_exp\n",
        "    user_exposures.loc[user_exposures['user_id'] == user_id, 'group_id'] = group_index\n",
        "  print('all_exposures size:', len(all_exposures))\n",
        "  mean = st.mean(all_exposures)\n",
        "  return mean, user_exposures\n",
        "\n",
        "def normalize_exposure(df_exposure):\n",
        "  normazized_df_exposure = pd.DataFrame(columns=['user_id','group_id','exposure'])\n",
        "  normalizing_factor=max(df_exposure['exposure'].tolist())#for MAX\n",
        "  #normalizing_factor=np.linalg.norm(df_exposure['exposure'].tolist())# for EUCLIDEAN NORM\n",
        "  for i, elem in df_exposure.iterrows():\n",
        "    row = {\n",
        "        'user_id': elem['user_id'],\n",
        "        'group_id': elem['group_id'],\n",
        "        'exposure': float(elem['exposure'])/normalizing_factor\n",
        "    }\n",
        "    normazized_df_exposure = normazized_df_exposure.append(row, ignore_index=True)\n",
        "  return normazized_df_exposure\n",
        "\n",
        "def mean_normalized_exposure(df_exposure):\n",
        "  #df_groups = pd.read_csv('groups_' + method + '_' + business_id + '.csv')\n",
        "  i = 0\n",
        "  exposures = pd.DataFrame(columns=['group_id', 'exposure'])\n",
        "  #user_exposures = pd.DataFrame(columns=['user_id', 'group_id', 'exposure'])\n",
        "  #user_exposures['user_id'] = df_exposure['user_id']\n",
        "  while i <= df_exposure['group_id'].max():\n",
        "      current_exp = st.mean(df_exposure[df_exposure['group_id'] == i]['exposure'].values.tolist())\n",
        "      exposures.loc[i, 'group_id'] = i\n",
        "      exposures.loc[i, 'exposure'] = current_exp\n",
        "      i = i + 1\n",
        "\n",
        "  return exposures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Tk_BjuuTS3"
      },
      "source": [
        "#ESECUZIONE PIPELINE SENZA ASSUNZIONI\n",
        "drive = authenticate()\n",
        "download = drive.CreateFile({'id': '1V-L7IqaPVcBp4RvWKnUXt00SHrsT1e9O'})\n",
        "download.GetContentFile('queries.csv')\n",
        "\n",
        "queries = pd.read_csv(\"queries.csv\")\n",
        "queries = drop_unnamed(queries)\n",
        "\n",
        "query=4\n",
        "products_gtree_id = queries.loc[queries[\"cardinal\"]==query,\"products_gtree\"].tolist()[0]\n",
        "products_folder_id = queries.loc[queries[\"cardinal\"]==query,\"products_folder\"].tolist()[0]\n",
        "reviews_folder_id = queries.loc[queries[\"cardinal\"]==query,\"reviews_folder\"].tolist()[0]\n",
        "fairness_data_id = queries.loc[queries[\"cardinal\"]==query,\"fairness_data\"].tolist()[0]\n",
        "face_data_id = queries.loc[queries[\"cardinal\"]==query,\"face_data\"].tolist()[0]\n",
        "\n",
        "download = drive.CreateFile({'id': products_gtree_id})\n",
        "download.GetContentFile(str(query)+'_gtree.csv')\n",
        "\n",
        "gtree = pd.read_csv(str(query)+'_gtree.csv')\n",
        "gtree = drop_unnamed(gtree)\n",
        "\n",
        "products_id_list = [\"B01LXY19XD\"]\n",
        "\n",
        "\n",
        "#list_of_attributes = ['review_count'] # alluser\n",
        "#list_of_attributes = ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool'] # dataset\n",
        "#list_of_attributes = ['loc1', 'loc2', 'loc3'] # dataset\n",
        "#list_of_attributes = ['age', 'gender', 'ethnicity'] # alluser\n",
        "#list_of_attributes = ['fans'] # dataset\n",
        "#list_of_attributes = ['useful', 'funny', 'cool'] # dataset\n",
        "#list_of_attributes = ['gender', 'ethnicity'] # alluser\n",
        "#list_of_attributes = ['ethnicity'] # alluser\n",
        "#list_of_attributes = ['review_count', 'friend_count', 'photo_count'] # alluser\n",
        "#list_of_attributes = ['review_sentiment'] # alluser\n",
        "list_of_attributes = ['gender'] # alluser\n",
        "method = 'custom'\n",
        "folder_name = '#14_custom_gender_'\n",
        "gtree_folder_destination_name = 'gfolder_#14'\n",
        "N_of_groups = 3\n",
        "alluser = True\n",
        "\n",
        "for id in products_id_list:\n",
        "  print(id)\n",
        "  drive = authenticate()\n",
        "  df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1(id, alluser) \n",
        "  \n",
        "  destination = create_folder_in_drive(drive, folder_name+str(id), products_folder_id)\n",
        "  destination_copy = destination\n",
        "\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                  N_of_groups, list_of_attributes, method,\n",
        "                                  alluser, destination, '')\n",
        "  \n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "            percents, list_of_attributes, alluser, destination, group_list)\n",
        "print(\"END\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35X6gpHNgvUM"
      },
      "source": [
        "#GENERA GRAFICI DI DISTRIBUZIONE CON TAGLI 0.05\n",
        "drive = authenticate()\n",
        "id_item = \"B01LXY19XD\"\n",
        "example_name = \"Amazon_4_5_\"+ id_item +\"_taglio5\"\n",
        "percentuale_taglio = 0.05\n",
        "\n",
        "width = 0.20\n",
        "y_min = 0.0\n",
        "y_max = 1.5\n",
        "ticks = 0.1\n",
        "\n",
        "def taglia(lista,percentuale):\n",
        "  temp = []\n",
        "  point = None\n",
        "  for elem in lista:\n",
        "    if point is None:\n",
        "      point = elem\n",
        "      temp.append(point)\n",
        "    if point is not None:\n",
        "      if ((point-elem)/point) > percentuale:\n",
        "        point = elem\n",
        "        temp.append(point)\n",
        "    \n",
        "  return temp\n",
        "\n",
        "example_folder = create_folder_in_drive(drive, example_name, \"15NfEUQv3U5ARe7jBPzf4SrAx8SN-vTbO\")\n",
        "\n",
        "demog_folder = create_folder_in_drive(drive, \"Demographic parity\", example_folder)\n",
        "dispimp_folder = create_folder_in_drive(drive, \"Disparate impact\", example_folder)\n",
        "disptr_folder = create_folder_in_drive(drive, \"Disparate treatment\", example_folder)\n",
        "\n",
        "all_demog_folder = create_folder_in_drive(drive, \"All_rankings\", demog_folder)\n",
        "amazon_demog_folder = create_folder_in_drive(drive, \"amazon_rankings\", demog_folder)\n",
        "date_demog_folder = create_folder_in_drive(drive, \"date_rankings\", demog_folder)\n",
        "rand_demog_folder = create_folder_in_drive(drive, \"random_rankings\", demog_folder)\n",
        "\n",
        "all_dispimp_folder = create_folder_in_drive(drive, \"All_rankings\", dispimp_folder)\n",
        "amazon_dispimp_folder = create_folder_in_drive(drive, \"amazon_rankings\", dispimp_folder)\n",
        "date_dispimp_folder = create_folder_in_drive(drive, \"date_rankings\", dispimp_folder)\n",
        "rand_dispimp_folder = create_folder_in_drive(drive, \"random_rankings\", dispimp_folder)\n",
        "\n",
        "all_disptr_folder = create_folder_in_drive(drive, \"All_rankings\", disptr_folder)\n",
        "amazon_disptr_folder = create_folder_in_drive(drive, \"amazon_rankings\", disptr_folder)\n",
        "date_disptr_folder = create_folder_in_drive(drive, \"date_rankings\", disptr_folder)\n",
        "rand_disptr_folder = create_folder_in_drive(drive, \"random_rankings\", disptr_folder)\n",
        "\n",
        "###   DEMOG   ###\n",
        "\n",
        "df = pd.read_csv('user_exp_demgr_custom_yelp_'+ id_item +'.csv')\n",
        "df_date = pd.read_csv('user_exp_demgr_custom_date_'+ id_item +'.csv')\n",
        "df_random = pd.read_csv('user_exp_demgr_custom_rand_'+ id_item +'.csv')\n",
        "\n",
        "x1=[]\n",
        "x2=[]\n",
        "x3=[]\n",
        "\n",
        "x1_date=[]\n",
        "x2_date=[]\n",
        "x3_date=[]\n",
        "\n",
        "x1_random=[]\n",
        "x2_random=[]\n",
        "x3_random=[]\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3.append(el['exposure'])\n",
        "    \n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_random.append(el['exposure'])\n",
        "\n",
        "x1.sort(reverse=True)\n",
        "x2.sort(reverse=True)\n",
        "x3.sort(reverse=True)\n",
        "\n",
        "x1_date.sort(reverse=True)\n",
        "x2_date.sort(reverse=True)\n",
        "x3_date.sort(reverse=True)\n",
        "\n",
        "x1_random.sort(reverse=True)\n",
        "x2_random.sort(reverse=True)\n",
        "x3_random.sort(reverse=True)\n",
        "\n",
        "\n",
        "if percentuale_taglio is not 0:\n",
        "  x1 = taglia(x1,percentuale_taglio)\n",
        "  x2 = taglia(x2,percentuale_taglio)\n",
        "  x3 = taglia(x3,percentuale_taglio)\n",
        "\n",
        "  x1_date = taglia(x1_date,percentuale_taglio)\n",
        "  x2_date = taglia(x2_date,percentuale_taglio)\n",
        "  x3_date = taglia(x3_date,percentuale_taglio)\n",
        "\n",
        "  x1_random = taglia(x1_random,percentuale_taglio)\n",
        "  x2_random = taglia(x2_random,percentuale_taglio)\n",
        "  x3_random = taglia(x3_random,percentuale_taglio)\n",
        "\n",
        "min_lenght = len(x1)\n",
        "if len(x2) < min_lenght:\n",
        "  min_lenght = len(x2)\n",
        "if len(x3) < min_lenght:\n",
        "  min_lenght = len(x3)\n",
        "\n",
        "x1 = x1[:min_lenght]\n",
        "x2 = x2[:min_lenght]\n",
        "x3 = x3[:min_lenght]\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x3 = sum(x3)\n",
        "\n",
        "min_lenght_date = len(x1_date)\n",
        "if len(x2_date) < min_lenght_date:\n",
        "  min_lenght_date = len(x2_date)\n",
        "if len(x3_date) < min_lenght_date:\n",
        "  min_lenght_date = len(x3_date)\n",
        "\n",
        "x1_date = x1_date[:min_lenght_date]\n",
        "x2_date = x2_date[:min_lenght_date]\n",
        "x3_date = x3_date[:min_lenght_date]\n",
        "\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x3_date = sum(x3_date)\n",
        "\n",
        "min_lenght_random = len(x1_random)\n",
        "if len(x2_random) < min_lenght_random:\n",
        "  min_lenght_random = len(x2_random)\n",
        "if len(x3_random) < min_lenght_random:\n",
        "  min_lenght_random = len(x3_random)\n",
        "\n",
        "x1_random = x1_random[:min_lenght_random]\n",
        "x2_random = x2_random[:min_lenght_random]\n",
        "x3_random = x3_random[:min_lenght_random]\n",
        "\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "print(\"### 1 ###\")\n",
        "\n",
        "### AMAZON ###\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns.png')\n",
        "upload_file('0-unknowns.png', amazon_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman.png')\n",
        "upload_file('1-woman.png', amazon_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man.png')\n",
        "upload_file('2-man.png', amazon_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1) + ' f: ' + str(sum_x2) + ' m: ' + str(sum_x3))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All.png')\n",
        "upload_file('All.png', amazon_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "### DATE ###\n",
        "\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-date.png')\n",
        "upload_file('0-unknowns-date.png', date_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-date.png')\n",
        "upload_file('1-woman-date.png', date_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-date.png')\n",
        "upload_file('2-man-date.png', date_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1_date) + ' f: ' + str(sum_x2_date) + ' m: ' + str(sum_x3_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All-date.png')\n",
        "upload_file('All-date.png', date_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "### RANDOM ###\n",
        "\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-random.png')\n",
        "upload_file('0-unknowns-random.png', rand_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-random.png')\n",
        "upload_file('1-woman-random.png', rand_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-random.png')\n",
        "upload_file('2-man-random.png', rand_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1_random) + ' f: ' + str(sum_x2_random) + ' m: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All-random.png')\n",
        "upload_file('All-random.png', rand_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "### ALL ###\n",
        "\n",
        "min_lenght = len(x1)\n",
        "if len(x1_date) < min_lenght:\n",
        "  min_lenght = len(x1_date)\n",
        "if len(x1_random) < min_lenght:\n",
        "  min_lenght = len(x1_random)\n",
        "if len(x2) < min_lenght:\n",
        "  min_lenght = len(x2)\n",
        "if len(x2_date) < min_lenght:\n",
        "  min_lenght = len(x2_date)\n",
        "if len(x2_random) < min_lenght:\n",
        "  min_lenght = len(x2_random)\n",
        "if len(x3) < min_lenght:\n",
        "  min_lenght = len(x3)\n",
        "if len(x3_date) < min_lenght:\n",
        "  min_lenght = len(x3_date)\n",
        "if len(x3_random) < min_lenght:\n",
        "  min_lenght = len(x3_random)\n",
        "\n",
        "print(\"### PRE ALL ###\")\n",
        "\n",
        "x1 = x1[:min_lenght]\n",
        "x1_date = x1_date[:min_lenght]\n",
        "x1_random = x1_random[:min_lenght]\n",
        "x2 = x2[:min_lenght]\n",
        "x2_date = x2_date[:min_lenght]\n",
        "x2_random = x2_random[:min_lenght]\n",
        "x3 = x3[:min_lenght]\n",
        "x3_date = x3_date[:min_lenght]\n",
        "x3_random = x3_random[:min_lenght]\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3 = sum(x3)\n",
        "sum_x3_date = sum(x3_date)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "print(\"### ALL ###\")\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x1) + ' date: ' + str(sum_x1_date) + ' rand: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-allranks.png')\n",
        "upload_file('0-unknowns-allranks.png', all_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x2) + ' date: ' + str(sum_x2_date) + ' rand: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-allranks.png')\n",
        "upload_file('1-woman-allranks.png', all_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x3) + ' date: ' + str(sum_x3_date) + ' rand: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-allranks.png')\n",
        "upload_file('2-man-allranks.png', all_demog_folder)\n",
        "plt.close()\n",
        "\n",
        "###   DISPIMP   ###\n",
        "\n",
        "width = 0.20\n",
        "y_min = 0.0\n",
        "y_max = 12\n",
        "ticks = 1\n",
        "\n",
        "df = pd.read_csv('user_exp_dispimp_custom_yelp_'+ id_item +'.csv')\n",
        "df_date = pd.read_csv('user_exp_dispimp_custom_date_'+ id_item +'.csv')\n",
        "df_random = pd.read_csv('user_exp_dispimp_custom_rand_'+ id_item +'.csv')\n",
        "\n",
        "x1=[]\n",
        "x2=[]\n",
        "x3=[]\n",
        "\n",
        "x1_date=[]\n",
        "x2_date=[]\n",
        "x3_date=[]\n",
        "\n",
        "x1_random=[]\n",
        "x2_random=[]\n",
        "x3_random=[]\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3.append(el['exposure'])\n",
        "    \n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_random.append(el['exposure'])\n",
        "\n",
        "x1.sort(reverse=True)\n",
        "x2.sort(reverse=True)\n",
        "x3.sort(reverse=True)\n",
        "\n",
        "x1_date.sort(reverse=True)\n",
        "x2_date.sort(reverse=True)\n",
        "x3_date.sort(reverse=True)\n",
        "\n",
        "x1_random.sort(reverse=True)\n",
        "x2_random.sort(reverse=True)\n",
        "x3_random.sort(reverse=True)\n",
        "\n",
        "\n",
        "if percentuale_taglio is not 0:\n",
        "  x1 = taglia(x1,percentuale_taglio)\n",
        "  x2 = taglia(x2,percentuale_taglio)\n",
        "  x3 = taglia(x3,percentuale_taglio)\n",
        "\n",
        "  x1_date = taglia(x1_date,percentuale_taglio)\n",
        "  x2_date = taglia(x2_date,percentuale_taglio)\n",
        "  x3_date = taglia(x3_date,percentuale_taglio)\n",
        "\n",
        "  x1_random = taglia(x1_random,percentuale_taglio)\n",
        "  x2_random = taglia(x2_random,percentuale_taglio)\n",
        "  x3_random = taglia(x3_random,percentuale_taglio)\n",
        "\n",
        "min_lenght = len(x1)\n",
        "if len(x2) < min_lenght:\n",
        "  min_lenght = len(x2)\n",
        "if len(x3) < min_lenght:\n",
        "  min_lenght = len(x3)\n",
        "\n",
        "x1 = x1[:min_lenght]\n",
        "x2 = x2[:min_lenght]\n",
        "x3 = x3[:min_lenght]\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x3 = sum(x3)\n",
        "\n",
        "min_lenght_date = len(x1_date)\n",
        "if len(x2_date) < min_lenght_date:\n",
        "  min_lenght_date = len(x2_date)\n",
        "if len(x3_date) < min_lenght_date:\n",
        "  min_lenght_date = len(x3_date)\n",
        "\n",
        "x1_date = x1_date[:min_lenght_date]\n",
        "x2_date = x2_date[:min_lenght_date]\n",
        "x3_date = x3_date[:min_lenght_date]\n",
        "\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x3_date = sum(x3_date)\n",
        "\n",
        "min_lenght_random = len(x1_random)\n",
        "if len(x2_random) < min_lenght_random:\n",
        "  min_lenght_random = len(x2_random)\n",
        "if len(x3_random) < min_lenght_random:\n",
        "  min_lenght_random = len(x3_random)\n",
        "\n",
        "x1_random = x1_random[:min_lenght_random]\n",
        "x2_random = x2_random[:min_lenght_random]\n",
        "x3_random = x3_random[:min_lenght_random]\n",
        "\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "print(\"### 2 ###\")\n",
        "\n",
        "### AMAZON ###\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns.png')\n",
        "upload_file('0-unknowns.png', amazon_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman.png')\n",
        "upload_file('1-woman.png', amazon_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man.png')\n",
        "upload_file('2-man.png', amazon_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1) + ' f: ' + str(sum_x2) + ' m: ' + str(sum_x3))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All.png')\n",
        "upload_file('All.png', amazon_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "### DATE ###\n",
        "\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-date.png')\n",
        "upload_file('0-unknowns-date.png', date_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-date.png')\n",
        "upload_file('1-woman-date.png', date_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-date.png')\n",
        "upload_file('2-man-date.png', date_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1_date) + ' f: ' + str(sum_x2_date) + ' m: ' + str(sum_x3_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All-date.png')\n",
        "upload_file('All-date.png', date_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "### RANDOM ###\n",
        "\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-random.png')\n",
        "upload_file('0-unknowns-random.png', rand_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-random.png')\n",
        "upload_file('1-woman-random.png', rand_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-random.png')\n",
        "upload_file('2-man-random.png', rand_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1_random) + ' f: ' + str(sum_x2_random) + ' m: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All-random.png')\n",
        "upload_file('All-random.png', rand_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "### ALL ###\n",
        "\n",
        "min_lenght = len(x1)\n",
        "if len(x1_date) < min_lenght:\n",
        "  min_lenght = len(x1_date)\n",
        "if len(x1_random) < min_lenght:\n",
        "  min_lenght = len(x1_random)\n",
        "if len(x2) < min_lenght:\n",
        "  min_lenght = len(x2)\n",
        "if len(x2_date) < min_lenght:\n",
        "  min_lenght = len(x2_date)\n",
        "if len(x2_random) < min_lenght:\n",
        "  min_lenght = len(x2_random)\n",
        "if len(x3) < min_lenght:\n",
        "  min_lenght = len(x3)\n",
        "if len(x3_date) < min_lenght:\n",
        "  min_lenght = len(x3_date)\n",
        "if len(x3_random) < min_lenght:\n",
        "  min_lenght = len(x3_random)\n",
        "print(\"### PRE ALL ###\")\n",
        "\n",
        "x1 = x1[:min_lenght]\n",
        "x1_date = x1_date[:min_lenght]\n",
        "x1_random = x1_random[:min_lenght]\n",
        "x2 = x2[:min_lenght]\n",
        "x2_date = x2_date[:min_lenght]\n",
        "x2_random = x2_random[:min_lenght]\n",
        "x3 = x3[:min_lenght]\n",
        "x3_date = x3_date[:min_lenght]\n",
        "x3_random = x3_random[:min_lenght]\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3 = sum(x3)\n",
        "sum_x3_date = sum(x3_date)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "print(\"### ALL ###\")\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x1) + ' date: ' + str(sum_x1_date) + ' rand: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-allranks.png')\n",
        "upload_file('0-unknowns-allranks.png', all_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x2) + ' date: ' + str(sum_x2_date) + ' rand: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-allranks.png')\n",
        "upload_file('1-woman-allranks.png', all_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x3) + ' date: ' + str(sum_x3_date) + ' rand: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-allranks.png')\n",
        "upload_file('2-man-allranks.png', all_dispimp_folder)\n",
        "plt.close()\n",
        "\n",
        "###   DISP TR   ###\n",
        "\n",
        "width = 0.20\n",
        "y_min = 0.0\n",
        "y_max = 1.5\n",
        "ticks = 0.1\n",
        "\n",
        "df = pd.read_csv('user_exp_disptr_custom_yelp_'+ id_item +'.csv')\n",
        "df_date = pd.read_csv('user_exp_disptr_custom_date_'+ id_item +'.csv')\n",
        "df_random = pd.read_csv('user_exp_disptr_custom_rand_'+ id_item +'.csv')\n",
        "\n",
        "x1=[]\n",
        "x2=[]\n",
        "x3=[]\n",
        "\n",
        "x1_date=[]\n",
        "x2_date=[]\n",
        "x3_date=[]\n",
        "\n",
        "x1_random=[]\n",
        "x2_random=[]\n",
        "x3_random=[]\n",
        "\n",
        "all_df = df\n",
        "all_df_date = df_date\n",
        "all_df_random = df_random\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3.append(el['exposure'])\n",
        "    \n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_random.append(el['exposure'])\n",
        "\n",
        "x1.sort(reverse=True)\n",
        "x2.sort(reverse=True)\n",
        "x3.sort(reverse=True)\n",
        "\n",
        "x1_date.sort(reverse=True)\n",
        "x2_date.sort(reverse=True)\n",
        "x3_date.sort(reverse=True)\n",
        "\n",
        "x1_random.sort(reverse=True)\n",
        "x2_random.sort(reverse=True)\n",
        "x3_random.sort(reverse=True)\n",
        "\n",
        "\n",
        "if percentuale_taglio is not 0:\n",
        "  x1 = taglia(x1,percentuale_taglio)\n",
        "  x2 = taglia(x2,percentuale_taglio)\n",
        "  x3 = taglia(x3,percentuale_taglio)\n",
        "\n",
        "  x1_date = taglia(x1_date,percentuale_taglio)\n",
        "  x2_date = taglia(x2_date,percentuale_taglio)\n",
        "  x3_date = taglia(x3_date,percentuale_taglio)\n",
        "\n",
        "  x1_random = taglia(x1_random,percentuale_taglio)\n",
        "  x2_random = taglia(x2_random,percentuale_taglio)\n",
        "  x3_random = taglia(x3_random,percentuale_taglio)\n",
        "\n",
        "min_lenght = len(x1)\n",
        "if len(x2) < min_lenght:\n",
        "  min_lenght = len(x2)\n",
        "if len(x3) < min_lenght:\n",
        "  min_lenght = len(x3)\n",
        "\n",
        "x1 = x1[:min_lenght]\n",
        "x2 = x2[:min_lenght]\n",
        "x3 = x3[:min_lenght]\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x3 = sum(x3)\n",
        "\n",
        "min_lenght_date = len(x1_date)\n",
        "if len(x2_date) < min_lenght_date:\n",
        "  min_lenght_date = len(x2_date)\n",
        "if len(x3_date) < min_lenght_date:\n",
        "  min_lenght_date = len(x3_date)\n",
        "\n",
        "x1_date = x1_date[:min_lenght_date]\n",
        "x2_date = x2_date[:min_lenght_date]\n",
        "x3_date = x3_date[:min_lenght_date]\n",
        "\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x3_date = sum(x3_date)\n",
        "\n",
        "min_lenght_random = len(x1_random)\n",
        "if len(x2_random) < min_lenght_random:\n",
        "  min_lenght_random = len(x2_random)\n",
        "if len(x3_random) < min_lenght_random:\n",
        "  min_lenght_random = len(x3_random)\n",
        "\n",
        "x1_random = x1_random[:min_lenght_random]\n",
        "x2_random = x2_random[:min_lenght_random]\n",
        "x3_random = x3_random[:min_lenght_random]\n",
        "\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "### AMAZON ###\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns.png')\n",
        "upload_file('0-unknowns.png', amazon_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman.png')\n",
        "upload_file('1-woman.png', amazon_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man.png')\n",
        "upload_file('2-man.png', amazon_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1) + ' f: ' + str(sum_x2) + ' m: ' + str(sum_x3))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All.png')\n",
        "upload_file('All.png', amazon_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "### DATE ###\n",
        "\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-date.png')\n",
        "upload_file('0-unknowns-date.png', date_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-date.png')\n",
        "upload_file('1-woman-date.png', date_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-date.png')\n",
        "upload_file('2-man-date.png', date_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1_date) + ' f: ' + str(sum_x2_date) + ' m: ' + str(sum_x3_date))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All-date.png')\n",
        "upload_file('All-date.png', date_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "### RANDOM ###\n",
        "\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-random.png')\n",
        "upload_file('0-unknowns-random.png', rand_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-random.png')\n",
        "upload_file('1-woman-random.png', rand_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-random.png')\n",
        "upload_file('2-man-random.png', rand_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: u: ' + str(sum_x1_random) + ' f: ' + str(sum_x2_random) + ' m: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('All-random.png')\n",
        "upload_file('All-random.png', rand_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "### ALL ###\n",
        "\n",
        "min_lenght = len(x1)\n",
        "if len(x1_date) < min_lenght:\n",
        "  min_lenght = len(x1_date)\n",
        "if len(x1_random) < min_lenght:\n",
        "  min_lenght = len(x1_random)\n",
        "if len(x2) < min_lenght:\n",
        "  min_lenght = len(x2)\n",
        "if len(x2_date) < min_lenght:\n",
        "  min_lenght = len(x2_date)\n",
        "if len(x2_random) < min_lenght:\n",
        "  min_lenght = len(x2_random)\n",
        "if len(x3) < min_lenght:\n",
        "  min_lenght = len(x3)\n",
        "if len(x3_date) < min_lenght:\n",
        "  min_lenght = len(x3_date)\n",
        "if len(x3_random) < min_lenght:\n",
        "  min_lenght = len(x3_random)\n",
        "print(\"### PRE ALL ###\")\n",
        "\n",
        "x1 = x1[:min_lenght]\n",
        "x1_date = x1_date[:min_lenght]\n",
        "x1_random = x1_random[:min_lenght]\n",
        "x2 = x2[:min_lenght]\n",
        "x2_date = x2_date[:min_lenght]\n",
        "x2_random = x2_random[:min_lenght]\n",
        "x3 = x3[:min_lenght]\n",
        "x3_date = x3_date[:min_lenght]\n",
        "x3_random = x3_random[:min_lenght]\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3 = sum(x3)\n",
        "sum_x3_date = sum(x3_date)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "print(\"### ALL ###\")\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x1) + ' date: ' + str(sum_x1_date) + ' rand: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-allranks.png')\n",
        "upload_file('0-unknowns-allranks.png', all_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x2) + ' date: ' + str(sum_x2_date) + ' rand: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-allranks.png')\n",
        "upload_file('1-woman-allranks.png', all_disptr_folder)\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x3) + ' date: ' + str(sum_x3_date) + ' rand: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-allranks.png')\n",
        "upload_file('2-man-allranks.png', all_disptr_folder)\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_XSW1390jCd"
      },
      "source": [
        "#GENERA GRAFICI DI DISTRIBUZIONE CON TAGLI PER DEMOGRAPHIC PARITY\n",
        "def taglia(lista,percentuale):\n",
        "  temp = []\n",
        "  point = None\n",
        "  for elem in lista:\n",
        "    if point is None:\n",
        "      point = elem\n",
        "      temp.append(point)\n",
        "    if point is not None:\n",
        "      if ((point-elem)/point) > percentuale:\n",
        "        print(((point-elem)/point))\n",
        "        point = elem\n",
        "        temp.append(point)\n",
        "    \n",
        "  return temp\n",
        "\n",
        "df = pd.read_csv('user_exp_demgr_custom_yelp_B01LXY19XD.csv')\n",
        "df_date = pd.read_csv('user_exp_demgr_custom_date_B01LXY19XD.csv')\n",
        "df_random = pd.read_csv('user_exp_demgr_custom_rand_B01LXY19XD.csv')\n",
        "\n",
        "percentuale_taglio = 0.01\n",
        "\n",
        "width = 0.20\n",
        "y_min = 0.0\n",
        "y_max = 1.5\n",
        "ticks = 0.1\n",
        "\n",
        "x1=[]\n",
        "x2=[]\n",
        "x3=[]\n",
        "\n",
        "x1_date=[]\n",
        "x2_date=[]\n",
        "x3_date=[]\n",
        "\n",
        "x1_random=[]\n",
        "x2_random=[]\n",
        "x3_random=[]\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3.append(el['exposure'])\n",
        "    \n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_random.append(el['exposure'])\n",
        "\n",
        "x1.sort(reverse=True)\n",
        "x2.sort(reverse=True)\n",
        "x3.sort(reverse=True)\n",
        "\n",
        "x1_date.sort(reverse=True)\n",
        "x2_date.sort(reverse=True)\n",
        "x3_date.sort(reverse=True)\n",
        "\n",
        "x1_random.sort(reverse=True)\n",
        "x2_random.sort(reverse=True)\n",
        "x3_random.sort(reverse=True)\n",
        "\n",
        "\n",
        "if percentuale_taglio is not 0:\n",
        "  x1 = taglia(x1,percentuale_taglio)\n",
        "  x2 = taglia(x2,percentuale_taglio)\n",
        "  x3 = taglia(x3,percentuale_taglio)\n",
        "\n",
        "  x1_date = taglia(x1_date,percentuale_taglio)\n",
        "  x2_date = taglia(x2_date,percentuale_taglio)\n",
        "  x3_date = taglia(x3_date,percentuale_taglio)\n",
        "\n",
        "  x1_random = taglia(x1_random,percentuale_taglio)\n",
        "  x2_random = taglia(x2_random,percentuale_taglio)\n",
        "  x3_random = taglia(x3_random,percentuale_taglio)\n",
        "\n",
        "min_lenght = len(x1)\n",
        "if len(x2) < min_lenght:\n",
        "  min_lenght = len(x2)\n",
        "if len(x3) < min_lenght:\n",
        "  min_lenght = len(x3)\n",
        "\n",
        "x1 = x1[:min_lenght]\n",
        "x2 = x2[:min_lenght]\n",
        "x3 = x3[:min_lenght]\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x3 = sum(x3)\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x3_date = sum(x3_date)\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "print(len(x1))\n",
        "print(len(x2))\n",
        "print(len(x3))\n",
        "\n",
        "### AMAZON ###\n",
        "generate_all_plots = 1\n",
        "if generate_all_plots is not 0:\n",
        "  plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1) + ' f: ' + str(sum_x2) + ' m: ' + str(sum_x3))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All.png')\n",
        "  plt.close()\n",
        "\n",
        "  ### DATE ###\n",
        "\n",
        "  plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1_date) + ' f: ' + str(sum_x2_date) + ' m: ' + str(sum_x3_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  ### RANDOM ###\n",
        "\n",
        "  plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Demographic Parity)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1_random) + ' f: ' + str(sum_x2_random) + ' m: ' + str(sum_x3_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All-random.png')\n",
        "  plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x1) + ' date: ' + str(sum_x1_date) + ' rand: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-allranks.png')\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x2) + ' date: ' + str(sum_x2_date) + ' rand: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-allranks.png')\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Demographic Parity)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x3) + ' date: ' + str(sum_x3_date) + ' rand: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-allranks.png')\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mo75xMzUhrc"
      },
      "source": [
        "#GENERA GRAFICI DI DISTRIBUZIONE CON TAGLI PER DISPARATE IMPACT\n",
        "def taglia(lista,percentuale):\n",
        "  temp = []\n",
        "  point = None\n",
        "  for elem in lista:\n",
        "    if point is None:\n",
        "      point = elem\n",
        "      temp.append(point)\n",
        "    if point is not None:\n",
        "      if ((point-elem)/point) > percentuale:\n",
        "        print(((point-elem)/point))\n",
        "        point = elem\n",
        "        temp.append(point)\n",
        "    \n",
        "  return temp\n",
        "\n",
        "df = pd.read_csv('user_exp_dispimp_custom_yelp_B01LXY19XD.csv')\n",
        "df_date = pd.read_csv('user_exp_dispimp_custom_date_B01LXY19XD.csv')\n",
        "df_random = pd.read_csv('user_exp_dispimp_custom_rand_B01LXY19XD.csv')\n",
        "\n",
        "percentuale_taglio = 0.01\n",
        "\n",
        "width = 0.20\n",
        "y_min = 0.0\n",
        "y_max = 12\n",
        "\n",
        "x1=[]\n",
        "x2=[]\n",
        "x3=[]\n",
        "\n",
        "x1_date=[]\n",
        "x2_date=[]\n",
        "x3_date=[]\n",
        "\n",
        "x1_random=[]\n",
        "x2_random=[]\n",
        "x3_random=[]\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3.append(el['exposure'])\n",
        "    \n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_random.append(el['exposure'])\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x3 = sum(x3)\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x3_date = sum(x3_date)\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "x1.sort(reverse=True)\n",
        "x2.sort(reverse=True)\n",
        "x3.sort(reverse=True)\n",
        "\n",
        "x1_date.sort(reverse=True)\n",
        "x2_date.sort(reverse=True)\n",
        "x3_date.sort(reverse=True)\n",
        "\n",
        "x1_random.sort(reverse=True)\n",
        "x2_random.sort(reverse=True)\n",
        "x3_random.sort(reverse=True)\n",
        "\n",
        "\n",
        "if percentuale_taglio is not 0:\n",
        "\n",
        "  x1 = taglia(x1,percentuale_taglio)\n",
        "  x2 = taglia(x2,percentuale_taglio)\n",
        "  x3 = taglia(x3,percentuale_taglio)\n",
        "\n",
        "\n",
        "  x1_date = taglia(x1_date,percentuale_taglio)\n",
        "  x2_date = taglia(x2_date,percentuale_taglio)\n",
        "  x3_date = taglia(x3_date,percentuale_taglio)\n",
        "\n",
        "  x1_random = taglia(x1_random,percentuale_taglio)\n",
        "  x2_random = taglia(x2_random,percentuale_taglio)\n",
        "  x3_random = taglia(x3_random,percentuale_taglio)\n",
        "\n",
        "### AMAZON ###\n",
        "generate_all_plots = 1\n",
        "if generate_all_plots is not 0:\n",
        "  plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1) + ' f: ' + str(sum_x2) + ' m: ' + str(sum_x3))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All.png')\n",
        "  plt.close()\n",
        "\n",
        "  ### DATE ###\n",
        "\n",
        "  plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1_date) + ' f: ' + str(sum_x2_date) + ' m: ' + str(sum_x3_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  ### RANDOM ###\n",
        "\n",
        "  plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Impact)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1_random) + ' f: ' + str(sum_x2_random) + ' m: ' + str(sum_x3_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, 1))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All-random.png')\n",
        "  plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x1) + ' date: ' + str(sum_x1_date) + ' rand: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, 1))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-allranks.png')\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x2) + ' date: ' + str(sum_x2_date) + ' rand: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, 1))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-allranks.png')\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Impact)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x3) + ' date: ' + str(sum_x3_date) + ' rand: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, 1))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-allranks.png')\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCXZQkB2ZPFJ"
      },
      "source": [
        "#GENERA GRAFICI DI DISTRIBUZIONE CON TAGLI PER DISPARATE TREATMENT\n",
        "def taglia(lista,percentuale):\n",
        "  temp = []\n",
        "  point = None\n",
        "  for elem in lista:\n",
        "    if point is None:\n",
        "      point = elem\n",
        "      temp.append(point)\n",
        "    if point is not None:\n",
        "      if ((point-elem)/point) > percentuale:\n",
        "        print(((point-elem)/point))\n",
        "        point = elem\n",
        "        temp.append(point)\n",
        "    \n",
        "  return temp\n",
        "\n",
        "df = pd.read_csv('user_exp_disptr_custom_yelp_B01LXY19XD.csv')\n",
        "df_date = pd.read_csv('user_exp_disptr_custom_date_B01LXY19XD.csv')\n",
        "df_random = pd.read_csv('user_exp_disptr_custom_rand_B01LXY19XD.csv')\n",
        "\n",
        "percentuale_taglio = 0.02\n",
        "\n",
        "width = 0.20\n",
        "y_min = 0.0\n",
        "y_max = 1.2\n",
        "ticks = 0.1\n",
        "\n",
        "x1=[]\n",
        "x2=[]\n",
        "x3=[]\n",
        "\n",
        "x1_date=[]\n",
        "x2_date=[]\n",
        "x3_date=[]\n",
        "\n",
        "x1_random=[]\n",
        "x2_random=[]\n",
        "x3_random=[]\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2.append(el['exposure'])\n",
        "\n",
        "for i, el in df.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3.append(el['exposure'])\n",
        "    \n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_date.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_date.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 0:\n",
        "    x1_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 1:\n",
        "    x2_random.append(el['exposure'])\n",
        "\n",
        "for i, el in df_random.iterrows():\n",
        "  if el['group_id'] == 2:\n",
        "    x3_random.append(el['exposure'])\n",
        "\n",
        "sum_x1 = sum(x1)\n",
        "sum_x2 = sum(x2)\n",
        "sum_x3 = sum(x3)\n",
        "sum_x1_date = sum(x1_date)\n",
        "sum_x2_date = sum(x2_date)\n",
        "sum_x3_date = sum(x3_date)\n",
        "sum_x1_random = sum(x1_random)\n",
        "sum_x2_random = sum(x2_random)\n",
        "sum_x3_random = sum(x3_random)\n",
        "\n",
        "x1.sort(reverse=True)\n",
        "x2.sort(reverse=True)\n",
        "x3.sort(reverse=True)\n",
        "\n",
        "x1_date.sort(reverse=True)\n",
        "x2_date.sort(reverse=True)\n",
        "x3_date.sort(reverse=True)\n",
        "\n",
        "x1_random.sort(reverse=True)\n",
        "x2_random.sort(reverse=True)\n",
        "x3_random.sort(reverse=True)\n",
        "\n",
        "\n",
        "if percentuale_taglio is not 0:\n",
        "  x1 = taglia(x1,percentuale_taglio)\n",
        "  x2 = taglia(x2,percentuale_taglio)\n",
        "  x3 = taglia(x3,percentuale_taglio)\n",
        "\n",
        "  x1_date = taglia(x1_date,percentuale_taglio)\n",
        "  x2_date = taglia(x2_date,percentuale_taglio)\n",
        "  x3_date = taglia(x3_date,percentuale_taglio)\n",
        "\n",
        "  x1_random = taglia(x1_random,percentuale_taglio)\n",
        "  x2_random = taglia(x2_random,percentuale_taglio)\n",
        "  x3_random = taglia(x3_random,percentuale_taglio)\n",
        "\n",
        "print(x1)\n",
        "print(x2)\n",
        "print(x3)\n",
        "\n",
        "### AMAZON ###\n",
        "generate_all_plots = 1\n",
        "if generate_all_plots is not 0:\n",
        "  plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2))),x2, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3))),x3, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1) + ' f: ' + str(sum_x2) + ' m: ' + str(sum_x3))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All.png')\n",
        "  plt.close()\n",
        "\n",
        "  ### DATE ###\n",
        "\n",
        "  plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1_date) + ' f: ' + str(sum_x2_date) + ' m: ' + str(sum_x3_date))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All-date.png')\n",
        "  plt.close()\n",
        "\n",
        "  ### RANDOM ###\n",
        "\n",
        "  plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x1_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('0-unknowns-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x2_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('1-woman-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: ' + str(sum_x3_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('2-man-random.png')\n",
        "  plt.close()\n",
        "\n",
        "  plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='red', label='unknowns')\n",
        "  plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='green', label='woman')\n",
        "  plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='man')\n",
        "  plt.legend(loc=\"upper center\")\n",
        "  plt.ylabel('Exposure (Disparate Treatment)')\n",
        "  plt.xlabel('Exp sum: u: ' + str(sum_x1_random) + ' f: ' + str(sum_x2_random) + ' m: ' + str(sum_x3_random))\n",
        "  plt.xticks([])\n",
        "  plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "  axes = plt.gca()\n",
        "  axes.set_ylim([y_min,y_max])\n",
        "  axes.yaxis.grid()\n",
        "  plt.savefig('All-random.png')\n",
        "  plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x1))),x1, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x1_date))),x1_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x1_random))),x1_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x1) + ' date: ' + str(sum_x1_date) + ' rand: ' + str(sum_x1_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('0-unknowns-allranks.png')\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x2))),x2, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x2_date))),x2_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x2_random))),x2_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x2) + ' date: ' + str(sum_x2_date) + ' rand: ' + str(sum_x2_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('1-woman-allranks.png')\n",
        "plt.close()\n",
        "\n",
        "plt.scatter(list(range(len(x3))),x3, s=10, color='red', label='amazon')\n",
        "plt.scatter(list(range(len(x3_date))),x3_date, s=10, color='green', label='date')\n",
        "plt.scatter(list(range(len(x3_random))),x3_random, s=10, color='blue', label='random')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.ylabel('Exposure (Disparate Treatment)')\n",
        "plt.xlabel('Exp sum: amazon: ' + str(sum_x3) + ' date: ' + str(sum_x3_date) + ' rand: ' + str(sum_x3_random))\n",
        "plt.xticks([])\n",
        "plt.yticks(np.arange(y_min, y_max, ticks))\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([y_min,y_max])\n",
        "axes.yaxis.grid()\n",
        "plt.savefig('2-man-allranks.png')\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw83AXAAmZGb"
      },
      "source": [
        "#ESECUZIONE VELOCE SENZA ASSUNZIONI; MULTI-RANKING INCLUSO\n",
        "drive = authenticate()\n",
        "download = drive.CreateFile({'id': '1V-L7IqaPVcBp4RvWKnUXt00SHrsT1e9O'})\n",
        "download.GetContentFile('queries.csv')\n",
        "\n",
        "queries = pd.read_csv(\"queries.csv\")\n",
        "queries = drop_unnamed(queries)\n",
        "\n",
        "query=4\n",
        "products_gtree_id = queries.loc[queries[\"cardinal\"]==query,\"products_gtree\"].tolist()[0]\n",
        "products_folder_id = queries.loc[queries[\"cardinal\"]==query,\"products_folder\"].tolist()[0]\n",
        "reviews_folder_id = queries.loc[queries[\"cardinal\"]==query,\"reviews_folder\"].tolist()[0]\n",
        "fairness_data_id = queries.loc[queries[\"cardinal\"]==query,\"fairness_data\"].tolist()[0]\n",
        "face_data_id = queries.loc[queries[\"cardinal\"]==query,\"face_data\"].tolist()[0]\n",
        "\n",
        "download = drive.CreateFile({'id': products_gtree_id})\n",
        "download.GetContentFile(str(query)+'_gtree.csv')\n",
        "\n",
        "gtree = pd.read_csv(str(query)+'_gtree.csv')\n",
        "gtree = drop_unnamed(gtree)\n",
        "#gtree['asin'].tolist()\n",
        "products_id_list = [\"B01LXY19XD\"]\n",
        "\n",
        "#list_of_attributes = ['review_count'] # alluser\n",
        "#list_of_attributes = ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool'] # dataset\n",
        "#list_of_attributes = ['loc1', 'loc2', 'loc3'] # dataset\n",
        "#list_of_attributes = ['age', 'gender', 'ethnicity'] # alluser\n",
        "#list_of_attributes = ['fans'] # dataset\n",
        "#list_of_attributes = ['useful', 'funny', 'cool'] # dataset\n",
        "#list_of_attributes = ['gender', 'ethnicity'] # alluser\n",
        "#list_of_attributes = ['ethnicity'] # alluser\n",
        "#list_of_attributes = ['review_count', 'friend_count', 'photo_count'] # alluser\n",
        "#list_of_attributes = ['review_sentiment'] # alluser\n",
        "list_of_attributes = ['gender'] # alluser\n",
        "method = 'custom'\n",
        "folder_name = '#14_custom_gender_'\n",
        "gtree_folder_destination_name = 'gfolder_#14'\n",
        "N_of_groups = 3\n",
        "alluser = True\n",
        "\n",
        "for id in products_id_list:\n",
        "  print(id)\n",
        "  drive = authenticate()\n",
        "  df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1(id, alluser) \n",
        "  \n",
        "  destination = create_folder_in_drive(drive, folder_name+str(id), products_folder_id)\n",
        "  destination_copy = destination\n",
        "\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                  N_of_groups, list_of_attributes, method,\n",
        "                                  alluser, destination, '')\n",
        "  \n",
        "  pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "            percents, list_of_attributes, alluser, destination, group_list)\n",
        "      # MULTI RANKING\n",
        "  drive = authenticate()\n",
        "  print('''#####################################################################\n",
        "            ################## MULTI RANKING ####################################\n",
        "            #####################################################################''')\n",
        "  position = gtree.loc[gtree['asin']==id,\"position\"].tolist()[0]\n",
        "  votes = gtree['vote'].tolist()\n",
        "  reviewers = gtree['reviews_count'].tolist()\n",
        "  \n",
        "  destination_new = create_folder_in_drive(drive, \"multi_rank\", destination)\n",
        "\n",
        "  group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                  N_of_groups, list_of_attributes, method,\n",
        "                                  alluser, destination_new, '')\n",
        "  \n",
        "  print(percents)\n",
        "\n",
        "  pipeline3m(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "            percents, list_of_attributes, alluser, destination, group_list, position, votes, reviewers)\n",
        "print('FIN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb8Ik2_EAenG"
      },
      "source": [
        "#ESECUZIONE CON ASSUNZIONI\n",
        "drive = authenticate()\n",
        "download = drive.CreateFile({'id': '1V-L7IqaPVcBp4RvWKnUXt00SHrsT1e9O'})\n",
        "download.GetContentFile('queries.csv')\n",
        "\n",
        "queries = pd.read_csv(\"queries.csv\")\n",
        "queries = drop_unnamed(queries)\n",
        "\n",
        "query=4\n",
        "products_gtree_id = queries.loc[queries[\"cardinal\"]==query,\"products_gtree\"].tolist()[0]\n",
        "products_folder_id = queries.loc[queries[\"cardinal\"]==query,\"products_folder\"].tolist()[0]\n",
        "reviews_folder_id = queries.loc[queries[\"cardinal\"]==query,\"reviews_folder\"].tolist()[0]\n",
        "fairness_data_id = queries.loc[queries[\"cardinal\"]==query,\"fairness_data\"].tolist()[0]\n",
        "face_data_id = queries.loc[queries[\"cardinal\"]==query,\"face_data\"].tolist()[0]\n",
        "\n",
        "download = drive.CreateFile({'id': products_gtree_id})\n",
        "download.GetContentFile(str(query)+'_gtree.csv')\n",
        "\n",
        "gtree = pd.read_csv(str(query)+'_gtree.csv')\n",
        "gtree = drop_unnamed(gtree)\n",
        "\n",
        "products_id_list = [\"B01LXY19XD\"]\n",
        "\n",
        "#list_of_attributes = ['review_count'] # alluser\n",
        "#list_of_attributes = ['review_count', 'fans', 'average_stars', 'useful', 'funny', 'cool'] # dataset\n",
        "#list_of_attributes = ['loc1', 'loc2', 'loc3'] # dataset\n",
        "#list_of_attributes = ['age', 'gender', 'ethnicity'] # alluser\n",
        "#list_of_attributes = ['fans'] # dataset\n",
        "#list_of_attributes = ['useful', 'funny', 'cool'] # dataset\n",
        "#list_of_attributes = ['gender', 'ethnicity'] # alluser\n",
        "#list_of_attributes = ['ethnicity'] # alluser\n",
        "#list_of_attributes = ['review_count', 'friend_count', 'photo_count'] # alluser\n",
        "#list_of_attributes = ['review_sentiment'] # alluser\n",
        "list_of_attributes = ['gender'] # alluser\n",
        "method = 'custom'\n",
        "folder_name = '#14_custom_gender_'\n",
        "gtree_folder_destination_name = 'gfolder_#14'\n",
        "N_of_groups = 3\n",
        "alluser = True\n",
        "\n",
        "for id in products_id_list:\n",
        "  try:\n",
        "    print(id)\n",
        "    drive = authenticate()\n",
        "    df_rel_ranking, df_date_ranking, df_rand_ranking = pipeline1(id, alluser) \n",
        "    \n",
        "    destination = create_folder_in_drive(drive, folder_name+str(id), products_folder_id)\n",
        "    destination_copy = destination\n",
        "\n",
        "    group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                    N_of_groups, list_of_attributes, method,\n",
        "                                    alluser, destination, '')\n",
        "    \n",
        "    pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "              percents, list_of_attributes, alluser, destination, group_list)\n",
        "    \n",
        "    # MULTI RANKING\n",
        "    drive = authenticate()\n",
        "    print('''#####################################################################\n",
        "              ################## MULTI RANKING ####################################\n",
        "              #####################################################################''')\n",
        "    position = gtree.loc[gtree['asin']==id,\"position\"].tolist()[0]\n",
        "    votes = gtree['vote'].tolist()\n",
        "    reviewers = gtree['reviews_count'].tolist()\n",
        "    \n",
        "    destination_new = create_folder_in_drive(drive, \"multi_rank\", destination)\n",
        "\n",
        "    group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                    N_of_groups, list_of_attributes, method,\n",
        "                                    alluser, destination_new, '')\n",
        "    \n",
        "    print(percents)\n",
        "\n",
        "    pipeline3m(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "              percents, list_of_attributes, alluser, destination, group_list, position, votes, reviewers)\n",
        "    # ASSUMPTIONS\n",
        "    # ALL MEN\n",
        "    drive = authenticate()\n",
        "    print('''#####################################################################\n",
        "              ################## ASSUMPTION ALL MEN ###############################\n",
        "              #####################################################################''')\n",
        "\n",
        "    destination = destination_copy\n",
        "    group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                    N_of_groups, list_of_attributes, method,\n",
        "                                    alluser, destination, 'all_men')\n",
        "    \n",
        "    print(percents)\n",
        "\n",
        "    pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "              percents, list_of_attributes, alluser, destination, group_list)\n",
        "    \n",
        "    # ALL WOMEN\n",
        "    drive = authenticate()\n",
        "    print('''#####################################################################\n",
        "              ################## ASSUMPTION ALL WOMEN #############################\n",
        "              #####################################################################''')\n",
        "    destination = destination_copy\n",
        "    group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                    N_of_groups, list_of_attributes, method,\n",
        "                                    alluser, destination, 'all_women')\n",
        "    print(group_list)\n",
        "    print(percents)\n",
        "\n",
        "    pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "              percents, list_of_attributes, alluser, destination, group_list)\n",
        "    \n",
        "    # 50%\n",
        "    drive = authenticate()\n",
        "    print('''#####################################################################\n",
        "              ################## ASSUMPTION 50 AND 50 #############################\n",
        "              #####################################################################''')\n",
        "    destination = destination_copy\n",
        "    group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                    N_of_groups, list_of_attributes, method,\n",
        "                                    alluser, destination, '50')\n",
        "    print(group_list)\n",
        "    print(percents)\n",
        "\n",
        "    pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking, \n",
        "              percents, list_of_attributes, alluser, destination, group_list)\n",
        "\n",
        "    # EQUALLY DISTRIBUTED\n",
        "    drive = authenticate()\n",
        "    print('''#####################################################################\n",
        "              ################## ASSUMPTION EQUALLY DISTRIBUTED ###################\n",
        "              #####################################################################''')\n",
        "    destination = destination_copy\n",
        "    group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                    N_of_groups, list_of_attributes, method,\n",
        "                                    alluser, destination, 'equal')\n",
        "    print(group_list)\n",
        "    print(percents)\n",
        "\n",
        "    pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "              percents, list_of_attributes, alluser, destination, group_list)\n",
        "    \n",
        "    # MAINTAIN PROPORTION\n",
        "    drive = authenticate()\n",
        "    print('''#####################################################################\n",
        "              ################## ASSUMPTION MAINTAIN PROPORTION ###################\n",
        "              #####################################################################''')\n",
        "    destination = destination_copy\n",
        "    group_list, percents, destination = pipeline2(df_rel_ranking, id,\n",
        "                                    N_of_groups, list_of_attributes, method,\n",
        "                                    alluser, destination, 'proportioned')\n",
        "    print(group_list)\n",
        "    print(percents)\n",
        "\n",
        "    pipeline3(id, method, df_rel_ranking, df_date_ranking, df_rand_ranking,\n",
        "              percents, list_of_attributes, alluser, destination, group_list)\n",
        "  except:\n",
        "    failure = pd.DataFrame()\n",
        "    failure.to_csv(\"0_failure\"+str(id)+\".csv\")\n",
        "    upload_file(\"0_failure\"+str(id)+\".csv\",products_folder_id)\n",
        "    print(\"FAILURE\")\n",
        "print('FIN')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}